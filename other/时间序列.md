#### 时间序列

###### 简介

时间数据类型:在定性预测的时候我们收集到的时间序列数据往往分为两类:一类时间序列数据是定期收集的;另一类数据则是剖面数据(在时间的某一个点上收集得到).

三类时间预测模型:假设我们需要预测夏天某一个区域下一个小时的用电需求,一个最简单的带有预测变量的模型的形式为:

- $ED = f(current temperature,strength of economy,\cdots,error)$该定义中关系的定义较为明确,也较易解释,所以我们称此模型为解释模型.
- $ED_{t+1} = f(ED_t,ED_{t-1},ED_{t-2},...,error)$,其中$t$表示当前时间,$t+1$表示下一时间...... 此处下一时刻的预测完全依赖于历史上的单变量的数据,不依赖于其他的变量.
- $ED_{t+1} = f(ED_t,current temperature,\cdots, error)$,此类模型则被称为是动态回归模型。

三类时间预测模型的选择: 解释模型是首选,因为它融入了大量的信息,而不仅仅是需要预测的历史变量的信息；当系统无法解释时或者即使可以解释但是内在关系却极其复杂较难刻画,同时我们较为关心下一步会发生什么,而不是为什么它会发生,该时刻可以考虑第二种模型或者第三种模型.

时间序列数据的趋势变动可分为以下几点： 

1. 趋势性：某个变量随着时间进展或自变量变化，呈现一种比较缓慢而长期的持续上升、下降、停留的同性质变动趋向，但变动幅度可能不等。

2. 周期性：某因素由于外部影响随着自然季节的交替出现高峰与低谷的规律。 

3. 随机性：个别为随机变动，整体呈统计规律。 

4. 循环性：周期不固定的波动变化 

5. 综合性：实际变化情况一般是几种变动的叠加或组合，预测时一般设法过滤除去不规则变动，突出反映趋势性和周期性变动。

###### 平稳和非平稳

平稳性是指：序列的平稳性是指序列的均值、任意前后的时刻的值的协方差不随时间而改变。按照序列平稳的严格性要求可以分为如下的几种情况：

1. 严格平稳：严格平稳序列满足平稳过程的数学定义。严格平稳序列的均值、方差和协方差均不是时间的函数。我们的目标是将一个非平稳序列转化为一个严格平稳序列，然后对它进行预测。

2. 趋势平稳：没有单位根但显示出趋势的序列被称为趋势平稳序列。一旦去除趋势之后，产生的序列将是严格平稳的。在没有单位根的情况下，`KPSS`检测将该序列归类为平稳。这意味着序列可以是严格平稳的，也可以是趋势平稳的。

3. 差分平稳：通过差分可以使时间序列成为严格平稳的时间序列。`ADF`检验也称为差分平稳性检验。

4. 非平稳：通过差分也不能平稳的序列（但可以考虑其它的变换，如取对数、求幂等）

###### 分析技术

1. 随机时序分析就是传统意义上的时序分析技术。常用的`ARMA`建模就是随机时序分析技术中的一部分。随机时序分析以随机过程理论作为其数学基础,试图通过对时序数据进行分析,完成对时序系统的预测、建模和控制。该方法的基本思想是将所观测的时序作为系统的一维或多维输出,同时把模型所描述的等价系统视为与输出同维的白噪声驱动下产生该输出的系统。随机时序分析处理的对象是线性系统和同质非线性系统的时序数据。

2. 状态空间重构。混沌是确定性系统中出现的一种貌似无规则的、类似随机的现象。确定性系统的短期行为是完全确定的,只是由于对初值依赖的敏感,使得长期的行为不可确切预测。
3. `Kolmogorvo`连续性定理为神经网络奠定了坚实的理论基础。它证明了存在一个三层网络,其隐单元输出函数为非线性函数,输入及输出单元函数为线性的函数,此网络的总输入输出关系可以逼近任意一个非线性函数。因为任何一个时间序列都可以看成一个由非线性机制确定的输入输出系统,所以`Kolmogorvo`定理从数学上保证了用神经网络对时间序列预测的可行性。

###### 时间序列特征表示 

1. 分段线性表示是一种使用线性模型来对时间序列进行分割的方法，根据不同的分割方法可以使用不同的分割策略来实现，如滑动窗口、自底向上和自顶向下。利用滑动窗口和自底向上方法的时间复杂度为序列长度的平方阶， 而自顶向下的时间复杂度为线性阶。滑动窗口在一些情况下对时间序列的拟合效果较差，不能很好地反映原时间序列的变化信息。

2. 分段聚合近似是通过对时间序列进行平均分割并利用分段序列的均值来表示原时间序列的方法。

3. 符号化表示方法是一种将时间序列转换为字符串序列的过程。在时间序列数据挖掘过程中，传统方法主要依赖于定量数据，远远不能满足数据挖掘领域中分析和解决问题的要求。在数据结构和算法设计中，字符串具有特定的数据存储结构以及较为成熟且高效的操作算法。

4. 将时间序列根据信号处理的方式实现时间域与频率域之间的转换，再利用频率域下的有限个特征数据来近似表示原始序列。离散傅里叶变换和离散小波变换是这种时频变换方法中最具代表性的两种方法，它们具有一定的联系，同时存在较大的区别。
5. 奇异值分解表示方法 奇异值分解是一种以主成分分析方法为驱动引擎的分析方法，它利用数值计算中的`KL`分解方法将高维时间序列转换为低维数据， 进而达到降维目的。
6. 基于模型的表示方法 基于模型的表示方法通过事先假定时间序列数据是由某个模型产生，如回归模型、 隐马尔可夫模型和神经网络等，通过构造合适的模型，然后使用模型的参数或系数来实现时间序列的特征表示。

###### 相似性度量 

相似性度量是衡量不同对象之间的相互关系的方法。在时间序列数据挖掘中，相似性度量是一项重要而又基础的任务。通常情况下，时间序列特征表示方法都伴随着相应的时间序列相似性度量方法，用来度量特征表示后的时间序列的相似性。

1. 欧氏距离是一种最为简单且可直接被应用于度量两条长度相等的时间序列，但多数情况下， 它将结合时间序列的特征表示方法来对时间序列进行距离度量。

2. 动态时间弯曲是一种通过弯曲时间轴来更好地对时间序列形态进行匹配映射的相似性度量方法。

3. 符号化表示方法可以将时间序列转换成字符串，其相似性度量方法也相应地由定量数据的距离度量转换为定性符号的距离度量。

4. 基于模型的距离度量方法考虑了时间序列数据产生过程的先验知识，通过对每条时间序列建立模型并计算出使用该模型从某一时间序列产生另一序列的似然值，进而实现时间序列的相似性度量。

###### **简单时序预测**

1. 朴素预测法：在这种预测方法中，新数据点预测值等于前一个数据点的值。结果将会是一条平行线，因为所有预测的新值采用的都是先前的值。
2. 简单平均值法：视下一个值为所有先前值的平均数。这一预测法要优于“朴素预测法”，因为它的结果不会是一条平行线。但是在简单平均值法中，过去的所有值都被考虑进去了，而这些值可能并不都是有用的。例如，当要求预测今天的温度时，你仅需要考虑前七天的温度，而不是一个月前的温度。
3. 移动平均法：这是对前两个方法的改进。不取前面所有点的平均值，而是将n个先前的点的平均值作为预测值。
4. 加权移动平均法：加权移动平均是带权重的移动平均，先前的n个值被赋予不同的权重。
5. 简单指数平滑法：在这种方法中，更大的权重被分配给更近期的观测结果，来自遥远过去的观测值则被赋予较小的权重。
6. 霍尔特（Holt）线性趋势模型：该方法考虑了数据集的趋势。所谓趋势，指的是数据的递增或递减的性质。假设旅馆的预订数量每年都在增加，那么我们可以说预订数量呈现出增加的趋势。该方法的预测函数是值和趋势的函数。
7. 霍尔特-温特斯（Holt Winters）方法：该算法同时考虑了数据的趋势和季节性。例如，一家酒店的预订数量在周末很高，而在工作日则很低，并且每年都在增加；因此存在每周的季节性和增长的趋势。
8. `ARIMA`：`ARIMA`是一种非常流行的时间序列建模方法。它描述了数据点之间的相关性，并考虑了数值之间的差异。`ARIMA`的改进版是`SARIMA` (或季节性`ARIMA`)。

```python
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.metrics import mean_squared_error
from math import sqrt

#Importing data
df = pd.read_csv('../../data/international-airline-passengers.csv')
df.columns = ['ds','count']
df = df.dropna()
df.Timestamp = pd.to_datetime(df.ds,format='%Y-%m') 
df.index = df.Timestamp 

#Creating train and test set 
train=df[0:100] 
test=df[100:]

dd= np.asarray(train['count'])
y_hat = test.copy()
y_hat['naive'] = dd[len(dd)-1]
y_hat_avg['avg_forecast'] = train['count'].mean()
y_hat_avg['moving_avg_forecast'] = train['count'].rolling(14).mean().iloc[-1]

from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt
fit2=SimpleExpSmoothing(np.asarray(train['count'])).fit(smoothing_level=0.6,optimized=False)
y_hat_avg['SES'] = fit2.forecast(len(test))

import statsmodels.api as sm
sm.tsa.seasonal_decompose(train['count']).plot()
result = sm.tsa.stattools.adfuller(train['count'])
plt.show()
y_hat_avg = test.copy()
fit1 = Holt(np.asarray(train['count'])).fit(smoothing_level = 0.3, smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit1.forecast(len(test))

y_hat_avg = test.copy()
fit1 = ExponentialSmoothing(np.asarray(train['count']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit1.forecast(len(test))

rms = sqrt(mean_squared_error(test['count'], y_hat['naive']))

```

###### 平稳序列检验

 序列单位根的检验就是对时间序列平稳性的检验，非平稳时间序列如果存在单位根，则一般可以通过差分的方法来消除单位根，得到平稳序列。对于存在单位根的时间序列，一般都显示出明显的记忆性和波动的持续性，因此单位根检验是有关协整关系存在性检验和序列波动持续性讨论的基础。 

ADF检验是增项DF检验，可以用它来确定序列中单位根的存在，从而帮助判断序列是否是平稳。这一检验的原假设与备择假设如下：

原假设：序列有一个单位根(a=1的值)
备择假设：该序列没有单位根。

如果不能拒绝原假设，则该序列是非平稳的，这意味着序列可以是线性的，也可以是差分平稳的。

KPSS检验是另一种用于检查时间序列的平稳性 (与迪基-福勒检验相比稍逊一筹) 的统计检验方法。KPSS检验的原假设与备择假设与ADF检验的原假设与备择假设相反，常造成混淆。KPSS检验的作者将原假设定义为趋势平稳，并将备择假设定义为单位根序列。

原假设：序列是趋势平稳的。
备择假设：序列有一个单位根(序列是非平稳的)。

在为时间序列数据集准备模型之前，通常会同时进行两种检验。两种检验有时显示出相互矛盾的结果：其中一个检验结果表明该序列是平稳的，而另一个则表明该序列是非平稳的。ADF检验有线性平稳或差分平稳的备择假设，而KPSS检验则是识别序列的趋势平稳。

- 结果1：两种检验均得出结论：序列是非平稳的->序列是非平稳的
- 结果2：两种检验均得出结论：序列是平稳的->序列是平稳的
- 结果3：KPSS =平稳；ADF =非平稳->趋势平稳，去除趋势后序列严格平稳
- 结果4：KPSS =非平稳；ADF =平稳->差分平稳，利用差分可使序列平稳。

```python
from statsmodels.tsa.stattools import adfuller
print("原始序列的检验结果为", adfuller(data))
```

###### `ARIMA`模型预测

实现`ARIMA`模型的通用步骤如下：

- 加载数据：构建模型的第一步当然是加载数据集。
- 预处理：根据数据集定义预处理步骤。包括创建时间戳、日期/时间列转换为d类型、序列单变量化等。
- 序列平稳化：为了满足假设，应确保序列平稳。这包括检查序列的平稳性和执行所需的转换（平稳化处理后，若偏自相关函数是截尾的，而自相关函数是拖尾的，则建立AR模型；若偏自相关函数是拖尾的，而自相关函数是截尾的，则建立MA模型；若偏自相关函数和自相关函数均是拖尾的，则序列适合ARMA模型）。
- 确定d值：为了使序列平稳，执行差分操作的次数将确定为d值。
- 确定p值和q值：利用aic或bic来选择模型参数：p和q的值。
- 拟合ARIMA模型：利用我们从前面步骤中计算出来的数据和参数值，拟合ARIMA模型。
- 在验证集上进行预测：预测未来的值。
- 计算RMSE：通过检查RMSE值来检查模型的性能，用验证集上的预测值和实际值检查RMSE值。

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#Importing data
df = pd.read_csv('../data/international-airline-passengers.csv')

df.columns = ['ds','count']
df = df.dropna()
df.Timestamp = pd.to_datetime(df.ds,format='%Y-%m')
df.index = df.Timestamp

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
data = train['count'].values
plot_acf(data)
plot_pacf(data)
plt.show()

from statsmodels.tsa.stattools import adfuller
print("原始序列的检验结果为", adfuller(data))
data = pd.DataFrame(data)
D_data = data.diff().dropna()
print("差分序列的ADF 检验结果为", adfuller(D_data[0]))
D_data = D_data.diff().dropna()
plt.plot(D_data)
plt.show()
plot_acf(D_data)    #画出自相关图
plot_pacf(D_data)   #画出偏相关图
print("差分序列的ADF 检验结果为", adfuller(D_data[0]))

from statsmodels.stats.diagnostic import acorr_ljungbox
print("差分序列的白噪声检验结果：" , acorr_ljungbox(D_data, lags= 1))

from statsmodels.tsa.arima_model import ARIMA
pmax = int(len(D_data) / 10)
qmax = int(len(D_data) / 10)
bic_matrix = []
for p in range(pmax +1):
    temp= []
    for q in range(qmax+1):
        try:
            temp.append(ARIMA(D_data[0], (p, 2, q)).fit().bic)
        except:
            pass
            #temp.append(None)
        bic_matrix.append(temp)

bic_matrix = pd.DataFrame(bic_matrix)
p,q = bic_matrix.stack().idxmin()
print("BIC 最小的p值 和 q 值：%s,%s" %(p,q))

model = ARIMA(D_data[0], (0,2,1)).fit()
pred = model.forecast(20)[2][:,1]
print(pred)
plt.plot(test['count'].values, label='test')
plt.plot(pred, label='pred')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error
from math import sqrt
rms = sqrt(mean_squared_error(test['count'].values, pred))
print("RESM:", rms)

tes = list(train['count'].values) + list(test['count'].values)
pre = list(train['count'].values) + list(pred)
plt.plot(tes, label='test')
plt.plot(pre, label='pred')
plt.legend()
plt.show()
```

##### 多维时间序列

多元时间序列分析一般可以用两种方法来进行预测：

**第一种方法**是按照传统机器学习流程提取特征，选取可能影响预测值的`features`，将这些`features`引入模型，应用机器学习的分类/回归模型来进行预测。为提取`features`，机器学习方法需要多个维度的数据，预测精度较高，建立的模型较为复杂，但是模型往往不够通用，针对不同应用场景需要重新提取`features`，建立模型。 在特征提取方面，与非时序的数据不同，我们可以用一些通用的**时序特征衍生方法**，包括：**序列特征**、**基于小波变换的时域**和**频域特征**、**基于多尺度滑动窗口的统计特征**（最大值、最小值、均值、中位数、标准差、偏度、峰度、变异系数）、**基于差分的特征**（一阶、二阶甚至更高阶）、**比值特征**（各类特征与当前值的比值）等。 

 **第二种方法**就是直接利用序列建模的分析方法，利用序列前面的窗口数据来预测序列后几个窗口的数据，一般无需构建时序衍生特征，比如典型的`RNN`、`LSTM`、`GRU`等，复杂点的模型一般会结合序列建模的常用方法，比如`seq2seq`、`Attention`等 

##### 时间序列分解

假设$y_t$是在时间段$t$处的数据,$S_t$为在时间段$t$处的季节性成份,$T_t$为趋势周期成份,$R_t$为在时间段$t$处的剩余成份.那么

- **加法模型**:$y_t = S_t + T_t + R_t$,如果季节性波动是在周期趋势上下进行,那么加法模型是一个不错的选择.
- **乘法模型**:$y_t = S_t * T_t * R_t$.如果波动和时间序列的level成比例相关,那么乘法模型会比较好.

加法模型和乘法模型的联系:$y_t = S_t * T_t * R_t$ = $logy_t = logS_t + logT_t + logR_t$

###### 移动平均值

在经典的分解模型中,第一步往往是使用移动平均值来估计趋势和周期(trend-cycle),所以这一节我们介绍最经典的移动平均值.

**m-MA**:一个阶数为$m$的易懂平均可以被表示为, $\bar{T}_t = \frac{1}{m} \Sigma_{j=-k}^{k} y_{t+j}$, 其中 $m=2k+1$, 也就是说我们希望利用均值来删除数据的随机扰动,是的我们的趋势周期能够更加平滑.当$m$越大,我们的移动均值就平滑.

###### 经典分解

加法分解

1. 如果$m$是一个偶数,那么我们就使用$2*m-MA$来计算趋势周期成份$\bar{T}_t$,如果$m$是一个奇数,那么我们就使用$m-MA$来计算趋势周期成份$\bar{T}_t$.
2. 计算去趋势序列$y_t - \bar{T}_t$.
3. 为了估计每个季节的季节成份,我们直接对去趋势的数据求均值,例如,计算三月的季节成份,我们就对所有历史上去趋势的三月的数据求均值.季节性的成份可以通过将数据每一年的的值链接起来获得.这样我们就可以得到$\bar{S}_t$.
4. 我们将数据减去趋势周期值和季节值得到我们的剩余部分,$\bar{R}_t = y_t - \bar{T}_t - \bar{S}_t$.

乘法分解：乘法分解和加法分解较大的区别就是将减替换为除即可.

虽然经典的分解方法还常常被使用,但是它却存在非常多的缺点。

1. 趋势周期的估计在一些地方无法估计,例如时间序列的开始和时间序列的结尾处都无法计算,所以相应的,这些点的剩余成份和季节性成份都无法计算.
2. 趋势周的估计很容易太过平滑.
3. 经典的分解假设季节性的成份年年会重复,这对于很多长的时间序列并不是一个非常好的假设.

 监督学习往往需要针对样本数据进行特征和标签的划分，对于单维、多维时间序列以及时序图，其划分的方法都比较类似，具体可以分析如下几种情况： 

![](../picture/1/127.png)

one to one
不论是单维、多维还是时序图，都是把t时刻的数据作为特征，t+1的数据作
为标签，随着t从序列头部到后部不断移动，由此构造出序列的特征和标签。

one to many:
不论是单维、多维还是时序图，都是把t时刻的数据作为特征，t+K的数据作为标签，随着t从序列头部到后部不断移动，由此构造出序列的特征和标签。

many to one:
不论是单维、多维还是时序图，都是把t-k时刻的数据作为特征，t时刻的数据作为标签，随着t从序列头部到后部不断移动，由此构造出序列的特征和标签。

many to many:
不论是单维、多维还是时序图，都是把t-k时刻的数据作为特征，t+K的数据作为标签，随着t从序列头部到后部不断移动，由此构造出序列的特征和标签。

```python
def createSamples(ts, lookBack, lookAhead):
   dataX, dataY = [], []
   for i in range(len(ts) - lookBack - lookAhead):
       history_seq = ts[i: i + lookBack]
       future_seq = ts[i + lookBack: i + lookBack + lookAhead]
       dataX.append(history_seq)
       dataY.append(future_seq)
   dataX = np.array(dataX)
   dataY = np.array(dataY)
   return dataX, dataY
```

```python
def predict_SVR_iteration(testX, lookAhead,svrModel):
   testBatchSize = testX.shape[0]
   ans = []
 
   for i in range(lookAhead):
       pred = svrModel.predict(testX)
       ans.append(pred)
       testX = testX[:, 1:]
       pred = pred.reshape((testBatchSize, 1))
       testX = np.append(testX, pred, axis=1)
   ans = np.array(ans)
   ans = ans.transpose([1, 0])
   return ans
```

