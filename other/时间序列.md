#### 时间序列

###### 简介

时间数据类型:在定性预测的时候我们收集到的时间序列数据往往分为两类:一类时间序列数据是定期收集的;另一类数据则是剖面数据(在时间的某一个点上收集得到).

三类时间预测模型:假设我们需要预测夏天某一个区域下一个小时的用电需求,一个最简单的带有预测变量的模型的形式为:

- $ED = f(current temperature,strength of economy,\cdots,error)$该定义中关系的定义较为明确,也较易解释,所以我们称此模型为解释模型.
- $ED_{t+1} = f(ED_t,ED_{t-1},ED_{t-2},...,error)$,其中$t$表示当前时间,$t+1$表示下一时间...... 此处下一时刻的预测完全依赖于历史上的单变量的数据,不依赖于其他的变量.
- $ED_{t+1} = f(ED_t,current temperature,\cdots, error)$,此类模型则被称为是动态回归模型。

三类时间预测模型的选择: 解释模型是首选,因为它融入了大量的信息,而不仅仅是需要预测的历史变量的信息；当系统无法解释时或者即使可以解释但是内在关系却极其复杂较难刻画,同时我们较为关心下一步会发生什么,而不是为什么它会发生,该时刻可以考虑第二种模型或者第三种模型.

时间序列数据的趋势变动可分为以下几点： 

| 因素        | 说明                                                         | 实例                                                   |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------------ |
| 长期趋势T   | 指在一个相当长的时间内表现为一种近似直线的持续向上或持续向下或平稳的趋势 | 国内生产总值                                           |
| 季节变动S   | 指受季节变动影响所形成的一种长度或幅度固定的短期周期波动，季节的周期不局限于自然季节，还包括月、周等短期周期 | 冷饮、羽绒服的销售某写字楼人浏览在一周内的波动         |
| 循环变动C   | 指一种较长时间的上下起伏周期性波动，通常来说，循环时间为2~15年 | 太阳黑子数量变化                                       |
| 不规则变动I | 指受偶然因素影响所形成的不规则波动，在时间序列中无法预测     | 股票市场突然出现利好或利空信息的影响使得股价产生的波动 |

时间序列四种因素有两种组合方式。

1.四种因素相互独立，即时间序列是由四种因素直接叠加而形成的，可用加法模型表示：

Y=T+S+C+I

2.四种因素相互影响，即时间序列是综合四种因素而形成的，可用乘法模型表示：

Y=T×S×C×I，通常遇到的时间序列都是乘法模型。其中，原始时间序列值和长期趋势可用绝对数表示，季节变动、循环变动和不规则变动则用相对数（通常是变动百分比）表示。

混合模型就是公式中既有加号也有乘号。

如果时间序列图的趋势随着时间的推移，序列的季节波动变得越来越大，则建议使用乘法模型；如果序列的季节波动能够基本维持恒定，则建议使用加法模型。

###### 平稳和非平稳

平稳性是时间序列分析的基础，时间序列$\{r_t\}$称为严平稳，如果对所有的$t$，任何$k$正整数和任意$k$个正整数$(t_1,\cdots,t_k), (r_{t_1},\cdots,r_{t_k})$的联合分布与$(r_{t_1+t},\cdots,r_{t_k+t})$的联合分布是相同的。换言之，严平稳性要求$(r_{t_1},\cdots,r_{t_k})$的联合分布在时间的平移变换下保持不变。时间序列$\{r_t\}$称为弱平稳，如果$r_t$的均值与$r_t$和$r_{t-l}$的协方差不随时间而改变，其中$l$是任意整数。更具体地说，$\{r_t\}$是弱平稳的，若$E[r_t]=\mu$，$\mu$是一个常数；$Cov(r_t,r_{t-l})=\gamma_l$，$\gamma_l$只依赖于$l$。弱平稳意味着数据的时间图显示在一个常数水平下以相同幅度波动。在应用中，弱平稳性使我们可以对未来观测进行推断。$r_t$与$r_{t-l}$的相关系数成为$r_t$的时间间隔为$l$的自相关系数，通常记为$\rho_l$。$\rho_l = \frac{\gamma_l}{\gamma_0}$

给定一个时间序列$\{r_t\}_{t=1}^T$，设$\overline{r}$是样本均值，则$r_t$的间隔为$l$的样本自相关系数定义为：
$$
\hat{\rho_l} = \frac{\sum_{t=l+1}{T}(r_t-\overline{r})(r_{t-l}-\overline{r})}{\sum_{t=1}^{T}(r_t-\overline{r})^2}
$$
若$\{r_t\}$是一个独立同分布序列，满足$E[r_t^2]<\infin$，则对任意固定的正整数$l$，$\hat{\rho_l}$渐进地服从均值为0，方差为$\frac{1}{T}$的正态分布。更一般地，若$\{r+t\}$是一个弱平稳序列，满足$r_t=\mu+\sum_{i=0}^q\varphi_ia_{t-i}$，其中$\varphi_0=1$，$\{a_j\}$是均值为0的独立同分布任意变量的序列，则对$l>q$，$\hat{\rho_l}$渐近地服从均值为0、方差为$\frac{1+2\sum_{i=1}^{q}\rho_i^2}{T}$的正态分布。

检验单个ACF：对一个给定的正整数$l$，$H_0:\rho_l=0$和$H_1:\rho_l\ne0$。检验统计量为
$$
t = \frac{\hat{\rho_l}}{\sqrt{\frac{1+2\sum_{i=1}^{q}\rho_i^2}{T}}}
$$
如果$\{r_t\}$是一个平稳高斯序列并且满足当$j>l$时$\rho_j=0$，则$t$渐近服从标准正态分布。

金融应用中常需要检验$r_t$的几个自相关系数是否同时为0。原假设$H_0:\rho_1=\cdots=\rho_m=0$和备择假设$H_1:\exists i\in\{1,\cdots,m\},\rho_i\ne0$。在$\{r_t\}$满足一定距条件的独立同分布序列假设下，$Q^*(m)$渐近地服从自由度为$m$的$\chi^2$分布。
$$
Q^*(m)=T\sum_{l=1}^m\hat{\rho_l}\\
Q(m) = T(T+2)\sum_{l=1}^m\frac{\hat{\rho_l}^2}{T-l}
$$
时间序列$\{r_t\}$成为一个白噪声序列，如果$\{r_t\}$是一个具有有限均值和有限方差的独立同分布随机变量序列。特别地，若$r_t$还服从均值为0、方差为$\sigma^2$的正态分布，则称这个序列为高斯白噪声。对于白噪声序列，所有自相关函数为0。在实际应用中，如果所有样本自相关函数接近于零，则认为该序列是白噪声序列。

时间序列$\{r_t\}$称为线性序列，如果它能写成$r_t = \mu+\sum_{i=0}^{\infin}\varphi_ia_{t-i}$。其中$\mu$是$r_t$的均值，$\varphi_0=1$，$a_t$是零均值独立同分布的随机变量序列。可以得到$r_t$的均值和方差：$E[r_t] = \mu, Var[r_t]=\sigma_a^2\sum_{i=0}^{\infin}\varphi_i^2$。因为$Var[r_t]<\infin$，所以$\{\varphi_i^2\}$必须是收敛序列，即当$i\to\infin$时，$\varphi_i^2\to0$。

$r_t$的间隔为$l$的自协方差为
$$
\begin{equation}\begin{array}{l}\gamma_l = \text{Cov}(r_t,r_{t-l}) = E[(\sum_{i=0}^{\infin}\varphi_ia_{t-i})(\sum_{j=0}^{\infin}\varphi_ja_{t-l-j})]\\
\quad =E(\sum_{i,j=0}^{\infin}\varphi_i\varphi_ja_{t-i}a_{t-l-j})\\
\quad = \sigma_a^2\sum_{j=0}^{\infin}\varphi_j\varphi_{j+l}
\end{array}\end{equation}
$$
则可以得到：
$$
\rho_l = \frac{\gamma_l}{\gamma_0} = \frac{\sum_{i=0}^{\infin}\varphi_i\varphi_{i+l}}{1+\sum_{i=1}^{\infin}\varphi_i^2}
$$
自相关函数随着间隔$l$的增加而趋向于0，这个趋于0的性质是一个时间序列平稳的必要条件。

##### 简单自回归模型

$AR(p)$模型：$r_t = \phi_0+\phi_1r_{t-1}+\cdots+\phi_pr_{t-p}+a_t$

对两边同时取期望可以得到：
$$
\begin{equation}\begin{array}{l}E(r_t) = \phi_0+\phi_1+E(r_{t-1})+\cdots+\phi_pE(r_{t-p})+E(a_t)\\
\quad = \frac{\phi_0}{1-\phi_1-\cdots-\phi_p}
\end{array}\end{equation}
$$
将$AR(p)$模型进行改写：$(r_t-\mu) = \phi_1(r_{t-1}-\mu)+\cdots+\phi_p(r_{t-p}-\mu)+a_t$

将上式两端同乘以$(r_{t-l}-\mu)$，我们有
$$
\begin{equation}\begin{array}{l}(r_{t-l}-\mu)(r_t-\mu)=\phi_1(r_{t-1}-\mu)(r_{t-l}-\mu)+\cdots+\phi_p(r_{t-p}-\mu)(r_{t-l}-\mu)+a_t(r_{t-l}-\mu)\\
\end{array}\end{equation}
$$
再取期望，并利用当$l>0$时$E[(r_{t-l}-u)a_t]=0$这个性质，我们可以得到
$$
\gamma_l = \phi_1\gamma_{l-1}+\cdots+\phi_p\gamma_{l-p}
$$
在上式的两端同除以$\gamma_0$，得到$r_t$的$\text{ACF}$的性质：$\rho_l = \phi_1\rho_{l-1}+\cdots+\phi_p\rho_{l-p}$

对应的特征方程为：$1-\phi_1x-\cdots-\phi_px^p=0$

由于需要满足$\rho_l\to0$的性质，特征方程所有解的模都大于1，则序列$\{r_t\}$是平稳的。

###### 偏自相关系数

平稳时间序列的$\text{PACF}$是它的$\text{ACF}$的一个函数，它在给$\text{AR}$模型定阶时是一个有用的工具，一个简单而有效的引进$\text{PACF}$的方式时考虑如下一连串的$\text{AR}$模型：
$$
\begin{equation}\begin{array}{l}r_t = \phi_{0,1}+\phi_{1,1}r_{t-1}+e_{1t}\\
r_t = \phi_{0,2}+\phi_{1,2}r_{t-1}+\phi_{2,2}r_{t-2}+e_{2t}\\
r_t = \phi_{0,3}+\phi_{1,3}r_{t-1}+\phi_{2,3}r_{t-2}+\phi_{3,3}r_{t-3}+e_{3t}\\
\cdot\\
\cdot

\end{array}\end{equation}
$$
其中，$\phi_{0,j}$是常数项，$\phi_{i,j}$是的系数$r_{t-i}$，是$\text{AR}(j)$模型的误差想，这些模型董事多元线性回归的形式，可用最小二乘法来估计。事实上由于它们是按阶的高低排列的，故我们可以应用多元线性回归分析中的偏F检验的思想，第一个式子中的估计$\hat{\phi_{1,1}}$称为$r_t$的间隔为1的样本偏自相关函数；第二个式子中的估计$\hat{\phi_{2,2}}$称为的间隔为2的样本偏自相关函数；依此类推。对于一个$\text{AR}(p)$模型，间隔为$p$的样本偏自相关函数不应为0，而对所有$j>p$，$\hat{\phi_{j,j}}$应接近于0。可以证明样本偏自相关函数有如下性质：

- 当样本容量$T$趋于无穷时，$\hat{\phi_{p,p}}$收敛于$\phi_p$
- 对于$l>p$，$\hat{\phi_{l,l}}$收敛于0
- 对于$l>p$，$\hat{\phi_{l,l}}$的渐近方差为$\frac{1}{T}$

这些结果表明，$\text{AR}(p)$序列的样本偏自相关函数是$p$步结尾的。



###### 信息准则

###### 参数估计

对于一个具体的$\text{AR}(p)$模型，我们常用条件最小二乘发来估计其参数，条件最小二乘从第$p+1$个观测值开始。记$\hat{\phi_{i}}$为$\phi_i$的估计，所拟合的模型为$\hat{r_t} =\hat{\phi_{0}}+\hat{\phi_{1}}r_{t-1}+\cdots+\hat{\phi_{p}}r_{t-p} $。对应的残差为：$\hat{a_t} = r_t-\hat{r_t}$。称$\{\hat{a_t}\}$为残差序列，并得到
$$
\hat{\sigma_a}^2 = \frac{\sum_{t=p+1}^T\hat{a_t}^2}{T-2p-1}
$$
如果用最大似然方法，$\phi_{i}$的估计保持不变，而$\sigma_a^2$的估计变为$\tilde{\sigma_a}^2 = \hat{\sigma_a}^2\times \frac{T-2p-1}{T-p}$

###### 模型检验

如果模型是充分的，则其残差序列应是白噪声。残差的样本自相关函数和$\text{Ljung-Box}$统计量$Q(m)$渐近服从自由度为$m-g$的$\chi^2$分布，其中$g$是所用模型中$\text{AR}$系数的个数。

###### 拟合优度

衡量平稳模型拟合优度的一个常用统计量是$R^2$统计量，其定义为：$R^2=1-\frac{\text{残差的平方和}}{\text{总的平方和}}$.对于平稳$\text{AR}(p)$模型，则$R^2$变为
$$
R^2 = 1-\frac{\sum_{t=p+1}^T\hat{a_t}^2}{\sum_{t=p+1}^{T}(r_t-\overline{r})^2}
$$


##### 简单滑动平均模型

$\text{MA}(q)$模型为：$r_t = c_0+a_t-\theta_1a_{t-1}-\cdots-\theta_qa_{t-1}$

##### 简单$\text{ARMA}$模型



$\text{MA}$模型总是弱平稳的，因为它们是白噪声序列的有限线性组合，其其两阶矩是不随时间变化的。

平稳性是指：序列的平稳性是指序列的均值、任意前后的时刻的值的协方差不随时间而改变。按照序列平稳的严格性要求可以分为如下的几种情况：

1. 严格平稳：严格平稳序列满足平稳过程的数学定义。严格平稳序列的均值、方差和协方差均不是时间的函数。我们的目标是将一个非平稳序列转化为一个严格平稳序列，然后对它进行预测。

2. 趋势平稳：没有单位根但显示出趋势的序列被称为趋势平稳序列。一旦去除趋势之后，产生的序列将是严格平稳的。在没有单位根的情况下，`KPSS`检测将该序列归类为平稳。这意味着序列可以是严格平稳的，也可以是趋势平稳的。

3. 差分平稳：通过差分可以使时间序列成为严格平稳的时间序列。`ADF`检验也称为差分平稳性检验。

4. 非平稳：通过差分也不能平稳的序列（但可以考虑其它的变换，如取对数、求幂等）

###### 分析技术

1. 随机时序分析就是传统意义上的时序分析技术。常用的`ARMA`建模就是随机时序分析技术中的一部分。随机时序分析以随机过程理论作为其数学基础,试图通过对时序数据进行分析,完成对时序系统的预测、建模和控制。该方法的基本思想是将所观测的时序作为系统的一维或多维输出,同时把模型所描述的等价系统视为与输出同维的白噪声驱动下产生该输出的系统。随机时序分析处理的对象是线性系统和同质非线性系统的时序数据。

2. 状态空间重构。混沌是确定性系统中出现的一种貌似无规则的、类似随机的现象。确定性系统的短期行为是完全确定的,只是由于对初值依赖的敏感,使得长期的行为不可确切预测。
3. `Kolmogorvo`连续性定理为神经网络奠定了坚实的理论基础。它证明了存在一个三层网络,其隐单元输出函数为非线性函数,输入及输出单元函数为线性的函数,此网络的总输入输出关系可以逼近任意一个非线性函数。因为任何一个时间序列都可以看成一个由非线性机制确定的输入输出系统,所以`Kolmogorvo`定理从数学上保证了用神经网络对时间序列预测的可行性。

###### 时间序列特征表示 

1. 分段线性表示是一种使用线性模型来对时间序列进行分割的方法，根据不同的分割方法可以使用不同的分割策略来实现，如滑动窗口、自底向上和自顶向下。利用滑动窗口和自底向上方法的时间复杂度为序列长度的平方阶， 而自顶向下的时间复杂度为线性阶。滑动窗口在一些情况下对时间序列的拟合效果较差，不能很好地反映原时间序列的变化信息。

2. 分段聚合近似是通过对时间序列进行平均分割并利用分段序列的均值来表示原时间序列的方法。

3. 符号化表示方法是一种将时间序列转换为字符串序列的过程。在时间序列数据挖掘过程中，传统方法主要依赖于定量数据，远远不能满足数据挖掘领域中分析和解决问题的要求。在数据结构和算法设计中，字符串具有特定的数据存储结构以及较为成熟且高效的操作算法。

4. 将时间序列根据信号处理的方式实现时间域与频率域之间的转换，再利用频率域下的有限个特征数据来近似表示原始序列。离散傅里叶变换和离散小波变换是这种时频变换方法中最具代表性的两种方法，它们具有一定的联系，同时存在较大的区别。
5. 奇异值分解表示方法 奇异值分解是一种以主成分分析方法为驱动引擎的分析方法，它利用数值计算中的`KL`分解方法将高维时间序列转换为低维数据， 进而达到降维目的。
6. 基于模型的表示方法 基于模型的表示方法通过事先假定时间序列数据是由某个模型产生，如回归模型、 隐马尔可夫模型和神经网络等，通过构造合适的模型，然后使用模型的参数或系数来实现时间序列的特征表示。

###### 相似性度量 

相似性度量是衡量不同对象之间的相互关系的方法。在时间序列数据挖掘中，相似性度量是一项重要而又基础的任务。通常情况下，时间序列特征表示方法都伴随着相应的时间序列相似性度量方法，用来度量特征表示后的时间序列的相似性。

1. 欧氏距离是一种最为简单且可直接被应用于度量两条长度相等的时间序列，但多数情况下， 它将结合时间序列的特征表示方法来对时间序列进行距离度量。

2. 动态时间弯曲是一种通过弯曲时间轴来更好地对时间序列形态进行匹配映射的相似性度量方法。

3. 符号化表示方法可以将时间序列转换成字符串，其相似性度量方法也相应地由定量数据的距离度量转换为定性符号的距离度量。

4. 基于模型的距离度量方法考虑了时间序列数据产生过程的先验知识，通过对每条时间序列建立模型并计算出使用该模型从某一时间序列产生另一序列的似然值，进而实现时间序列的相似性度量。

###### **简单时序预测**

1. 朴素预测法：在这种预测方法中，新数据点预测值等于前一个数据点的值。结果将会是一条平行线，因为所有预测的新值采用的都是先前的值。
2. 简单平均值法：视下一个值为所有先前值的平均数。这一预测法要优于“朴素预测法”，因为它的结果不会是一条平行线。但是在简单平均值法中，过去的所有值都被考虑进去了，而这些值可能并不都是有用的。例如，当要求预测今天的温度时，你仅需要考虑前七天的温度，而不是一个月前的温度。
3. 移动平均法：这是对前两个方法的改进。不取前面所有点的平均值，而是将n个先前的点的平均值作为预测值。
4. 加权移动平均法：加权移动平均是带权重的移动平均，先前的n个值被赋予不同的权重。
5. 简单指数平滑法：在这种方法中，更大的权重被分配给更近期的观测结果，来自遥远过去的观测值则被赋予较小的权重。
6. 霍尔特（Holt）线性趋势模型：该方法考虑了数据集的趋势。所谓趋势，指的是数据的递增或递减的性质。假设旅馆的预订数量每年都在增加，那么我们可以说预订数量呈现出增加的趋势。该方法的预测函数是值和趋势的函数。
7. 霍尔特-温特斯（Holt Winters）方法：该算法同时考虑了数据的趋势和季节性。例如，一家酒店的预订数量在周末很高，而在工作日则很低，并且每年都在增加；因此存在每周的季节性和增长的趋势。
8. `ARIMA`：`ARIMA`是一种非常流行的时间序列建模方法。它描述了数据点之间的相关性，并考虑了数值之间的差异。`ARIMA`的改进版是`SARIMA` (或季节性`ARIMA`)。

```python
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.metrics import mean_squared_error
from math import sqrt

#Importing data
df = pd.read_csv('../../data/international-airline-passengers.csv')
df.columns = ['ds','count']
df = df.dropna()
df.Timestamp = pd.to_datetime(df.ds,format='%Y-%m') 
df.index = df.Timestamp 

#Creating train and test set 
train=df[0:100] 
test=df[100:]

dd= np.asarray(train['count'])
y_hat = test.copy()
y_hat['naive'] = dd[len(dd)-1]
y_hat_avg['avg_forecast'] = train['count'].mean()
y_hat_avg['moving_avg_forecast'] = train['count'].rolling(14).mean().iloc[-1]

from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt
fit2=SimpleExpSmoothing(np.asarray(train['count'])).fit(smoothing_level=0.6,optimized=False)
y_hat_avg['SES'] = fit2.forecast(len(test))

import statsmodels.api as sm
sm.tsa.seasonal_decompose(train['count']).plot()
result = sm.tsa.stattools.adfuller(train['count'])
plt.show()
y_hat_avg = test.copy()
fit1 = Holt(np.asarray(train['count'])).fit(smoothing_level = 0.3, smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit1.forecast(len(test))

y_hat_avg = test.copy()
fit1 = ExponentialSmoothing(np.asarray(train['count']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit1.forecast(len(test))

rms = sqrt(mean_squared_error(test['count'], y_hat['naive']))

```

###### 平稳序列检验

 序列单位根的检验就是对时间序列平稳性的检验，非平稳时间序列如果存在单位根，则一般可以通过差分的方法来消除单位根，得到平稳序列。对于存在单位根的时间序列，一般都显示出明显的记忆性和波动的持续性，因此单位根检验是有关协整关系存在性检验和序列波动持续性讨论的基础。 

ADF检验是增项DF检验，可以用它来确定序列中单位根的存在，从而帮助判断序列是否是平稳。这一检验的原假设与备择假设如下：

原假设：序列有一个单位根(a=1的值)
备择假设：该序列没有单位根。

如果不能拒绝原假设，则该序列是非平稳的，这意味着序列可以是线性的，也可以是差分平稳的。

KPSS检验是另一种用于检查时间序列的平稳性 (与迪基-福勒检验相比稍逊一筹) 的统计检验方法。KPSS检验的原假设与备择假设与ADF检验的原假设与备择假设相反，常造成混淆。KPSS检验的作者将原假设定义为趋势平稳，并将备择假设定义为单位根序列。

原假设：序列是趋势平稳的。
备择假设：序列有一个单位根(序列是非平稳的)。

在为时间序列数据集准备模型之前，通常会同时进行两种检验。两种检验有时显示出相互矛盾的结果：其中一个检验结果表明该序列是平稳的，而另一个则表明该序列是非平稳的。ADF检验有线性平稳或差分平稳的备择假设，而KPSS检验则是识别序列的趋势平稳。

- 结果1：两种检验均得出结论：序列是非平稳的->序列是非平稳的
- 结果2：两种检验均得出结论：序列是平稳的->序列是平稳的
- 结果3：KPSS =平稳；ADF =非平稳->趋势平稳，去除趋势后序列严格平稳
- 结果4：KPSS =非平稳；ADF =平稳->差分平稳，利用差分可使序列平稳。

```python
from statsmodels.tsa.stattools import adfuller
print("原始序列的检验结果为", adfuller(data))
```

###### `ARIMA`模型预测

实现`ARIMA`模型的通用步骤如下：

- 加载数据：构建模型的第一步当然是加载数据集。
- 预处理：根据数据集定义预处理步骤。包括创建时间戳、日期/时间列转换为d类型、序列单变量化等。
- 序列平稳化：为了满足假设，应确保序列平稳。这包括检查序列的平稳性和执行所需的转换（平稳化处理后，若偏自相关函数是截尾的，而自相关函数是拖尾的，则建立AR模型；若偏自相关函数是拖尾的，而自相关函数是截尾的，则建立MA模型；若偏自相关函数和自相关函数均是拖尾的，则序列适合ARMA模型）。
- 确定d值：为了使序列平稳，执行差分操作的次数将确定为d值。
- 确定p值和q值：利用aic或bic来选择模型参数：p和q的值。
- 拟合ARIMA模型：利用我们从前面步骤中计算出来的数据和参数值，拟合ARIMA模型。
- 在验证集上进行预测：预测未来的值。
- 计算RMSE：通过检查RMSE值来检查模型的性能，用验证集上的预测值和实际值检查RMSE值。

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#Importing data
df = pd.read_csv('../data/international-airline-passengers.csv')

df.columns = ['ds','count']
df = df.dropna()
df.Timestamp = pd.to_datetime(df.ds,format='%Y-%m')
df.index = df.Timestamp

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
data = train['count'].values
plot_acf(data)
plot_pacf(data)
plt.show()

from statsmodels.tsa.stattools import adfuller
print("原始序列的检验结果为", adfuller(data))
data = pd.DataFrame(data)
D_data = data.diff().dropna()
print("差分序列的ADF 检验结果为", adfuller(D_data[0]))
D_data = D_data.diff().dropna()
plt.plot(D_data)
plt.show()
plot_acf(D_data)    #画出自相关图
plot_pacf(D_data)   #画出偏相关图
print("差分序列的ADF 检验结果为", adfuller(D_data[0]))

from statsmodels.stats.diagnostic import acorr_ljungbox
print("差分序列的白噪声检验结果：" , acorr_ljungbox(D_data, lags= 1))

from statsmodels.tsa.arima_model import ARIMA
pmax = int(len(D_data) / 10)
qmax = int(len(D_data) / 10)
bic_matrix = []
for p in range(pmax +1):
    temp= []
    for q in range(qmax+1):
        try:
            temp.append(ARIMA(D_data[0], (p, 2, q)).fit().bic)
        except:
            pass
            #temp.append(None)
        bic_matrix.append(temp)

bic_matrix = pd.DataFrame(bic_matrix)
p,q = bic_matrix.stack().idxmin()
print("BIC 最小的p值 和 q 值：%s,%s" %(p,q))

model = ARIMA(D_data[0], (0,2,1)).fit()
pred = model.forecast(20)[2][:,1]
print(pred)
plt.plot(test['count'].values, label='test')
plt.plot(pred, label='pred')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error
from math import sqrt
rms = sqrt(mean_squared_error(test['count'].values, pred))
print("RESM:", rms)

tes = list(train['count'].values) + list(test['count'].values)
pre = list(train['count'].values) + list(pred)
plt.plot(tes, label='test')
plt.plot(pre, label='pred')
plt.legend()
plt.show()
```

##### 多维时间序列

多元时间序列分析一般可以用两种方法来进行预测：

**第一种方法**是按照传统机器学习流程提取特征，选取可能影响预测值的`features`，将这些`features`引入模型，应用机器学习的分类/回归模型来进行预测。为提取`features`，机器学习方法需要多个维度的数据，预测精度较高，建立的模型较为复杂，但是模型往往不够通用，针对不同应用场景需要重新提取`features`，建立模型。 在特征提取方面，与非时序的数据不同，我们可以用一些通用的**时序特征衍生方法**，包括：**序列特征**、**基于小波变换的时域**和**频域特征**、**基于多尺度滑动窗口的统计特征**（最大值、最小值、均值、中位数、标准差、偏度、峰度、变异系数）、**基于差分的特征**（一阶、二阶甚至更高阶）、**比值特征**（各类特征与当前值的比值）等。 

 **第二种方法**就是直接利用序列建模的分析方法，利用序列前面的窗口数据来预测序列后几个窗口的数据，一般无需构建时序衍生特征，比如典型的`RNN`、`LSTM`、`GRU`等，复杂点的模型一般会结合序列建模的常用方法，比如`seq2seq`、`Attention`等 

##### 时间序列分解

假设$y_t$是在时间段$t$处的数据,$S_t$为在时间段$t$处的季节性成份,$T_t$为趋势周期成份,$R_t$为在时间段$t$处的剩余成份.那么

- **加法模型**:$y_t = S_t + T_t + R_t$,如果季节性波动是在周期趋势上下进行,那么加法模型是一个不错的选择.
- **乘法模型**:$y_t = S_t * T_t * R_t$.如果波动和时间序列的level成比例相关,那么乘法模型会比较好.

加法模型和乘法模型的联系:$y_t = S_t * T_t * R_t$ = $logy_t = logS_t + logT_t + logR_t$

###### 移动平均值

在经典的分解模型中,第一步往往是使用移动平均值来估计趋势和周期(trend-cycle),所以这一节我们介绍最经典的移动平均值.

**m-MA**:一个阶数为$m$的易懂平均可以被表示为, $\bar{T}_t = \frac{1}{m} \Sigma_{j=-k}^{k} y_{t+j}$, 其中 $m=2k+1$, 也就是说我们希望利用均值来删除数据的随机扰动,是的我们的趋势周期能够更加平滑.当$m$越大,我们的移动均值就平滑.

###### 经典分解

加法分解

1. 如果$m$是一个偶数,那么我们就使用$2*m-MA$来计算趋势周期成份$\bar{T}_t$,如果$m$是一个奇数,那么我们就使用$m-MA$来计算趋势周期成份$\bar{T}_t$.
2. 计算去趋势序列$y_t - \bar{T}_t$.
3. 为了估计每个季节的季节成份,我们直接对去趋势的数据求均值,例如,计算三月的季节成份,我们就对所有历史上去趋势的三月的数据求均值.季节性的成份可以通过将数据每一年的的值链接起来获得.这样我们就可以得到$\bar{S}_t$.
4. 我们将数据减去趋势周期值和季节值得到我们的剩余部分,$\bar{R}_t = y_t - \bar{T}_t - \bar{S}_t$.

乘法分解：乘法分解和加法分解较大的区别就是将减替换为除即可.

虽然经典的分解方法还常常被使用,但是它却存在非常多的缺点。

1. 趋势周期的估计在一些地方无法估计,例如时间序列的开始和时间序列的结尾处都无法计算,所以相应的,这些点的剩余成份和季节性成份都无法计算.
2. 趋势周的估计很容易太过平滑.
3. 经典的分解假设季节性的成份年年会重复,这对于很多长的时间序列并不是一个非常好的假设.

 监督学习往往需要针对样本数据进行特征和标签的划分，对于单维、多维时间序列以及时序图，其划分的方法都比较类似，具体可以分析如下几种情况： 

![](../picture/1/127.png)

one to one
不论是单维、多维还是时序图，都是把t时刻的数据作为特征，t+1的数据作
为标签，随着t从序列头部到后部不断移动，由此构造出序列的特征和标签。

one to many:
不论是单维、多维还是时序图，都是把t时刻的数据作为特征，t+K的数据作为标签，随着t从序列头部到后部不断移动，由此构造出序列的特征和标签。

many to one:
不论是单维、多维还是时序图，都是把t-k时刻的数据作为特征，t时刻的数据作为标签，随着t从序列头部到后部不断移动，由此构造出序列的特征和标签。

many to many:
不论是单维、多维还是时序图，都是把t-k时刻的数据作为特征，t+K的数据作为标签，随着t从序列头部到后部不断移动，由此构造出序列的特征和标签。

```python
def createSamples(ts, lookBack, lookAhead):
   dataX, dataY = [], []
   for i in range(len(ts) - lookBack - lookAhead):
       history_seq = ts[i: i + lookBack]
       future_seq = ts[i + lookBack: i + lookBack + lookAhead]
       dataX.append(history_seq)
       dataY.append(future_seq)
   dataX = np.array(dataX)
   dataY = np.array(dataY)
   return dataX, dataY
```

```python
def predict_SVR_iteration(testX, lookAhead,svrModel):
   testBatchSize = testX.shape[0]
   ans = []
 
   for i in range(lookAhead):
       pred = svrModel.predict(testX)
       ans.append(pred)
       testX = testX[:, 1:]
       pred = pred.reshape((testBatchSize, 1))
       testX = np.append(testX, pred, axis=1)
   ans = np.array(ans)
   ans = ans.transpose([1, 0])
   return ans
```

