### 基础架构

离线数据仓库到实时数据仓库，从lambda架构到kappa架构、再到混合架构。

![](D:/MarkDown/picture/2/217.png)

![传统数仓架构](D:/MarkDown/picture/2/259.png)

![Lambda机构](D:/MarkDown/picture/2/260.png)

![Kappa架构](D:/MarkDown/picture/2/261.png)

#### Lambda架构

Lambda架构的目标是设计出一个能满足实时大数据系统关键特性的架构，包括有：高容错、低延时和可扩展等。Lambda架构整合离线计算和实时计算，融合不可变性，读写分离和复杂性隔离等一系列架构原则，可集成Hadoop，Kafka，Storm，Spark，Hbase等各类大数据组件。

###### 大数据系统的关键特性

大数据系统应具有以下的关键特性：

![](D:/MarkDown/picture/2/186.png)

##### 数据系统的本质

我们可将数据系统简化为：数据系统=数据+查询

###### 数据的本质

数据是一个不可分割的单位，数据有两个关键的性质：When和What。

![](D:/MarkDown/picture/2/187.png)

根据上述对数据本质特性的分析，Lamba架构中对数据的存储采用的方式是：数据不可变，存储所有数据。

通过采用不可变方式存储所有的数据，可以有如下好处：

![](D:/MarkDown/picture/2/188.png)

###### 查询

查询的定义：$Query=Function(All Data)$，查询是应用于数据集上的函数。

有一类称为Monoid特性的函数应用非常广泛。Monoid的概念来源于范畴学，其一个重要特性是满足结合律。如整数的加法就满足Monoid特性：$(a+b)+c = a+(b+c)$。不满足Monoid特性的函数很多时候可以转化成多个满足Monoid特性的函数的运算。如多个数的平均值Avg函数，多个平均值没法直接通过结合来得到最终的平均值，但是可以拆成分母除以分子，分母和分子都是整数的加法，从而满足Monoid特性。

Monoid的结合律特性在分布式计算中极其重要，满足Monoid特性意味着我们可以将计算分解到多台机器并行运算，然后再结合各自的部分运算结果得到最终结果。同时也意味着部分运算结果可以储存下来被别的运算共享利用（如果该运算也包含相同的部分子运算），从而减少重复运算的工作量。

![](D:/MarkDown/picture/2/185.png)

Lambda架构通过分解的三层架构来解决该问题：Batch Layer(批处理层)，Speed Layer(实时层)和Serving Layer(服务层)。

###### Batch Layer

储存数据集

根据前述对数据When&What特性的讨论，Batch Layer采用不可变模型存储所有的数据。因为数据量比较大，可以采用HDFS之类的大数据储存方案。如果需要按照数据产生的时间先后顺序存放数据，可以考虑如InfluxDB之类的时间序列数据库存储方案。

构建查询View

上面说到根据等式Query = Function(All Data)，在全体数据集上在线运行查询函数得到结果的代价太大。但如果我们预先在数据集上计算并保存查询函数的结果，查询的时候就可以直接返回结果或通过简单的加工运算就可得到结果而无需重新进行完整费时的计算了。这儿可以把Batch Layer看成是一个数据预处理的过程。我们把针对查询预先计算并保存的结果称为View，View是Lamba架构的一个核心概念，它是针对查询的优化，通过View即可以快速得到查询结果。

![](D:/MarkDown/picture/2/189.png)

对View的理解：View是一个和业务关联性比较大的概念，View的创建需要从业务自身的需求出发。一个通用的数据库查询系统，查询对应的函数千变万化，不可能穷举。但是如果从业务自身的需求出发，可以发现业务所需要的查询常常是有限的。Batch Layer需要做的一件重要的工作就是根据业务的需求，考察可能需要的各种查询，根据查询定义其在数据集上对应的Views。

###### Speed Layer

Speed Layer正是用来处理增量的实时数据。Speed Layer和Batch Layer比较类似，对数据进行计算并生成Realtime View，其主要区别在于：Speed Layer处理的数据是最近的增量数据流，Batch Layer处理的全体数据集Speed Layer为了效率，接收到新数据时不断更新Realtime View，而Batch Layer根据全体离线数据集直接得到Batch View。

Lambda架构将数据处理分解为Batch Layer和Speed Layer有如下优点：

容错性。Speed Layer中处理的数据也不断写入Batch Layer，当Batch Layer中重新计算的数据集包含Speed Layer处理的数据集后，当前的Realtime View就可以丢弃，这也就意味着Speed Layer处理中引入的错误，在Batch Layer重新计算时都可以得到修正。这点也可以看成是CAP理论中的最终一致性（Eventual Consistency）的体现。

![](D:/MarkDown/picture/2/190.png)

复杂性隔离。Batch Layer处理的是离线数据，可以很好的掌控。Speed Layer采用增量算法处理实时数据，复杂性比Batch Layer要高很多。通过分开Batch Layer和Speed Layer，把复杂性隔离到Speed Layer，可以很好的提高整个系统的鲁棒性和可靠性。

###### Serving Layer

Lambda架构的Serving Layer用于响应用户的查询请求，合并Batch View和Realtime View中的结果数据集到最终的数据集。如果查询函数满足Monoid性质，只需要简单的合并Batch View和Realtime View中的结果数据集即可。否则的话，可以把查询函数转换成多个满足Monoid性质的查询函数的运算，单独对每个满足Monoid性质的查询函数进行Batch View和Realtime View中的结果数据集合并，然后再计算得到最终的结果数据集。另外也可以根据业务自身的特性，运用业务自身的规则来对Batch View和Realtime View中的结果数据集合并。

![](D:/MarkDown/picture/2/191.png)

下图给出了Lambda架构的一个完整视图和流程。

![](D:/MarkDown/picture/2/192.png)

数据流进入系统后，同时发往Batch Layer和Speed Layer处理。Batch Layer以不可变模型离线存储所有数据集，通过在全体数据集上不断重新计算构建查询所对应的Batch Views。Speed Layer处理增量的实时数据流，不断更新查询所对应的Realtime Views。Serving Layer响应用户的查询请求，合并Batch View和Realtime View中的结果数据集到最终的数据集。

数据流存储可选用基于不可变日志的分布式消息系统Kafka；Batch Layer数据集的存储可选用Hadoop的HDFS，或者是阿里云的ODPS；Batch View的预计算可以选用MapReduce或Spark；Batch View自身结果数据的存储可使用MySQL（查询少量的最近结果数据），或HBase（查询大量的历史结果数据）。Speed Layer增量数据的处理可选用Storm或Spark Streaming；Realtime View增量结果数据集为了满足实时更新的效率，可选用Redis等内存NoSQL。

Kappa架构  简化了Lambda架构。Kappa架构系统是删除了批处理系统的架构。要取代批处理，数据只需通过流式传输系统快速提供：

![](D:/MarkDown/picture/2/229.png)

那如何用流计算系统对全量数据进行重新计算，步骤如下：

1、用Kafka或类似的分布式队列保存数据，需要几天数据量就保存几天。

2、当需要全量计算时，重新起一个流计算实例，从头开始读取数据进行处理，并输出到一个结果存储中。

3、当新的实例完成后，停止老的流计算实例，并把老的一引起结果删除。

一个典型的Kappa架构如下：

![](D:/MarkDown/picture/2/231.png)

和Lambda架构相比，在Kappa架构下，只有在有必要的时候才会对历史数据进行重复计算，并且实时计算和批处理过程使用的是同一份代码。或许有些人会质疑流式处理对于历史数据的高吞吐量会力不从心，但是这可以通过控制新实例的并发数进行改善。

  Kappa架构的核心思想包括以下三点：用Kafka或者类似的分布式队列系统保存数据，你需要几天的数据量就保存几天；当需要全量重新计算时，重新起一个流计算实例，从头开始读取数据进行处理，并输出到一个新的结果存储中；当新的实例做完后，停止老的流计算实例，并把老的一些结果删除。

数据架构经历了从最初的离线大数据架构、Lambda 架构、Kappa 架构以及 Flink 的火热带出的流批一体架构，数据架构技术不断演进，本质是在往流批一体的方向发展，让用户能以最自然、最小的成本完成实时计算。

- 离线大数据架构：数据源通过离线的方式导入到离线数仓中，下游应用根据业务需求选择直接读取 DM 或加一层数据服务，比如 MySQL 或 Redis，数据存储引擎是 HDFS/Hive，ETL 工具可以是 MapReduce 脚本或 HiveSQL。数据仓库从模型层面分为操作数据层 ODS、数据仓库明细层 DWD、数据集市层 DM；
- Lambda 架构：随着大数据应用的发展，人们逐渐对系统的实时性提出了要求，为了计算一些实时指标，就在原来离线数仓的基础上增加了一个实时计算的链路，并对数据源做流式改造（即把数据发送到消息队列），实时计算去订阅消息队列，直接完成指标增量的计算，推送到下游的数据服务中去，由数据服务层完成离线&实时结果的合并；
- Kappa 架构：Lambda 架构虽然满足了实时的需求，但带来了更多的开发与运维工作，其架构背景是流处理引擎还不完善，流处理的结果只作为临时的、近似的值提供参考。后来随着 Flink 等流处理引擎的出现，流处理技术成熟起来，这时为了解决两套代码的问题，LickedIn 的 Jay Kreps 提出了 Kappa 架构；
- 流批一体架构：流批一体架构比较完美的实现方式是采用流计算 + 交互式分析双引擎架构，在这个架构中，流计算负责的是基础数据，而交互式分析引擎是中心，流计算引擎对数据进行实时 ETL 工作，与离线相比，降低了 ETL 过程的 latency，交互式分析引擎则自带存储，通过计算存储的协同优化， 实现高写入 TPS、高查询 QPS 和低查询 latency ，从而做到全链路的实时化和 SQL 化，这样就可以用批的方式实现实时分析和按需分析，并能快速的响应业务的变化，两者配合，实现 1 + 1 > 2 的效果； 该架构对交互式分析引擎的要求非常高，也许是未来大数据库技术发展的一个重点和方向。

![](D:/MarkDown/picture/2/239.png)

##### 数据仓库分层模型

数据从多个源头采集上来之后，格式化便成为了原始数据。**原始数据经过MR的开发之后，生成了各个报表数据**，然后统一导入到Mysql或者Oracle平台之后，便可以直接看到报表

ODS：通常而言，**原始数据的种类是非常丰富的**，我们可能从几十个业务方把数据拉回来，然后格式化放到HDFS上。但很多时候，情况并不这么简单，虽然有很多的损坏数据、脏数据等是不需要统计的，但是我们需要来看**为什么会产生脏数据**，这时候原始数据就会提供很好的样板。再有些时候，针对一些流量作弊的数据，如果按照**统一规则**，很容易就给过滤掉了，然后运营就问过来为什么对方提供的数据与我们的差异这么多大，这时候同样需要去看原始日志。因而，ODS的意义，在于保存最完整的数据现场，便于一些特殊场景下的问题排查使用。

- DWD：如果采集的数据没有问题了，我们这里就需要做数据的预处理了。通常情况下，预处理指将数据变成**半格式化或者是格式化数据**，例如存在HDFS上的标准格式，我们就用字符串的格式来统一存储。还有时候因为场景要求，需要直接转成**Parquent等列存格式**，也需要在这里做转换。但预处理并不是简单的转换格式，还需要处理一些脏数据，例如字段缺失、格式错误、乱码、空值，等等，在这一层处理好之后，后续的计算便不需要再担心各种各样的异常情况，对于开发效率的提升有着极大的帮助。有些时候还要发挥一些特定作用，因为业务的意外导致各种各样的错误数据进来，也是时有发生的。比如客户消费了，金额总得是正的吧，但如果业务那边产生了一些错误，需要将金额设置成负值，虽然业务那边好处理了，但数据这里就头疼了。所以还需要经常打一些补丁，来处理金额负值这种异常情况。还有各式各样的反作弊要求，也是需要在DWD进行处理的。所以看起来DWD像是多余的一层，但当业务场景足够复杂之后，它所发挥的作用还是很大的。这里数据预处理主要采用MR来进行，基本上遇不到数据倾斜等问题。
- DWS：当所有的数据都存好了，处理完脏数据之后，下一步我们就需要考虑**如何处理和组织统计逻辑**了。数据仓库之所以叫数据仓库，正是因为DWS层的重要。数据模型有很多，如：**范式模型、维度模型、Data Vault**等，但最常用的还是**星型模型**。通常我们会根据主题来进行表数据的统计，这里还有一个常用的说法，叫“中间层”。例如我们数据层次自上往下分别是：用户、广告投放计划、计划详情，用户本身有行业、主体公司等属性，广告投放计划包括了单元、创意等属性，计划详情包括了投放类型、投放地域等属性。那么我们在这个DWS层，就需要针对所有可能的维度，包括用户、行业、主体公司、广告投放计划、单元、创意、计划详情、投放类型、投放地域做统计，每个类型都尽可能的冗余维度信息，例如用户维度的统计要把行业、主体公司等维度冗余进来，放到一张表里。这么做虽然特别违反三范式的原则，也违反很多模型，但是冗余尽可能多的信息，对于**提高下游计算的速度、减少运算数据量、简化业务逻辑、合并计算单元**等具有特别大的好处。当然困难也是显而易见的，计算速度慢、数据倾斜等问题也都基本上集中在这个DWS层上。可以说，这里设计好了，整个数据仓库的设计就成功了一半。
- ADS：当需求足够多时，我们要提供的报表就不是几十张的概念了，而是成百上千张，这么多的表怎么保证**数据的一致性**呢？怎么保证**需求响应的速度**呢？基本上都是ADS层需要面临的问题。在前一个层次DWS中，我们把所有的主题都尽可能多的冗余了维度信息，因此这里需要尽量从单一中间层表中进行数据统计，中间层的数据一致性，就代表了最终业务数据的一致性。响应速度同理，在某些不得不关联的业务场景下，因为中间层的存在，使得数据量减少了很多，需求响应速度也就提升了很多。

DIM：维度信息

###### 挑战

在数据分层理论中，尽管理解不是什么难事，但在实际应用中的技术挑战是非常大的。这里简单列几个：

1. 关联范围广：在很多时候，有些数据是需要跨多个业务线的，每个业务线的数据都很大，这时候不仅是计算逻辑复杂无比，一个SQL几百行，而且对于数据倾斜的问题挑战更大，Hive运算的时间也非常长。这种情况下需要适当考虑在运算节点中**加入一些MR的运算过程**，以提高计算速度，单纯的优化Hive SQL并不是一个好主意。
2. 血缘关系乱：尽管DWS是统计中间层的数据，但由于业务的变化多种多样，一个中间层需要关联几张甚至十几张表，每张表都有自身的业务逻辑，关联很多，这就导致了一张完整的中间表上游特别多，发现某个数据异常时非常难以追溯问题。这时候你需要额外的技术支持：**元数据平台**，通过分析这张表的上游关联关系，来进行问题的定位。元数据的问题等到后续再统一讲解。
3. 产出时间长：某些DWS表动辄需要几个小时的计算时间，对于数据的准时产出影响很大。同时如果需要做小时级的报表统计，那么太过于复杂的中间层设计就显得很累赘。建议这个过程有产品经理的介入，以**梳理需求的重要性和优先级**，如果非必要统计，尽量的就不要做中间层，开放一些sql查询的权限也是可以的，这里做好数据安全管理即可。
4. 重构难度大：分层理论尽管听上去容易理解，但真的需要到这个理论时，你所搭建的数据平台势必已经非常大了，而需要适应这套理论，原有的统计逻辑大多数都要重写，这里花上几个月的时间都是很常见的，并且很可能需要双平台同时进行数据计算，以渡过重构的不稳定期。这个阶段的挑战就是**如何解释投入产出比**，要有充分的的信心，详情这项工作完成后，节省的开发时间至少是一个数量级的。原来1天的开发工作，因为有了数据分层，1小时甚至几分钟，都是可以开发完的。