| 算法         | 实现         | 源表要求   | 描述                                         | 适用                   |
| ------------ | ------------ | ---------- | -------------------------------------------- | ---------------------- |
| 全删全插     | `del/ins`    | 全量       | 不保留过往数，只保留当前最新全量数据         | 维度表、参数表、主档表 |
| 增量累全     | `upsert`     | 增量或全量 | 只保留一份最新且保留源系统过往记录数据       | 参数表、主档表         |
| 增量累加     | `append`     | 增量       | 源系统提供每日事件流水，仓库直接保留每日流水 | 流水表                 |
| 标准拉链     | 全历史拉链   | 全量       | 跟踪数据删除的变化历史                       | 拉链表                 |
| 增量拉链     | 增量拉链     | 增量或全量 | 记录数据变化历史                             | 拉链表                 |
| 增删拉链     | 增删拉链     | 增量       | 利用业务字段跟踪增量数据中包含删除的变化历史 | 拉链表                 |
| 全量增删拉链 | 全量增删拉链 | 全量       | 利用业务字段跟踪全量数据中包含删除的变化历史 | 拉链表                 |
| 自拉链       | 自拉链       | 全量或增量 | 流水表数据转化成拉链表数据                   | 拉链表                 |

###### 全删全插模式

主要应用在维表、参数表、主档表加载上，即适合源表是全量数据表，该数据表业务逻辑只需保存当前最新全量数据，不需跟踪过往历史信息。算法实现逻辑：清空目标表；源表全量插入；

```sql
--   1. 清理目标表
TRUNCATE TABLE <目标表>;
 
--   2. 全量插入
INSERT INTO <目标表>   (字段***)
SELECT 字段***
FROM <源表>
***JOIN <关联数据>
WHERE   ***;
```

###### 增量累全模式

主要应用在参数表、主档表加载上，即源表可以是增量或全量数据表，目标表始终最新最全记录。

算法实现逻辑：利用PK主键比对；目标表和源表PK一致的变化记录，更新目标表；源表存在但目标表不存在，直接插入；

```sql
-- 1. 生成加工源表
Create temp Table <临时表> ***;
INSERT INTO <临时表> (字段***)
SELECT 字段***  
FROM <源表>
***JOIN <关联数据>
WHERE ***;
 
-- 2. 可利用Merge Into实现累全能力，当前也可以采用分步Delete/Insert或Update/Insert操作
Merge INTO <目标表> As T1 (字段***)
Using <临时表> as S1
on (***PK***)
when Matched then
update set Colx = S1.Colx ***
when Not Matched then
INSERT (字段***)   values (字段*** );
```

###### 增量累加模式

主要应用在流水表加载上，即每日产生的流水、事件数据，追加到目标表中保留全历史数据。流水表、快照表、统计分析表等均是通过该逻辑实现。

算法实现逻辑：源表直接插入目标表；

```sql
--   1.插入目标表
INSERT INTO <目标表>   (字段***)
SELECT 字段***
FROM <源表>	
***JOIN <关联数据>
WHERE   ***;
```

###### 拉链表

要确定拉链表的时间粒度，比如说拉链表每天只取一个状态，也就是说如果一天有3个状态变更，我们只取最后一个状态。假设我们每天都会获得一份切片数据，我们可以通过取两天切片数据的不同来作为每日更新表，这种情况下我们可以对所有的字段先进行concat，再取md5

```
#ods层的user表
CREATE EXTERNAL TABLE ods.user (
  user_num STRING COMMENT '用户编号',
  mobile STRING COMMENT '手机号码',
  reg_date STRING COMMENT '注册日期'
COMMENT '用户资料表'
PARTITIONED BY (dt string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n'
STORED AS ORC
LOCATION '/ods/user';
)

#user_update表
CREATE EXTERNAL TABLE ods.user_update (
  user_num STRING COMMENT '用户编号',
  mobile STRING COMMENT '手机号码',
  reg_date STRING COMMENT '注册日期'
COMMENT '每日用户资料更新表'
PARTITIONED BY (dt string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n'
STORED AS ORC
LOCATION '/ods/user_update';
)

#用户拉链表
CREATE EXTERNAL TABLE dws.user_his (
  user_num STRING COMMENT '用户编号',
  mobile STRING COMMENT '手机号码',
  reg_date STRING COMMENT '用户编号',
  t_start_date ,
  t_end_date
COMMENT '用户资料拉链表'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n'
STORED AS ORC
LOCATION '/dws/user_his';
)
```

假设已经初始化了2017-01-01的日期，然后需要更新2017-01-02那一天的数据

```SPARQL
INSERT OVERWRITE TABLE dws.user_his
SELECT * FROM
(  SELECT A.user_num,
           A.mobile,
           A.reg_date,
           A.t_start_time,
           CASE
                WHEN A.t_end_time = '9999-12-31' AND B.user_num IS NOT NULL THEN '2017-01-01'
                ELSE A.t_end_time
           END AS t_end_time
    FROM dws.user_his AS A
    LEFT JOIN ods.user_update AS B
    ON A.user_num = B.user_num
UNION
    SELECT C.user_num,
           C.mobile,
           C.reg_date,
           '2017-01-02' AS t_start_time,
           '9999-12-31' AS t_end_time
    FROM ods.user_update AS C
) AS T
```

分区方式
方式1（使用start_day作为分区键）：缺点是查询最新数据无法走分区；查询某一天数据时end_day限制条件无法走分区；加工历史拉链表数据时，end_day=30001231的结果数据不方便入库
方式2（使用end_day作为分区键）：缺点是查询某一天数据时start_day限制条件无法走分区；优点是加工历史拉链表数据时，结果数据入库方便，直接insert overwrite覆盖分区30001231和20200201即可
方式3（使用start_day和end_day作为联合分区键，start_day为父分区）：查询最新数据时需要改变下SQL语句，不然无法走分区（比如当前日期是20200401，SQL语句需改为select * from table_name where start_day <= ‘20200401’ and end_day > '20200401'，即查询某一天数据的写法）；缺点是加工历史拉链表数据时，end_day=30001231和end_day=20200201的结果数据都不方便入库；而且分区数会越来越多，一年下来最多可能产生365*364/2=66430个分区；优点是查询数据时start_day和end_day的限制条件都可以走分区
方式4（使用start_day和end_day作为联合分区键，end_day为父分区）：缺点同方式3，但加工历史拉链表数据时，结果数据入库相对方便（首先将结果数据存入临时表，然后清空拉链表的分区end_day=30001231和end_day=20200201，最后将临时表数据以insert into方式入库）；优点同方式3
综上所述，分区方式可在2和4中选择。

选择方式2，需要考虑随着时间的推移，查询某一天的维度状态数据，消耗的计算资源会越来越多。可考虑删除或者备份部分历史数据至其他地方。
选择方式4，需要考虑随着时间的推移，分区数量会越来越多。可考虑定期重构历史拉链表，比如在每个月月初强制重新开始做历史拉链表（比如在20200401时，先将end_day=30001231的数据修改为end_day=20200401，再基于最新全量快照表生成一份start_day=20200401，end_day=30001231的数据）。



###### 全历史拉链模式

拉链表是一张至少存在PK字段、跟踪变化的字段、开链日期、闭链日期组成的数据仓库ETL数据表；全历史拉链，跟踪源表全量变化历史，若源表记录不存在，则说明数据闭链；根据PK新拉一条有效记录。

算法实现逻辑：提取当前有效记录；提取当日源系统最新数据；根据PK字段比对当前有效记录与最新源表，更新目标表当前有效记录，进行闭链操作；根据全字段比对最新源表与当前有效记录，插入目标表；

```sql
-- 1. 提取当前有效记录
Insert into <临时表-开链-pre> (不含开闭链字段***)
Select 不含开闭链字段***
From <目标表>
Where 结束日期 =date'<最大日期>';
;
-- 2. 提取当日源系统最新数据
<源表临时表-cur>
-- 3 今天全部开链的数据，即包含今天全新插入、数据发生变化的记录
Insert Into <临时表-增量-ins>
Select 不含开闭链字段***
From <源表临时表-cur>
where (不含开闭链字段***) not in
   (Select 不含开闭链字段***
 From <临时表-开链-pre>
   );
-- 4 今天需要闭链的数据，即今天发生变化的记录
Insert into <临时表-增量-upd>
Select 不含开闭链字段***,开始时间
From <临时表-开链-pre>
where (不含开闭链字段***) not in
   (Select 不含开闭链字段***
 From <临时表-开链-cur>
   );
-- 5 更新闭链数据，即历史记录闭链（删除-插入替代更新）
DELETE FROM <目标表>
WHERE (PK***) IN
(Select PK*** From <临时表-增量-upd>)
AND 结束日期=date'<最大日期>';
INSERT INTO <目标表>
      (不含开闭链字段***,开始时间,结束日期)
Select 不含开闭链字段***,开始时间,date'<数据日期>'
From <临时表-增量-upd>;
-- 6 插入开链数据，即当日新增记录
INSERT INTO <目标表> .
      (不含开闭链字段***,开始时间,结束日期)
Select 不含开闭链字段***,date'<数据日期>',date'<最大日期>'
From <临时表-增量-ins>;
```

###### 增量拉链模式

增量拉链，目的是追踪数据增量变化历史，根据PK比对新拉一条开链数据；

算法实现逻辑：提取上日开链数据；PK相同变化记录，关闭旧记录链，开启新记录链；PK不同，源表存在，新增开链记录

```sql
--   1. 提取当前有效记录
Insert into <临时表-开链-pre> (不含开闭链字段***)
Select 不含开闭链字段***
From <目标表>
Where 结束日期 =date'<最大日期>';
--   2. 提取当日源系统增量记录
<源表临时表-cur>
--   3. 提取当日源系统新增记录
Insert into <临时表-增量-ins>
Select 不含开闭链字段***
From <临时表-开链-cur>
where (***PK***) not in
  (select ***PK*** from <临时表-开链-pre>);
--   4. 提取当日源系统历史变化记录
Insert into <临时表-增量-upd>
Select 不含开闭链字段***
From <临时表-开链-cur>
inner join <临时表-开链-pre>
on (***PK 等值***)
where (***变化字段 非等值***);
--   5. 更新历史变化记录，关闭历史旧链，开启新链
update <目标表> AS T1
SET <***变化字段 S1赋值***>,结束日期 = date'<数据日期>'
FROM <临时表-增量-upd> AS S1
WHERE ( <***PK 等值***> )
AND   T1.结束日期 =date'<最大日期>'
;
INSERT INTO <目标表>
      (不含开闭链字段***,开始时间,结束日期)
SELECT 不含开闭链字段***,date'<数据日期>',date'<最大日期>'
FROM <临时表-增量-upd>;
--   6. 插入全新开链数据
INSERT INTO <目标表>
      (不含开闭链字段***,开始时间,结束日期)
SELECT 不含开闭链字段***,date'<数据日期>',date'<最大日期>'
FROM <临时表-增量-ins>;
```

###### 增删拉链模型

主要是利用业务字段跟踪增量数据中包含删除的变化历史。

算法实现逻辑：提取上日开链数据；提取源表非删除记录；PK相同变化记录，关闭旧记录链，开启新记录链；PK比对，源表存在，新增开链记录；提取源表删除记录；PK比对，旧开链记录存在，关闭旧记录链；

###### 全量增删拉链模型

主要是利用业务字段跟踪全量数据中包含删除的变化历史。

算法实现逻辑：提取上日开链数据；提取源表非删除记录；PK相同变化记录，关闭旧记录链，开启新记录链；PK比对，源表存在，新增开链记录；提取源表删除记录；PK比对，旧开链记录存在，关闭旧记录链；PK比对，提取旧开链存在但源表不存在记录，关闭旧记录链；

###### 自拉链模型

主要将流水表数据转化成拉链表数据。算法实现逻辑：借助源表业务日期字段，和目标表开链、闭链日期比对，首尾相接，拉出全历史拉链；

#### 数据同步方案

##### 关系型数据库之间

全量同步，比如从oracle数据库中同步一张表的数据到Mysql中，通常的做法就是 分页查询源端的表，然后通过 jdbc的batch 方式插入到目标表，这个地方需要注意的是，分页查询时，一定要按照主键id来排序分页，避免重复插入。

![](../picture/2/339.png)

基于数据文件导出和导入的全量同步，这种同步方式一般只适用于同种数据库之间的同步，如果是不同的数据库，这种方式可能会存在问题。

###### 基于触发器的增量同步

增量同步一般是做实时的同步，早期很多数据同步都是基于关系型数据库的触发器trigger来做的。使用触发器实时同步数据的步骤：

A、 基于原表创触发器，触发器包含insert，modify，delete 三种类型的操作，数据库的触发器分Before和After两种情况，一种是在insert，modify，delete 三种类型的操作发生之前触发（比如记录日志操作，一般是Before），一种是在insert，modify，delete 三种类型的操作之后触发。

B、 创建增量表，增量表中的字段和原表中的字段完全一样，但是需要多一个操作类型字段（分表代表insert，modify，delete 三种类型的操作），并且需要一个唯一自增ID，代表数据原表中数据操作的顺序，这个自增id非常重要，不然数据同步就会错乱。

C、 原表中出现insert，modify，delete 三种类型的操作时，通过触发器自动产生增量数据，插入增量表中。

D、处理增量表中的数据，处理时，一定是按照自增id的顺序来处理，这种效率会非常低，没办法做批量操作，不然数据会错乱。 有人可能会说，是不是可以把insert操作合并在一起，modify合并在一起，delete操作合并在一起，然后批量处理，我给的答案是不行，因为数据的增删改是有顺序的，合并后，就没有顺序了，同一条数据的增删改顺序一旦错了，那数据同步就肯定错了。

![](../picture/2/340.png)

###### 基于时间戳的增量同步

A、首先我们需要一张临时temp表，用来存取每次读取的待同步的数据，也就是把每次从原表中根据时间戳读取到数据先插入到临时表中，每次在插入前，先清空临时表的数据

B、我们还需要创建一个时间戳配置表，用于存放每次读取的处理完的数据的最后的时间戳。

C、每次从原表中读取数据时，先查询时间戳配置表，然后就知道了查询原表时的开始时间戳。

D、根据时间戳读取到原表的数据，插入到临时表中，然后再将临时表中的数据插入到目标表中。

E、从缓存表中读取出数据的最大时间戳，并且更新到时间戳配置表中。缓存表的作用就是使用sql获取每次读取到的数据的最大的时间戳，当然这些都是完全基于sql语句在kettle中来配置，才需要这样的一张临时表。

![](../picture/2/341.png)

##### 大数据时代下的数据同步

###### 基于数据库日志的同步

数据库都支持了主从自动同步，尤其是mysql，可以支持多主多从的模式。mysql的主从同步的过程是这样的。

A、master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events，可以通过show binlog events进行查看）；

B、slave将master的binary log events拷贝到它的中继日志(relay log)；

C、slave重做中继日志中的事件，将改变反映它自己的数据。

阿里巴巴开源的canal就完美的使用这种方式，canal 伪装了一个Slave 去喝Master进行同步。

A、 canal模拟mysql slave的交互协议，伪装自己为mysql slave，向mysql master发送dump协议

B、 mysql master收到dump请求，开始推送binary log给slave(也就是canal)

C、 canal解析binary log对象(原始为byte流)

![](../picture/2/342.png)

###### 基于BulkLoad的数据同步

比如从hive同步数据到hbase。我们有两种方式可以实现，

A、 使用spark任务，通过HQl读取数据，然后再通过hbase的Api插入到hbase中。

但是这种做法，效率很低，而且大批量的数据同时插入Hbase，对Hbase的性能影响很大。

在大数据量的情况下，使用BulkLoad可以快速导入，BulkLoad主要是借用了hbase的存储设计思想，因为hbase本质是存储在hdfs上的一个文件夹，然后底层是以一个个的Hfile存在的。HFile的形式存在。

B、 BulkLoad实现的原理就是按照HFile格式存储数据到HDFS上，生成Hfile可以使用hadoop的MapReduce来实现。如果不是hive中的数据，比如外部的数据，那么我们可以将外部的数据生成文件，然后上传到hdfs中，组装RowKey，然后将封装后的数据在回写到HDFS上，以HFile的形式存储到HDFS指定的目录中。

#### 数据脱敏

数据脱敏也叫数据的去隐私化，在我们给定脱敏规则和策略的情况下，对敏感数据比如 `手机号`、`银行卡号` 等信息，进行转换或者修改的一种技术手段，防止敏感数据直接在不可靠的环境下使用。

数据脱敏又分为静态数据脱敏`SDM`和 动态数据脱敏`DDM`

静态数据脱敏（`SDM`）：适用于将数据抽取出生产环境脱敏后分发至测试、开发、数据分析等场景。脱敏后的数据与生产环境隔离，满足业务需要的同时又保障了生产数据的安全。

动态数据脱敏（`DDM`）：一般用在生产环境，访问敏感数据时实时进行脱敏，因为有时在不同情况下对于同一敏感数据的读取，需要做不同级别的脱敏处理，例如：不同角色、不同权限所执行的脱敏方案会不同。

在抹去数据中的敏感内容同时，也需要保持原有的数据特征、业务规则和数据关联性，保证开发、测试以及数据分析类业务不会受到脱敏的影响，使脱敏前后的数据一致性和有效性。

##### 脱敏方案

###### 无效化

无效化方案在处理待脱敏的数据时，通过对字段数据值进行 `截断`、`加密`、`隐藏` 等方式让敏感数据脱敏，使其不再具有利用价值。一般采用特殊字符（`*`等）代替真值，这种隐藏敏感数据的方法简单，但缺点是用户无法得知原数据的格式，如果想要获取完整信息，要让用户授权查询。

###### 随机值

随机值替换，字母变为随机字母，数字变为随机数字，文字随机替换文字的方式来改变敏感数据，这种方案的优点在于可以在一定程度上保留原有数据的格式，往往这种方法用户不易察觉的。

###### 数据替换

数据替换与前边的无效化方式比较相似，不同的是这里不以特殊字符进行遮挡，而是用一个设定的虚拟值替换真值。比如说我们将手机号统一设置成 “13651300000”。

###### 对称加密

对称加密是一种特殊的可逆脱敏方法，通过加密密钥和算法对敏感数据进行加密，密文格式与原始数据在逻辑规则上一致，通过密钥解密可以恢复原始数据，要注意的就是密钥的安全性。

###### 平均值

平均值方案经常用在统计场景，针对数值型数据，我们先计算它们的均值，然后使脱敏后的值在均值附近随机分布，从而保持数据的总和不变。

###### 偏移和取整

这种方式通过随机移位改变数字数据，偏移取整在保持了数据的安全性的同时保证了范围的大致真实性，比之前几种方案更接近真实数据，在大数据分析场景中意义比较大。

比如把`create_time`中 `2020-12-08 15:12:25`变为 `2018-01-02 15:00:00`。