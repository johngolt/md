### 推荐算法评估

#### 推荐系统实验

推荐系统中，主要有3种评测推荐效果的实验方法，即离线实验、用户调查和在线实验。

##### 离线实验

离线实验的方法一般由如下几个步骤构成： (1) 通过日志系统获得用户行为数据，并按照一定格式生成一个标准的数据集； (2) 将数据集按照一定的规则分成训练集和测试集； (3) 在训练集上训练用户兴趣模型，在测试集上进行预测； (4) 通过事先定义的离线指标评测算法在测试集上的预测结果。 

从上面的步骤可以看到，推荐系统的离线实验都是在数据集上完成的，也就是说它不需要一 个实际的系统来供它实验，而只要有一个从实际系统日志中提取的数据集即可。这种实验方法的 好处是不需要真实用户参与，可以直接快速地计算出来，从而方便、快速地测试大量不同的算法。

它的主要缺点是无法获得很多商业上关注的指标，如点击率、转化率等，而找到和商业指标非常 相关的离线指标也是很困难的事情。

##### 用户调查

离线实验的指标和实际的商业指标存在差距。因此，如果要准确评测一个算法，需要相对比较真实的环境。最好的方法就是将算法直接上线测试，但在对算法会不会降低用户满意度不太有把握的情况下，上线测试具有较高的风险，所以在上线测试前一般需要做一次称为用户调查的测试。 

用户调查需要有一些真实用户，让他们在需要测试的推荐系统上完成一些任务。在他们完成任务时，我们需要观察和记录他们的行为，并让他们回答一些问题。最后，我们需要通过分析他们的行为和答案了解测试系统的性能

用户调查是推荐系统评测的一个重要工具，很多离线时没有办法评测的与用户主观感受有关 的指标都可以通过用户调查获得。

但是，用户调查也有一些缺点。首先，用户调查成本很高，需要用户花大量 时间完成一个个任务，并回答相关的问题。有些时候，还需要花钱雇用测试用户。因此，大多数情况下很难进行大规模的用户调查，而对于参加人数较少的用户调查，得出的很多结论往往 没有统计意义。此外，测试用户也不是随便选择的。需要尽量保证测试用户的分布和真实用户的分布相同， 比如男女各半，以及年龄、活跃度的分布都和真实用户分布尽量相同。此外，用户调查要尽量保证是双盲实验，不要让实验人员和用户事先知道测试的目标，以免用户的回答和实验人员的测试受主观成分的影响

它的优点是可以获得很多体现用户主观感受的指标，相对在线 实验风险很低，出现错误后很容易弥补。缺点是招募测试用户代价较大，很难组织大规模的测试 用户，因此会使测试结果的统计意义不足。此外，在很多时候设计双盲实验非常困难，而且用户 在测试环境下的行为和真实环境下的行为可能有所不同，因而在测试环境下收集的测试指标可能 在真实环境下无法重现

##### 在线实验

在完成离线实验和必要的用户调查后，可以将推荐系统上线做AB测试，将它和旧的算法进行比较。 AB测试是一种很常用的在线评测算法的实验方法。它通过一定的规则将用户随机分成几组， 并对不同组的用户采用不同的算法，然后通过统计不同组用户的各种不同的评测指标比较不同算 法，比如可以统计不同组用户的点击率，通过点击率比较不同算法的性能。

AB测试的优点是可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标。 AB测试的缺点主要是周期比较长，必须进行长期的实验才能得到可靠的结果。因此一般不会用 AB测试测试所有的算法，而只是用它测试那些在离线实验和用户调查中表现很好的算法。其次， 一个大型网站的AB测试系统的设计也是一项复杂的工程。一个大型网站的架构分前端和后端， 从前端展示给用户的界面到最后端的算法，中间往往经过了很多层，这些层往往由不同的团队控 制，而且都有可能做AB测试。如果为不同的层分别设计AB测试系统，那么不同的AB测试之间往 往会互相干扰。比如，当我们进行一个后台推荐算法的AB测试，同时网页团队在做推荐页面的 界面AB测试，最终的结果就是你不知道测试结果是自己算法的改变，还是推荐界面的改变造成 的。因此，切分流量是AB测试中的关键，不同的层以及控制这些层的团队需要从一个统一的地 方获得自己AB测试的流量，而不同层之间的流量应该是正交的

#### 评价指标

##### 用户满意度

用户作为推荐系统的重要参与者，其满意度是评测推荐系统的最重要指标。但是，用户满意 度没有办法离线计算，只能通过用户调查或者在线实验获得。用户调查获得用户满意度主要是通过调查问卷的形式。用户对推荐系统的满意度分为不同的 层次。

在在线系统中，用户满意度主要通过一些对用户行为的统计得到。比如在电子商务网站中， 用户如果购买了推荐的商品，就表示他们在一定程度上满意。因此，我们可以利用购买率度量用 户的满意度。此外，有些网站会通过设计一些用户反馈界面收集用户满意度。更一般的情 况下，我们可以用点击率、用户停留时间和转化率等指标度量用户的满意度。

##### 预测准确度

预测准确度度量一个推荐系统或者推荐算法预测用户行为的能力。这个指标是最重要的推荐 系统离线评测指标。

对于推荐最常用的评估机制如下：假设我们有$n$个商品需要推荐,给定一个$x$用户,我们设计了一个推荐算法$A$, 会返回给我们一个$n$个商品的排序列表, $R(A,x)\subset\{1,2,\cdots,n\}$, $R$表示预测的排序。例如$R(A,x)=\{3,5\}$表示算法$A$会给$x$推荐两个相关的商品,并且推荐的位置为3和5.然后我们使用某个评估指标$M$对我们的推荐进行评估,一般我们会对一个集合$D=\{x_1,x2,\cdots\}$求个均值：$\frac{1}{|D|}\sum_{x\in D}M(R(A,x))$

###### 评分预测

对于测试集中的一个用户$u$和物品$i$，令$r_{ui}$是用户$u$对物品$i$的实际评分，而$\hat{r}_{ui}$是推荐算法给出的预测评分。评分预测的预测准确度一般通过均方根误差和平均绝对误差计算
$$
\text{RMSE}=\frac{\sqrt{\sum_{u,i\in T}(r_{ui}-\hat{r}_{ui})^2}}{|T|}\\
\text{MAE}=\frac{\sum_{u,i\in T}|r_{ui}-\hat{r}_{ui}|}{|T|}
$$

###### `TopN`推荐

网站在提供推荐服务时，一般是给用户一个个性化的推荐列表，这种推荐叫做`TopN`推荐。`TopN`推荐的预测准确率一般通过准确率/召回率度量。令$R(u)$是根据用户在训练集上的行为给用户作出的推荐列表，而$T(u)$是用户在测试集上的行为列表。
$$
\text{Recall}=\frac{\sum_{u\in U}|R(u)\cap T(u)|}{\sum_{u\in U}|T(u)|}\\
\text{Precision}=\frac{\sum_{u\in U}|R(u)\cap T(u)|}{\sum_{u\in U}|R(u)|}
$$


##### 覆盖率

覆盖率描述一个推荐系统对物品长尾的发掘能力。覆盖率有不同的定义方法， 最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例。假设系统的用户集合为$U$， 推荐系统给每个用户推荐一个长度为$N$的物品列表$R(u)$。那么推荐系统的覆盖率：
$$
\text{Coverage}=\frac{|\cup_{u\in U}R(u)|}{|I|}
$$
覆盖率为100%的系统可以有无数的物品流行度分布。为了更细 致地描述推荐系统发掘长尾的能力，需要统计推荐列表中不同物品出现次数的分布。如果所有的 物品都出现在推荐列表中，且出现的次数差不多，那么推荐系统发掘长尾的能力就很好。因此， 可以通过研究物品在推荐列表中出现次数的分布描述推荐系统挖掘长尾的能力。如果这个分布比 较平，那么说明推荐系统的覆盖率较高，而如果这个分布较陡峭，说明推荐系统的覆盖率较低。信息熵可以用来定义覆盖率
$$
H=-\sum_{i=1}^np(i)\log p(i)
$$
这里$p(i)$是物品$i$的流行度除以所有物品流行度之和。基尼系数用来定义覆盖率
$$
G=\frac{1}{n-1}\sum_{j=1}^n(2j-n-1)p(i_j)
$$
$i_j$是按照物品流行度$p(\cdot)$从小到大排序的物品列表中第$j$个物品。

如果一个系统会增 大热门物品和非热门物品的流行度差距，让热门的物品更加热门，不热门的物品更加不热门，那 么这个系统就有马太效应。评测推荐系统是否具有马太效应的简单办法就是使用基尼系数。如 果G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流 行度的基尼系数，那么如果G2 > G1，就说明推荐算法具有马太效应。

##### 多样性

多样性描述了推荐列表中物品两两之间的不相似性。因此，多样性和相似性是对应的。假设$s(i,j)\in [0,1]$定义了物品$i$和$j$之间的相似度，那么用户$u$的推荐列表$R(u)$的多样性定义如下：
$$
\text{Diversity}=1-\frac{\sum_{i,j\in R(u),i\ne j}s(i,j)}{\frac{1}{2}|R(u)|(|R(u)|-1)}
$$
而推荐系统的整体多样性可以定义为所有用户推荐列表多样性的平均值
$$
\text{Diversity}=\frac{1}{|U|}\sum_{u\in U}\text{Diversity}(R(u))
$$

##### 新颖性

新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。在一个网站中实现新颖性的最 简单办法是，把那些用户之前在网站中对其有过行为的物品从推荐列表中过滤掉。比如在一个视 频网站中，新颖的推荐不应该给用户推荐那些他们已经看过、打过分或者浏览过的视频。

##### 惊喜度

如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个 推荐结果

##### 信任度

对于基于机器学习的自动推荐系统，同样存在信任度的问题，如果用户信任推荐系统，那就会增加用户和推荐系统的交互。同样的推荐结果，以让用户信任的方式推荐给用户就更能让用户产生购买欲， 而以类似广告形式的方法推荐给用户就可能很难让用户产生购买的意愿。 度量推荐系统的信任度只能通过问卷调查的方式，询问用户是否信任推荐系统的推荐结果。

##### 实时性

推荐系统的实时性包括两个方面。首先，推荐系统需要实时地更新推荐列表来满足用户新的 行为变化。实时性的第二个方面是推荐系统需要能够将新加入系统的物品推荐给用户。

##### 健壮性

算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法 给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后 利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表 的相似度评测算法的健壮性。如果攻击后的推荐列表相对于攻击前没有发生大的变化，就说明算 法比较健壮

对于可以离线优化的指标，我个人的看法是应该在给定覆盖率、多样性、新颖性等限制条件 下，尽量优化预测准确度







其中$U$是所有提供推荐服务的用户的集合，$I$是所有标的物的集合，$R_u$是给用户$u$的推荐标的物构成的集合。

`MRR`：把标准答案在被评价系统给出结果中的排序取倒数作为它的准确度，再对所有的问题取平均。
$$
\text{MRR}=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{\text{rank}_i}
$$
![](../../picture/2/380.png)

平均`MRR`为（0.67+1+0.98）/3 ≈ 0.88

`NDCG`

`DCG`的两个思想：高关联度的结果比一般关联度的结果更影响最终的指标得分；有高关联度的结果出现在更靠前的位置的时候，指标会越高；根据推出的Item列表，定义DCG如下：
$$
\text{DCG}_i=p_i(1)+\sum_{k=2}^{n_i}\frac{p_i(k)}{\log_2k}
$$
`nDCG`为 normalized `DCG`，计算nDCG，标准化的目的在于，用户item量越少，预测命中就越困难。通过如下公式计算每个用户$i$的`nDCG`：
$$
\text{nDCG}_i=\frac{\text{DCG}_i}{1+\sum_{k=2}^{n_i^+}\frac{1}{\log_2k}}
$$
![](../../picture/2/381.png)

```
DCG（id1）= 0 + { 1/log(2,2) + 1/log(2,6) } ≈ 1.39
DCG（id2）= 1 + { 0 } ≈ 1
DCG（id3）= 0 + { 1/log(2,2) +1/log(2,3) +1/log(2,7) } ≈ 1.98
DCG（id4）= 0 + { 1/log(2,2) +1/log(2,7) } ≈ 1.35
nDCG（id1）= DCG（id1）/（1+{1/log(2,2)}）≈ 0.695
nDCG（id2）= DCG（id2）/（1）≈ 1
nDCG（id3）= DCG（id3）/（1+ 1/log(2,2) +1/log(2,3)）≈ 0.753
nDCG（id4）= DCG（id4）/（1+ 1/log(2,2) ）≈ 0.675
所以，id1,2,3,4,的nDCG值为（0.695+1+0.753+0.675）/4 
```

```python
import numpy as np


def getDCG(scores):
    return np.sum(
        np.divide(np.power(2, scores) - 1, np.log2(np.arange(scores.shape[0], dtype=np.float32) + 2)),
        dtype=np.float32)


def getNDCG(rank_list, pos_items):
    relevance = np.ones_like(pos_items)
    it2rel = {it: r for it, r in zip(pos_items, relevance)}
    rank_scores = np.asarray([it2rel.get(it, 0.0) for it in rank_list], dtype=np.float32)

    idcg = getDCG(relevance)

    dcg = getDCG(rank_scores)

    if dcg == 0.0:
        return 0.0

    ndcg = dcg / idcg
    return ndcg

l1 = [1, 4, 5]
l2 = [1, 2, 3]
a = getNDCG(l1, l2)
```

`MAP`对MAP进行的定义如下：
$$
\text{AP}=\frac{\sum_{j=1}^{n_i}P(j)\cdot y_{i,j}}{\sum_{i=1}^{n_i}y_{i,j}}
$$
其中，$y_{i,j}$表示排序中第$j$个排序结果对于用户User $i$是否被命中（点击/下单），是为1，否为0。其中
$$
P(j)=\frac{\sum_{k:\pi_i(k)\le\pi_i(j)y_{(i,k)}}}{\pi_i(j)}
$$
![](../../picture/2/382.png)

```
ID1的AP值为：
AP = [(0*0 +（1/2）*1+（1/3）*0 +（1/4*0 +（1/5）*0 +（2/6）*1 + （2/7）*0 ) ] / 2 ≈ 0.417
ID2的AP值为：
AP = [ ( 1*1+（1/2）*0+（1/3）*0 +（1/4）*0 +（1/5）*0 +（1/6）*0 +（1/7*0 ) ] / 1 = 1
ID3的AP值为：
AP = [ ( 0*0+（1/2*1 +（2/3*1+（2/4*0+（2/5*0 +（2/6*0 +（3/7*1 ) ]/ 3 ≈ 0.532
ID4的AP值为：
AP = [ ( 0*0 + （1/2）*1 + （1/3）*0 + （1/4）*0 + （1/5）*0 + （1/6）*0 + （2/7）*1 ) ] / 2 ≈ 0.393
MAP =（0.417+ 1+0.532+0.393）/4  ≈ 0.5855
```

用户行为在个性化推荐系统中一般分两种——显性反馈行为（explicit feedback）和隐性反馈 行为（implicit feedback）。显性反馈行为包括用户明确表示对物品喜好的行为。

和显性反馈行为相对应的是隐性反馈行为。隐性反馈行为指的是那些不能明确反应用户喜好 的行为。最具代表性的隐性反馈行为就是页面浏览行为。用户浏览一个物品的页面并不代表用户 一定喜欢这个页面展示的物品，比如可能因为这个页面链接显示在首页，用户更容易点击它而已。 相比显性反馈，隐性反馈虽然不明确，但数据量更大。在很多网站中，很多用户甚至只有隐性反 馈数据，而没有显性反馈数据。

一般来说，不同的数据集包含不同的行为， 目前比较有代表性的数据集有下面几个。  无上下文信息的隐性反馈数据集 每一条行为记录仅仅包含用户ID和物品ID。 Book-Crossing①就是这种类型的数据集。  无上下文信息的显性反馈数据集 每一条记录包含用户ID、物品ID和用户对物品的评分。  有上下文信息的隐性反馈数据集 每一条记录包含用户ID、物品ID和用户对物品产生行为的时间戳。Lastfm数据集①就是这种类型的数据集。  有上下文信息的显性反馈数据集 每一条记录包含用户ID、物品ID、用户对物品的评分 和评分行为发生的时间戳。Netflix Prize②提供的就是这种类型的数据集。

很多研究人员发现，用户行为数据也蕴含着这种规律。令$f_u(k)$为对$k$个物品产生过行为的用户数，令$f_i(k)$为被$k$个用户产生过行为的物品数。那么，$f_u(k)$和$f_i(k)$都满足长尾分布。也就是说：
$$
f_i(k)=\alpha_ik^{\beta_i}\\
f_u(k)=\alpha_uk^{\beta_u}
$$
一般认为，新用户倾向于浏览热门的物品，因为他 们对网站还不熟悉，只能点击首页的热门物品，而老用户会逐渐开始浏览冷门的物品。