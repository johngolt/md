##### `DeepMCP`

现有的大多数方法主要是对特征和 `CTR` 之间的关系进行建模，并存在数据稀疏的问题。`DeepMCP`模型还考虑了 `feature-feature` 关系如 `user-ad` 关系、`ad-ad` 关系，从而学习更多信息丰富的、统计可靠的特征`representation` ，最终提高 `CTR` 预估的性能。

`DeepMCP` 模型包含三个部分：一个 `matching subnet`、一个 `correlation subnet`、一个 `prediction subnet` 。这三个部分共享相同的 `embedding` 矩阵。

- `matching subnet` 对 `user-ad` 的关系进行建模，并旨在学习有用的用户`representation` 和有用的广告 `representation` 。
- `correlation subnet` 对 `ad-ad` 的关系（哪些广告位于用户点击序列的时间窗口内）进行建模，并旨在学习有用的广告 `representation` 。
- `prediction subnet` 对 `feature-CTR` 关系进行建模，并旨在根据所有特征预测 `CTR` 。

当这些 `subnet` 在目标`label` 的监督下联合优化时，学到的特征`representation`既具有良好的预测能力、又具有良好的表达能力。此外，由于同一个特征以不同的方式出现在不同的 `subnet` 中，因此学到的 `representation` 在统计上更加可靠。

`DeepMCP` 的另一个特点是：尽管在训练过程中所有 `subnet` 都处于活跃状态，但是在测试过程中只有 `prediction subnet` 处于活跃状态

我们将特征分为四组：用户特征、`query` 特征、广告特征、其它特征。每个 `subnet` 使用不同的特征集合。具体而言：`prediction subnet` 使用所有四组特征；`matching subnet` 使用 `user, query, ad` 三组特征；`correlation subnet` 仅使用 `ad` 特征。所有 `subnet` 共享相同的 `embedding` 矩阵。注意：`Context ad features` 和 `Negative ad features` 是 `correlation subnet` 中，位于用户点击序列的时间窗口内上下文广告、以及窗口外的负采样广告。它仅用于 `correlation subnet` 。

`DeepMCP` 的整体结构如下图所示，所有的`subnet` 共享相同的 `embedding` 矩阵。

![](../../picture/1/427.png)

###### Prediction Subnet

`prediction subnet` 这里是一个典型的 `DNN` 模型，它对 `feature-CTR` 关系进行建模。它旨在在目标 `label` 的监督下，根据所有特征预估点击率。

`prediction subnet` 的整体结构为：

- 首先，单个特征$x_i\in\mathbb{R}$（如用户`ID`）通过一个 `embedding` 层，然后映射到对应的`embedding` 向量$\vec{\mathbf{e}}_i\in\mathbb{R}^K$，其中$K$为`embedding` 向量维度。假设特征$x_i$的取值集合规模为$N$，则这个特征所有 `embedding` 的集合构成一个 `embedding` 矩阵$\mathbf{E}\in\mathbb{R}^{N\times K}$。注意：这里假设所有特征都是离散的。如果存在连续值的特征，则需要首先对其进行离散化。对于多类别的离散特征，我们首先将每个 `bi-gram` 映射到一个 `embedding` 向量，然后再执行一个 `sum pooling` 从而得到广告标题的、聚合后的 `embedding` 向量。

- 接着，对于单个样本上的多种特征，我们将每个特征的 `embedding` 向量拼接为一个长的向量$\vec{\mathbf{m}}$。然后向量$\vec{\mathbf{m}}$经过若干层全连接层`FC` 层，从而学到高阶的非线性特征交互。

- 最后，最后一层 `FC` 层的输出$\vec{\mathbf{z}}$通过一个 `sigmoid` 函数从而得到预估的 `CTR`：
  $$
  \hat{y}=\frac{1}{1+\exp\left[-(\vec{\mathbf{w}}^T\vec{\mathbf{z}}+b)\right]}
  $$

为缓解过拟合，我们在每个 `FC` 层之后都应用了 `dropout`。`prediction subnet` 的损失函数为训练集的交叉熵：
$$
\mathcal{L}_p=-\frac{1}{n}\sum_{i=1}^n[y_i\log\hat{y}_i+(1-y_i)\log(1-\hat{y}_i)]
$$
其中：$n$为训练样本的数量；$y_i\in\{0,1\}$为第$i$个样本是否点击的真实`label`；$\hat{y}_i$为第$i$个样本预估的`CTR` 。

###### Matching Subnet

`matching subnet` 对 `user-ad` 的关系进行建模，并旨在学习有用的用户`representation` 和有用的广告 `representation` 。具体而言，`matching subnet` 包含两个部分：

- 用户部分 `user part`：用户部分的输入是用户特征（如用户`ID`、年龄）和 `query` 特征（如`query category`）。像在 `prediction subnet` 中一样，单个特征$x_i\in\mathbb{R}$首先经过 `embedding` 层，然后映射为对应的 `embedding` 向量$\vec{\mathbf{e}}_i\in\mathbb{R}^K$。然后我们将单个用户$u$的多种特征的 `embedding` 拼接为长向量$\vec{\mathbf{m}}_u\in\mathbb{R}^{N_u}$，其中$N_u$为长向量的维度。然后向量$\vec{\mathbf{m}}_u$经过若干层全连接层`FC` 层，从而学到高阶的非线性特征交互。对于最后一个 `FC` 层，我们使用 `tanh` 非线性激活函数。用户部分的输出是一个 `high-level` 的用户 `representation` 向量$\vec{\mathbf{v}}_u\in\mathbb{R}^M$，其中$M$为向量维度。
- 广告部分 `ad part`：广告部分的输入是广告特征（如广告`ID`、广告标题）。首先将每个广告特征映射到对应的 `embedding` 向量，然后将单个广告$a$的多种特征的 `embedding` 拼接为长向量$\vec{\mathbf{m}}_a\in\mathbb{R}^{N_a}$，其中$N_a$为长向量的维度。然后向量 经过若干层全连接层`FC` 层，从而得到一个 `high-level` 的广告 `representation` 向量$\vec{\mathbf{v}}_a\in\mathbb{R}^M$。同样地，对于最后一个 `FC` 层，我们使用 `tanh` 非线性激活函数。

然后我们通过下式计算 `matching score` 为：
$$
s(\vec{\mathbf{v}}_u,\vec{\mathbf{v}}_a)=\frac{1}{1+\exp(-\vec{\mathbf{v}}_u^T\vec{\mathbf{v}}_a)}
$$
我们并没有使用 `ReLU` 作为最后一个 `FC` 层的激活函数，因为 `ReLU` 之后的输出将包含很多零。

至少有两种选择来建模 `matching score`：

- `point-wise` 模型：当用户$u$点击广告$a$时，则$s(\vec{\mathbf{v}}_u,\vec{\mathbf{v}}_a)\to1$；当用户$u$未点击广告$a$时，则$s(\vec{\mathbf{v}}_u,\vec{\mathbf{v}}_a)\to0$。
- `pair-wise` 模型：如果用户$u$点击了广告$a_i$但是未点击广告$a_j$，则$s(\vec{\mathbf{v}}_u,\vec{\mathbf{v}}_{a_i})>s(\vec{\mathbf{v}}_u,\vec{\mathbf{v}}_{a_j})+\delta$，其中$\delta>0$为 `margin` 超参数。

这里我们选择 `point-wise` 模型，因为它可以直接将训练数据集重新用于 `prediction subnet` 。我们将 `matching subnet` 的损失函数定义为：
$$
\mathcal{L}_m=-\frac{1}{n}\sum_{i=1}^n[y(u,a)\log s(\vec{\mathbf{v}}_u,\vec{\mathbf{v}}_a)+(1-y(u,a))\log(1-s(\vec{\mathbf{v}}_u,\vec{\mathbf{v}}_a))]
$$
其中：$n$为样本数量；如果用户$u$点击广告$a$则$y(u,a)=1$，否则$y(u,a)=0$。

`matching subnet` 也是采用是否点击作为`label`，这和 `prediction subnet` 完全相同。二者不同的地方在于`matching subnet` 是 `uv` 粒度，而 `prediction subnet` 是 `pv` 粒度，即如果广告$a$对用户$u$曝光多次且仅发生一次点击：

- 对于 `prediction subnet` 这将生成多个样本，每个样本对应一次曝光。只有发生点击的那个曝光对应的样本`label = 1`，其它曝光对应的样本 `label=0` 。
- 对于 `matching subnet` 这将生成一个样本，样本的`label=1` （如果所有这些曝光都未发生点击，则样本 `label=0` ）。另外，`matching subnet` 的样本不包含曝光上下文特征。

###### Correlation Subnet

给定单个用户点击广告的广告序列$\{a_1,\cdots,a_L\}$，我们最大化平均对数似然：
$$
\mathcal{U}=\frac{1}{L}\sum_{i=1}^L\sum_{-C\le j\le C} \log p(a_{i+j}|a_i)
$$
其中：$L$是广告序列长度，$C$为上下文窗口大小。概率$p(a_{i+j}|a_i)$可以通过不同的方式进行定义，例如 `softmax`、层次 `softmax`、负采样。由于负采样的效率高，我们选择负采样技术将$p(a_{i+j}|a_i)$定义为：
$$
p(a_{i+j}|a_i)=\sigma(\vec{\mathbf{h}}^T_{a_{i+j}}\vec{\mathbf{h}}_{a_{i}})\prod_{q=1}^Q\sigma(-\vec{\mathbf{h}}^T_{a_{q}}\vec{\mathbf{h}}_{a_{i}})
$$
其中：$Q$为负采样的广告数。$\sigma(\cdot)$为 `sigmoid` 函数。$\vec{\mathbf{h}}_{a_{i}}$为广告$a_i$的 `high-level representation`，它涉及广告$a_i$的所有特征，并经过多个 `FC` 层。

`correlation subnet` 的损失函数为负的对数似然：
$$
\mathcal{L}_c=\frac{1}{L}\sum_{i=1}^L\sum_{-C\le j\le C}\left[-\log\left[\sigma(\vec{\mathbf{h}}^T_{a_{i+j}}\vec{\mathbf{h}}_{a_{i}})\right]-\sum_{q=1}^Q\log\left[\sigma(-\vec{\mathbf{h}}^T_{a_{q}}\vec{\mathbf{h}}_{a_{i}})\right]\right]
$$
考虑所有用户的$\mathcal{L}_c$​则得到 `correlation subnet` 总的损失。

离线训练过程：`DeepMCP` 的最终联合损失函数为：
$$
\mathcal{L}=\mathcal{L}_p+\alpha\mathcal{L}_m+\beta\mathcal{L}_c
$$
其中$\alpha,\beta$为超参数，它们平衡了不同 `subnet` 的重要性。`DeepMCP` 通过在训练集上最小化联合损失函数来训练。由于我们的目标是最大化 `CTR` 预估性能，因此训练过程中我们在独立的验证集上评估模型，并记录验证 `AUC`。

##### `MIMN`



##### `DMR`

`Matching` 和 `Ranking` 是推荐系统中信息检索的两个经典阶段。

- `matching` 阶段通过将 `user` 和 `item` 进行 `matching` ，从而从整个`item` 集合中检索一小部分候选对象`candidate`。
- `ranking` 阶段通过 `ranking` 模型为不同 `matching` 方法生成的候选者打分，并将 `top-N` 打分的 `item` 呈现给最终用户。

`DMR` 。该模型将协同过滤的思想和 `matching` 思想相结合，用于 `CTR` 预估的 `ranking` 任务，从而提高了 `CTR` 预估的性能。

`DMR` 包含 `User-to-Item Network` 和 `Item-to-Item Network` 这两个子网来代表 `user-to-item` 的相关性。

- 在 `User-to-Item Network` ，论文通过`embedding` 空间中 `user embedding` 和 `item embedding` 的内积来表达用户和 `item` 之间的相关性。其中 `user embedding` 是从用户行为中抽取而来。
- 在 `Item-to-Item Network`，论文首先计算用户交互`item` 和目标 `item` 之间的 `item-to-item` 相似度，其中采用考虑了位置信息`position information` 的注意力机制。然后论文将`item-to-item` 相似性相加，从而获得了另一种形式的 `user-to-item` 相关性。

推荐系统中包含四类特征：用户画像`User Profile`。`Target Item` 特征。用户行为 `User Behavior`。上下文 `Context`。大多数特征都是离散型特征，可以将其转换为高维的 `one-hot` 向量。`one-hot` 向量通过 `embedding layer` 转换为低维稠密特征。

我们将离散特征 `embedding` 和 `normalized` 的连续特征拼接起来，其中：用户画像`User Profile` 的拼接特征为$\vec{\mathbf{x}}_p$、用户行为`User Behavior` 的拼接特征为$\vec{\mathbf{x}}_b$、`Target Item` 的拼接特征为$\vec{\mathbf{x}}_t$、上下文`Context`的拼接特征为$\vec{\mathbf{x}}_c$。

注意，用户行为序列包含很多个`item`，因此用`User Behavior` 的特征是由这些`item` 的特征向量列表拼接而成$\vec{\mathbf{x}}_b=[\vec{\mathbf{e}}_1||\cdots||\vec{\mathbf{e}}_T]\in\mathbb{R}^{Td_e\times 1}$，其中：$T$为用户行为序列的长度，由于不同用户的行为序列长度不同，因此$T$是可变的。$\vec{\mathbf{e}}_t$为第$t$个行为的特征向量，$d_e$为对应的维度，`||` 表示向量拼接。

`User Behavior` 特征和 `Target Item` 特征位于相同的特征空间，并共享相同的 `embedding` 矩阵以降低内存需求。

所有特征向量拼接起来构成样本的完整`representation` 之后，将`representation` 灌入 `MLP` 。`MLP` 隐层的激活函数为 `PRelu`，最终输出层采用 `sigmoid` 激活函数从而用于二分类任务。

- `MLP` 输入的长度需要固定，因此需要将`User Behavior` 特征向量$\vec{\mathbf{x}}_b$进行池化从而转换为固定长度的特征向量。

- 交叉熵损失函数通常和 `sigmoid` 函数一起使用，其对数分量可以低效 `sigmoid` 函数中指数的副作用。给定样本$(\vec{\mathbf{x}},y)$，其中$y\in\{0,1\}$为标签，$\vec{\mathbf{x}}=[\vec{\mathbf{x}}_p||\vec{\mathbf{x}}_b||\vec{\mathbf{x}}_t||\vec{\mathbf{x}}_c]$为样本特征，则损失函数为：
  $$
  \mathcal{L}_t=-\frac{1}{N}\sum_{(\vec{\mathbf{x}},y)\in\mathcal{D}}\left[y\log f(\vec{\mathbf{x}})+(1-y)\log\left(1-f(\vec{\mathbf{x}})\right)\right]
  $$
  其中$\mathcal{D}$为训练集，$N$为总的样本数量，$f(\vec{\mathbf{x}})$为 `MLP` 的预测输出的点击概率。

`DMR` 结构如下图所示：

- 输入特征向量是嵌入`embedded` 的离散特征、和正则化`normalized` 的连续特征的拼接。
- `DMR` 使用两个子网以两种形式来建模 `user-to-item` 相关性。两种形式的 `user-to-item` 相关性、用户的时间`temporal` 兴趣的 `representation` 、以及其它所有特征向量拼接起来，然后馈入到 `MLP` 中。

最终损失由 `MLP` 的 `target loss` 和辅助的 `match network loss` 组成。

![](../../picture/1/428.png)

###### `User-to-Item Network`

遵循基于矩阵分解的 `matching` 方法中的`representation` 形式，`User-to-Item Network` 通过`user representation` 和 `item representation` 的内积来建模用户和目标 `item` 之间的相关性，这可以视作用户和 `item` 之间的一种特征交互。

###### `Item-to-Item Network`



##### `MiNet`

##### `DSTN`

