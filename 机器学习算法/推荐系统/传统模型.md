- **静态画像**： 用户独立于产品场景之外的属性，例如用户的自然属性，这类信息比较稳定，具有统计性意义。
- **动态画像**： 用户在场景中所产生的显示行为或隐式行为。

隐式行为的权重往往不会有显示行为大，但是在实际业务中，用户的显示行为都是比较稀疏的，所以需要依赖大量的隐式行为。

推荐算法主要分为以下几步：

- **召回**：当用户以及内容量比较大的时候，往往先通过召回策略，将百万量级的内容先缩小到百量级。
- **过滤**：对于内容不可重复消费的领域，例如实时性比较强的新闻等，在用户已经曝光和点击后不会再推送到用户面前。
- **精排**：对于召回并过滤后的内容进行排序，将百量级的内容并按照顺序推送。
- **混排**：为避免内容越推越窄，将精排后的推荐结果进行一定修改，例如控制某一类型的频次。
- **强规则**：根据业务规则进行修改，例如在活动时将某些文章置顶。

总体而言，召回环节的有监督模型化以及一切 Embedding 化，这是两个相辅相成的总体发展趋势。如果我们根据召回路是否有用户个性化因素存在来划分，可以分成两大类：一类是无个性化因素的召回路，比如热门商品或者热门文章或者历史点击率高的物料的召回；另外一类是包含个性化因素的召回路，比如用户兴趣标签召回。

### `CTR`预估模型

CTR和推荐算法的本质区别只能有一个，就是：CTR最终预测的是那个有物理意义的数值`CTR`；推荐算法最终产生的是一个推荐列表，是一个item的相对位置。

CTR即点击通过率，指网络广告的点击到达率，即该广告的实际点击次数除以广告的展现量。

在 `cost-per-click:CPC` 广告中广告主按点击付费。为了最大化平台收入和用户体验，广告平台必须预测广告的 `CTR` ，称作 `predict CTR: pCTR` 。对每个用户的每次搜索`query`，有多个满足条件的广告同时参与竞争。只有 `pCTR x bid price` 最大的广告才能竞争获胜，从而最大化 `eCPM` ：$\text{eCPM}=\text{pCTR}\times \text{bid price}$

广告被点击的概率取决于两个因素：广告被浏览的概率、广告浏览后被点击的概率。因此有：
$$
P(\text{click}|ad, pos) = p(\text{click}|ad, pos, seen)\times p(seen|ad, pos)
$$
假设：在广告被浏览到的情况下，广告被点击的概率与其位置无关，仅与广告内容有关。广告被浏览的概率与广告内容无关，仅与广告位置有关。则有：
$$
P(\text{click}|ad, pos) = p(\text{click}|ad, seen)\times p(seen|pos)
$$
第一项$p(\text{click}|ad, seen)$就是我们关注和预测的 `CTR` 。第二项与广告无关，是广告位置的固有属性。可以通过经验来估计这一项：统计该广告位的总拉取次数impress，以及总曝光次数seen，则：$p(seen|pos)=\frac{seen}{impress}$，这也称作广告位的曝光拉取比。广告可能被拉取（推送到用户的页面），但是可能未被曝光（未被用户浏览到）。

数据集构建：样本的特征从广告基本属性中抽取，将每个广告的真实点击率 `CTR` 来作为 `label` 。考虑到真实点击率 `CTR` 无法计算，因此根据每个广告的累计曝光次数、累计点击次数从而得到其经验点击率$\overline{\text{CTR}}$来作为 `CTR` 。

##### 逻辑回归

将 `CTR` 预估问题视作一个回归问题，采用逻辑回归 `LR` 模型来建模，因为 `LR` 模型的输出是在 `0` 到 `1` 之间。
$$
\text{pCTR} = \frac{1}{1+\exp(-\sum_{i}\omega_i\times f_i)}
$$
其中$f_i$表示从广告中抽取的第$i$个特征，$\omega_i$为该特征对应的权重。

模型的损失函数为交叉熵：$\mathcal{L}= -[\text{pCTR}\times \log(\overline{\text{CTR}})]+(1-\text{pCTR})\times \log(1-\overline{\text{CTR}})$

###### 特征预处理方法

模型添加了一个`bias feature`，该特征的取值恒定为 1。

对于每个特征$f_i$，人工构造额外的两个非线性特征：$\log(f_i+1)$

对所有特征执行标准化，标准化为均值为0、方差为1 。

对所有特征执行异常值截断：对于每个特征，任何超过均值 5 个标准差的量都被截断。

###### 评价指标

`baseline` ：采用训练集所有广告的平均 `CTR` 作为测试集所有广告的 `pCTR` 。

评估指标：测试集上每个广告的 `pCTR` 和真实点击率的平均 `KL` 散度。
$$
\overline{\mathbb{D}_{KL}}=\frac{1}{T}\sum_{i=1}^{T}(\text{pCTR}(ad_i)\times \log\frac{\text{pCTR}(ad_i)}{\overline{\text{CTR}}(ad_i)}+(1-\text{pCTR}(ad_i))\times \log\frac{1-\text{pCTR}(ad_i)}{1-\overline{\text{CTR}}(ad_i)})
$$
`KL` 散度衡量了$\text{pCTR}$和真实点击率之间的偏离程度。一个理想的模型，其 `KL` 散度为 0 ，表示预估点击率和真实点击率完全匹配。

### 关联分析

关联分析的目标分为两项:发现频繁项集和发现关联规则。首先需要找到频繁项集，然后才能获得关联规则。

| 名称   | 定义                                                         |
| ------ | ------------------------------------------------------------ |
| 项     | 指我们分析数据中的一个对象，如豆奶                           |
| 项集   | 就是若干项的项构成的集合，如集合{豆奶，莴苣}是一个2项集。    |
| 支持度 | 某项集在数据集中出现的概率。即项集在记录中出现的次数，除以数据集中所有记录的数量。 |
| 置信度 | 关联规则{A->B}的置信度为A与B同时出现的次数，除以A出现的次数。即在A发生的条件下，B发生的概率。 |
| 提升度 | 关联规则{A->B}中，提升度是指{A->B}的置信度，除以B的支持度。提升度体现的是组合相对不组合的比值，如果提升度大于1，则说明应用该关联规则是有价值的。 |

支持度体现的是某项集的频繁程度，只有某项集的支持度达到一定程度，我们才有研究该项集的必要。

###### `Apriori`算法

![](../../picture/2/346.png)

1.生成单个物品的项集
2.剔除支持度小于阈值的项，得到频繁1项集
3.将频繁1项集组合得到2项集
4.剔除支持度小于阈值的项，得到频繁2项集
5.重复上述步骤直到所有项集都去掉

对于一个项集{0,1,2,3}产生关联规则，需要生成一个可能的规则列表，然后测试每条规则的可信度。如果某条规则不满足最小置信度，那么该规则的所有子集也都不满足最小置信度。比如规则{0,1,2}->{3}不满足最小可信度要求，那么任何左部为{0,1,2}子集的规则也不会满足最小可信度要求，或者说所有以{3}作为后件的规则不会满 足最小可信度要求。

Apriori 在计算的过程中有以下几个缺点：

1. 可能产生大量的候选集。因为采用排列组合的方式，把可能的项集都组合出来了
2. 每次计算都需要重新扫描数据集，来计算每个项集的支持度

###### `FP-Growth`算法

1. 首先创建项头表，这一步的流程是先扫描一遍数据集，对于满足最小支持度的单个项按照支持度从高到低进行排序，这个过程中删除了不满足最小支持度的项
2. 对于每一条数据，按照支持度从高到低的顺序进行创建节点（也就是第一步中项头表中的排序结果），节点如果存在就将计数 count+1，如果不存在就进行创建。同时在创建的过程中，需要更新项头表的链表
   1. 先把原始数据按照支持度排序，把以上每行数据，按照顺序插入到FP树中，注意FP树的根节点记为 NULL 节点。

得到 FP 树后，需要对每一个频繁项，逐个挖掘频繁项集。具体过程为：首先获得频繁项的前缀路径，然后将前缀路径作为新的数据集，以此构建前缀路径的条件 FP 树。然后对条件 FP 树中的每个频繁项，获得前缀路径并以此构建新的条件 FP 树。不断迭代，直到条件 FP 树中只包含一个频繁项为止

### 基于 match function learning 的深度匹配模型

#### feature-based的深度模型

##### deep crossing 模型





### 其他

##### 召回模型

###### 向量化召回

通过模型来学习用户和物品的兴趣向量，并通过内积来计算用户和物品之间的相似性，从而得到最终的候选集。

![](../../picture/1/214.png)
$$
P(\omega_t=i|U,C) = \frac{e^{v_iu}}{\sum_{j\in V}e^{v_ju}}
$$
两侧分别对 user 和 item 特征通过D`NN`输出向量，并在最后一层计算二个输出向量的内积。

![](../../picture/1/215.png)

![](../../picture/1/216.png)

多 Embedding 向量召回-用户多兴趣表达，通过一种模型来建模出用户多个 embedding 的表示。

![](../../picture/1/217.png)

Multi-Interest 抽取层负责建模用户多个兴趣向量 embedding，然后通过 Label-aware Attention 结构对多个兴趣向量加权。这是因为多个兴趣 embedding 和待推荐的 item 的相关性肯定不同。其中上图中的 K，V 都表示用户多兴趣向量，Q 表示待推荐 item 的 embedding 表示，最终用户的 embedding 表示为：
$$
\vec{\mathbf{v}_u} = Attention(\vec{\mathbf{e}_i}, V_u, V_u)=V_usoftmax{pow(V_u^T\vec{\mathbf{e}_i}, p)}
$$
 $e_i$表示 item embedding，$V_u$表示 Multi-Interest 抽取层输出的用户多个兴趣向量 embedding。然后使用$V_u$和待推荐 item embedding，计算用户u和商品i交互的概率，计算方法和 YouTube DNN 一样。

##### Embedding

 Embedding 层往往采用预训练的方式完成。  Embedding 的训练往往独立于深度学习网络进行。在得到稀疏特征的稠密表达之后，再与其他特征一起输入神经网络进行训练。 

#### 知识图谱

如果两个节点之间存在关系，他们就会被一条无向边连接在一起，那么这个节点，我们就称为**实体**（Entity），它们之间的这条边，我们就称为**关系**（Relationship）。知识图谱的基本单位，便是“实体（Entity）-关系（Relationship）-实体（Entity）”构成的三元组，这也是知识图谱的核心。

![](../../picture/1/204.png)

##### 架构

知识图谱的架构主要可以被分为：逻辑架构、技术架构

###### 逻辑架构

在逻辑上，我们通常将知识图谱划分为两个层次：数据层和模式层。模式层：在数据层之上，是知识图谱的核心，存储经过提炼的知识，通常通过本体库来管理这一层。数据层：存储真实的数据。本体库可以理解为面向对象里的“类”这样一个概念，本体库就储存着知识图谱的类

模式层：实体-关系-实体，实体-属性-性值；数据层：比尔盖茨-妻子-梅琳达·盖茨，比尔盖茨-总裁-微软

###### 技术架构

知识图谱有自顶向下和自底向上两种构建方式，这里提到的构建技术主要是自底向上的构建技术。构建知识图谱是一个迭代更新的过程，根据知识获取的逻辑，每一轮迭代包含三个阶段：信息抽取：从各种类型的数据源中提取出实体、属性以及实体间的相互关系，在此基础上形成本体化的知识表达；知识融合：在获得新知识之后，需要对其进行整合，以消除矛盾和歧义，比如某些实体可能有多种表达，某个特定称谓也许对应于多个不同的实体等；知识加工：对于经过融合的新知识，需要经过质量评估之后，才能将合格的部分加入到知识库中，以确保知识库的质量。

![](../../picture/1/205.png)

信息抽取是一种自动化地从半结构化和无结构数据中抽取实体、关系以及实体属性等结构化信息的技术。实体抽取，也称为命名实体识别（named entity recognition，NER），是指从文本数据集中自动识别出命名实体。关系抽取，从相关语料中提取出实体之间的关联关系，通过关系将实体联系起来，才能够形成网状的知识结构。属性抽取，从不同信息源中采集特定实体的属性信息，如针对某个公众人物，可以从网络公开信息中得到其昵称、生日、国籍、教育背景等信息。

知识融合：实体链接是指对于从文本中抽取得到的实体对象，将其链接到知识库中对应的正确实体对象的操作。其基本思想是首先根据给定的实体指称项，从知识库中选出一组候选实体对象，然后通过相似度计算将指称项链接到正确的实体对象。实体链接的流程：从文本中通过实体抽取得到实体指称项；进行实体消歧和共指消解，判断知识库中的同名实体与之是否代表不同的含义以及知识库中是否存在其他命名实体与之表示相同的含义；在确认知识库中对应的正确实体对象之后，将该实体指称项链接到知识库中对应实体。

知识加工：通过信息抽取，从原始语料中提取出了实体、关系与属性等知识要素，并且经过知识融合，消除实体指称项与实体对象之间的歧义，得到一系列基本的事实表达。然而事实本身并不等于知识。要想最终获得结构化，网络化的知识体系，还需要经历知识加工的过程。

本体：是指人工的概念集合、概念框架。本体可以采用人工编辑的方式手动构建，也可以以数据驱动的自动化方式构建本体。因为人工方式工作量巨大，且很难找到符合要求的专家，因此当前主流的全局本体库产品，都是从一些面向特定领域的现有本体库出发，采用自动构建技术逐步扩展得到的。

自动化本体构建过程包含三个阶段：实体并列关系相似度计算、实体上下位关系抽取、本体的生成

当知识图谱刚得到“阿里巴巴”、“腾讯”、“手机”这三个实体的时候，可能会认为它们三个之间并没有什么差别，但当它去计算三个实体之间的相似度后，就会发现，阿里巴巴和腾讯之间可能更相似，和手机差别更大一些。这就是第一步的作用，但这样下来，知识图谱实际上还是没有一个上下层的概念，它还是不知道，阿里巴巴和手机，根本就不隶属于一个类型，无法比较。因此我们在实体上下位关系抽取这一步，就需要去完成这样的工作，从而生成第三步的本体。当三步结束后，这个知识图谱可能就会明白，“阿里巴巴和腾讯，其实都是公司这样一个实体下的细分实体。它们和手机并不是一类。”

知识推理：在我们完成了本体构建这一步之后，一个知识图谱的雏形便已经搭建好了。但可能在这个时候，知识图谱之间大多数关系都是残缺的，缺失值非常严重，那么这个时候，我们就可以使用知识推理技术，去完成进一步的知识发现。

当然知识推理的对象也并不局限于实体间的关系，也可以是实体的属性值，本体的概念层次关系等。这一块的算法主要可以分为3大类，基于逻辑的推理、基于图的推理和基于深度学习的推理。

![](../../picture/1/206.png)

知识更新：从逻辑上看，知识库的更新包括概念层的更新和数据层的更新。概念层的更新是指新增数据后获得了新的概念，需要自动将新的概念添加到知识库的概念层中。数据层的更新主要是新增或更新实体、关系、属性值，对数据层进行更新需要考虑数据源的可靠性、数据的一致性等可靠数据源，并选择在各数据源中出现频率高的事实和属性加入知识库。

知识图谱的内容更新有两种方式：全面更新：指以更新后的全部数据为输入，从零开始构建知识图谱。这种方法比较简单，但资源消耗大，而且需要耗费大量人力资源进行系统维护；增量更新：以当前新增数据为输入，向现有知识图谱中添加新增知识。这种方式资源消耗小，但目前仍需要大量人工干预（定义规则等），因此实施起来十分困难。

#### `CTR`预估模型

在 `cost-per-click:CPC` 广告中广告主按点击付费。为了最大化平台收入和用户体验，广告平台必须预测广告的 `CTR` ，称作 `predict CTR: pCTR` 。对每个用户的每次搜索`query`，有多个满足条件的广告同时参与竞争。只有 `pCTR x bid price` 最大的广告才能竞争获胜，从而最大化 `eCPM` ：$eCPM=pCTR \times \text{bid price}$基于最大似然准则可以通过广告的历史表现得统计来计算 `pCTR` 。假设广告曝光了100次，其中发生点击5次，则 `pCTR = 5%`。其背后的假设是：忽略表现出周期性行为或者不一致行为的广告，随着广告的不断曝光每个广告都会收敛到一个潜在的真实点击率 。这种计算 `pCTR` 的方式对于新广告或者刚刚投放的广告问题较大：新广告没有历史投放信息，其曝光和点击的次数均为 0 。刚刚投放的广告，曝光次数和点击次数都很低，因此这种方式计算的 `pCTR` 波动非常大。

从经验上来看：广告在页面上的位置越靠后，用户浏览它的概率越低。因此广告被点击的概率取决于两个因素：广告被浏览的概率、广告浏览后被点击的概率。因此有：
$$
p(click|ad,pos) = p(click|ad,pos,seen)\times p(seen|add,pos)
$$
假设：在广告被浏览到的情况下，广告被点击的概率与其位置无关，仅与广告内容有关。广告被浏览的概率与广告内容无关，仅与广告位置有关。则有：
$$
p(click|ad,pos) = p(click|add,seen)\times p(seen|pos)
$$
第一项$p(click|add,seen)$就是我们关注和预测的 `CTR` 。第二项与广告无关，是广告位置的固有属性。可以通过经验来估计这一项：统计该广告位的总拉取次数$impress(poss)$，以及总曝光次数$seen(pos)$，则：
$$
p(seen|pos) = \frac{seen(pos)}{impress(poss)}
$$
这也称作广告位的曝光拉取比。







此处我们给出可疑样本的定义。

假设我们存在$N$个样本$(x_1,y_1),(x_2,y_2),...,(x_N,y_N)$,$x_i$为第$i$个样本的特征,$y_i$为第$i$个样本对应的标签,$y_i \in \{1,2,3...,K\}$,$K \ge 3$为类的个数,我们采用已经训练好的模型$Model$对$N$个样本进行预测,得到一个$N*K$的概率矩阵,我们用$p_{ij}$表示为$Model$把第$i$个样本预测为第$j$类的概率,并且将每一个样本中概率最大的值对应的类作为我们最终的预测结果.即$argmax_j ~ p_{ij}, j \in K$为第$i$个样本的预测结果.  

**可疑样本定义:** 对于每一个样本$x_i $, 令$ q_i = max ~ p_{ij},~ j \in \{1,2,...,K\}, i \in \{1,2,...,N\}$,我们将所有 $q_i \le threshold,i \in \{1,2,...,N\} $的样本定义为可疑样本,表示模型对该类样本的预测没有较强把握.

#### 算法步骤

**1.输入**: 训练数据$Train$,$\{(x_1,y_1),(x_2,y_2),...,(x_{N_1},y_{N_1}) \}, x_i \in R^d, y_i \in \{1,2,..., M\}$, 测试集$Test$,$\{(x_1,y_1),(x_2,y_2),...,(x_{N_2},y_{N_2}) \}$.可疑样本的$Threshold$. $KNN$的$K$以及采用的距离函数。 

**2.模型训练**: 对数据集Train进行训练获得模型$Model$。 

**3.模型预测:**使用模型$Model$分别对训练集和测试集进行预测得到训练集的预测概率矩阵$Matrix\_Tr \in R^{N_1 * M}$以及测试集的概率矩阵$Matrix\_Te \in R^{N_2 * M}$。 

**4.KNN纠正**: 将测试集中预测结果概率低于$Threshold$的样本的预测数据提取出来形成新的测试集$Test'$,将训练集的预测矩阵作为新的训练集的特征并使用$KNN$进行训练获得KNN模型,使用$KNN$对$Test'$进行预测,并将新的预测结果替代原先的预测结果。 

**5.输出:**将纠正后的预测结果作为最终结果进行输出。 

####  算法步骤

**1.输入**: 训练数据$Train$,$\{(x_1,y_1),(x_2,y_2),...,(x_{N_1},y_{N_1}) \}, x_i \in R^d, y_i \in \{1,2,..., K\}$, 测试集$Test$,$\{(x_1,y_1),(x_2,y_2),...,(x_{N_2},y_{N_2}) \}$.训练集可疑样本的$Threshold\_Tr$.测试集可疑样本的$Threshold\_Te$.$KNN$的$K$以及采用的距离函数。
**2.模型训练:** 对数据集$Train$进行训练获得模型$Model$。
**3.模型预测:****使用模型$Model$分别对训练集和测试集进行预测得到训练集的预测概率矩阵$Matrix\_Tr \in R^{N_1 * K}$以及测试集的概率矩阵$Matrix\_Te \in R^{N_2 * K}$。
**4.KNN纠正:** 将测试集中预测结果概率低于$Threshold\_Te$的样本的预测数据提取出来形成新的测试集$Test'$,将训练集的预测矩阵高于$Threshold\_Tr$作为过滤之后的训练集的特征并使用$KNN$进行训练获得$KNN$模型,使用$KNN$对$Test'$进行预测,并将新的预测结果替代原先的预测结果。 
**5.输出:**将纠正后的预测结果作为最终结果进行输出。



###### 推荐系统架构

架构设计的第一步是确定系统的边界。

所谓边界，就是区分什么是这个系统要负责的，也就是边界内的部分，以及什么是这个模型要依赖的，也就是边界外的部分。系统边界的确定，简单来说，就是在输入方面确定需要别人给我提供什么，而在输出方面确定我要给别人提供什么。

在输入方面，就是判断什么输入是需要别人提供给我的，要把握的主要原则包括：

- 这个数据或服务是否与我的业务强相关，在推荐业务中用到的每个东西，并不是都与推荐业务强相关，例如电商推荐系统中的商品信息，只有与推荐业务强相关的服务才应该被纳入推荐系统的边界中。

- 这个数据或服务除了我的业务在使用，是否还有其他业务也在使用，例如上面说到的商品信息服务，除了推荐系统在使用，其他子系统也在广泛使用，那么显然它应该是一个外部依赖。

推荐系统的效果和性能在一定程度上取决于这些依赖系统，所以在寻求推荐系统的优化目标时，目光不能只看到推荐系统本身，很多时候这些依赖系统也是重要的效果提升来源。

架构设计有很多不同的切入方式，最简单也是最常用的一种方式就是先决定某个模块或逻辑是运行在离线层、在线层还是近线层。

任何使用非实时数据、提供非实时服务的逻辑模块，都可以被定义为离线模块。其典型代表是离线的协同过滤算法，以及一些离线的标签挖掘类算法。离线层通常用来进行大数据量的计算，由于计算是离线进行的，因此用到的数据也都是非实时数据，最终会产出一份非实时的离线数据，供下游进一步处理使用。

近线层则处于离线层和在线层的中间位置，是一个比较奇妙的层。这一层的典型特点就是：使用实时数据（也会使用非实时数据），但不提供实时服务，而是提供一种近实时的服务。所谓近实时指的是越快越好，但并不强求像在线层一样在几十毫秒内给出结果，因为通常在近线层计算的结果会写入缓存系统，供在线层读取，做了一层隔离，因此对时效性无强要求。其典型代表是我们前面讲过的实时协同过滤算法，该算法通过用户的实时行为计算最新的相关性结果，但这些计算结果并不是实时提供给用户的，而是要等到用户发起请求时才会把最新的结果提供给他使用。

在具体实践中，经常放在离线层执行的任务主要包括：协同过滤等行为类相关性算法计算、用户标签挖掘、物品标签挖掘、用户长期兴趣挖掘、机器学习模型排序等。

![](../../picture/2/296.png)

通常放在近线层执行的任务包括实时指标统计、用户的实时兴趣计算、实时相关性算法计算、物品的实时标签挖掘、推荐结果的去重、机器学习模型统计类特征的实时更新、机器学习模型的在线更新等，这些任务通常会以如下两种方式进行计算。

个体实时：所谓个体实时，指的是每个实时数据点到来时都会触发一次计算，做到真正意义上的实时。

批量实时：很多时候并不需要到来一个实时数据点就计算一次，因为这会带来大量的计算和I/O，而是可以将一定的时间窗口或一定数量的数据收集起来，以小批次为单位进行计算，这可以有效减少I/O量。这种妥协对于很多应用来说，只要时间窗口不太大，就不会带来效果的显著下降。

![](../../picture/2/297.png)

在近线层中执行的典型任务包括但不限于：

- **特征的实时更新。**例如，根据用户的实时点击行为实时更新各维度的点击率特征。
- **用户实时兴趣的计算。**根据用户实时的喜欢和不喜欢行为计算其当下实时兴趣的变化。
- **物品实时标签的计算。**例如，在第6章用户画像系统中介绍过的实时提取标签的流程。
- **算法模型的在线更新。**通过实时消息队列接收和拼接实时样本，采用FTRL等在线更新算法来更新模型，并将更新后的模型推送到线上。
- **推荐结果的去重。**用户两次请求之间是有时间间隔的，所以无须在处理实时请求时进行去重，而是可以将这个信息通过消息队列发送给一个专门的服务，在近线层中处理。
- **实时相关性算法计算。**典型的如实时协同过滤算法，按照其原理，也可以把随机游走等行为类算法改写为实时计算，放到近线层中执行。

总结起来，凡是可以和实时请求解耦，但需要实时或近实时计算结果的任务，都可以放到近线层中执行。

在线层与离线层、近线层最大的差异在于，它是直接面对用户的，所有的用户请求都会发送到在线层，而在线层需要快速给出结果。在线层最本质的东西并不是在线计算部分，因为在极端情况下，在接收到用户请求之后，在线层可以直接从缓存或数据库中取出结果，返回给用户，而不做任何额外计算。

在线层的典型形态是一个RESTful API，对外提供服务。调用方传入的参数在不同公司的设计中差异较大，但基本都会包含访问用户的ID标识和推荐场景这两个核心信息，其他信息推荐系统都可以通过这两个信息从其他地方获取到。在线层接收到请求后会启动一套流程，将离线层和近线层生成的数据进行串联，在毫秒级响应时间内返回给调用方。这套流程的典型步骤包括：

- **AB实验分流**

根据用户ID或请求ID，决定当前用户要执行的策略版本。

- **获取用户画像**

根据传入的用户ID信息和场景信息，从Redis等缓存中获取用户的画像信息，用在后面的流程中。

- **相关性候选集召回**

包括行为相关性、内容相关性、上下文相关性、冷启动物品等多维度候选集的召回。

- **候选集融合排序**

将上面流程得到的候选集进行融合，再进一步进行机器学习模型排序，最后得到在算法上效果最优的结果列表。在当今推荐系统大量使用机器学习算法的背景下，这一部分的逻辑通常会比较复杂。而为了将机器学习模型预测这一越来越通用的逻辑和推荐主逻辑相剥离，通常也会为机器学习专门搭建一套在线系统，用来提供预测功能，包括对推荐结果的点击、转化预测。这样做的好处是机器学习模型的升级改造不会干扰到推荐系统本身，有利于模块化维护。

- **业务逻辑干预**

在完成算法逻辑之前或之后，还需要加入一些业务逻辑，例如去除或减少某些类别的物品，或者出于业务考虑插入一些在算法上非最优的结果，等等。

- **拼接展示信息**

在一些推荐系统中，推荐服务要负责将展示所需的所有信息集成到一起，这样调用方拿到结果后就可以直接展示了，而不需要再去获取其他内容。这看起来是一个负担，但从某些角度来看也是好事，因为我们可以做一些展示层面的个性化，典型的如根据不同的用户展示不同的图片或标题，要知道展示层对于用户是否对物品感兴趣是起着非常重要的作用的，毕竟这是一个处处看脸的时代。Netflix就做过剧集封面个性化的尝试，相比给所有人展示同样的封面，个性化封面使得在用户点击方面获得了显著的提升。

![](../../picture/2/298.png)

![](../../picture/2/299.png)