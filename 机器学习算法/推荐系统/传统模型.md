### 传统匹配模型

#### 基于Collaborative Filtering 的方法

##### CF模型

协同过滤基于一个最基本的假设：一个用户的行为，可以由和他行为相似的用户进行预测。协同过滤的基本思想是基于`<user, item>`的所有交互行为，利用集体智慧进行推荐。CF 按照类型可以分为3种，`user-based CF`、`item-based CF`和`model-based CF`。

协同过滤的思路要解决的问题用数据形式表达就是用户-物品的交互矩阵的未知部分如何填充问题。已知的值是用户已经交互过的物品，如何基于这些已知值填充矩阵剩下的未知值，也就是去预测用户没有交互过的物品是矩阵填充要解决的问题。

假设有$m$个用户$\mathbb{U}=\{u_1,\cdots,u_m\}$以及$n$个 `item`$\mathbb{I}=\{i_1,\cdots,i_n\}$。每个用户$u$有一个 `item` 评分列表$\mathbb{I}_u$，这个列表上用户有过评分，其中$\mathbb{I}_u\sub \mathbb{I}$且有可能为空。给定用户$u_a\in\mathbb{U}$，协同过滤算法任务包含两种形式：预测：预测用户$u_a$对于`item`$i_j \notin\mathbb{I}_{u_a}$的评分$P_{a,j}$；推荐：为用户$u_a$推荐他/她可能最喜欢的由$N$个 `item`组成的列表$\mathbb{I}_r \sub \mathbb{I}$，且$\mathbb{I}_r \cap \mathbb{I}=\empty$。

###### 基于用户的协同过滤

首先计算用户兴趣之间的相似性。给定用户 和 ，定义他们共同评分的`item`集合为：
$$
\mathbb{I}_{u,v} = \{j|r_{u,j}>0 \and r_{v,j}>0\}
$$
其中$r_{i,j}$表示用户$i$对于`item`$j$的评分，该评分可能是已知的，也可能是未知的.

然后计算用户$u,v$在这些共同评分`item`上的均值、方差、以及协方差，用户$u$和$v$的评分集合的相关系数：$s_{u,v}=\frac{\text{cov}(u,v)}{\sigma_u\sigma_v}$ 

预测用户$u$在未评分`item`$j^*$上的评分时，首先找出在$j^*$上存在评分的用户：$\mathbb{U}_{j^*}=\{i|r_{i,j^*}>0\}$。然后我们基于用户$u$和集合$\mathbb{U}$中用户的相似性来预测：
$$
w_v=\frac{s_{u,v}}{\sum_{v\in\mathbb{U}_{j^*}}s_{u,v}}\\
r_{u,j^*} = \overline{r}_u+\sum_{v\in \mathbb{U}_{j^*}}w_v(r_{v,j^*}-\overline{r}_v)
$$

###### 基于物品的协同过滤

首先探索`item` 之间的相似性；然后向用户推荐她/他喜欢的 `item` 类似的 `item` 。因为`item` 之间的关系是相对静态的，所以基于 `item` 的算法可以预计算`item-item` 相似性，从而减少在线计算的数量，同时保持较高的推荐质量。

选择目标用户$u_a$已经评分的`item` 集合$\mathbb{I}_{u_a}$，然后计算评分集合中的每个 `item` 和未评分集合$\mathbb{I}-\mathbb{I}_{u_a}$中每个 `item`$i_j$的相似度，然后选择$k$个最相似的、未评分的 `item`$\{i_1^a,\cdots,i_k^a\}$，并记录对应的相似度。

计算`item` 之间相似度的基本思想是：首先挑选既对$i$打分、又对$j$打分的用户$\mathbb{U}_{ij}$，然后基于这些用户的打分来计算相似度 。基于余弦的相似度：$sim(i,j) = \frac{\sum_{a\in\mathbb{U}_{ij}}r_{a,j}\times r_{a,j}}{\sqrt{\sum_{a\in\mathbb{U}_{ij}}r_{a,i}^2}\sqrt{\sum_{a\in\mathbb{U}_{ij}}r_{a,j}^2}}$

根据相似性得到了目标`item` 最相似的一组`item`，则下一步是为用户执行打分预测。

加权和 `weighted sum`：该方法通过用户已经评分的 `item` 中，根据和$i$的相似度进行加权得到预测结果。假设用户$a$已经评分，且与$i$相似的 `item` 集合为$\mathbb{S}_{i,a}$，则有
$$
P_{a,i} =\sum_{j\in\mathbb{S}_{i,a}}\omega_{ij}\times r_{a,j}\\
\omega_{i,j} = \frac{s_{i,j}}{\sum_{\mathbb{S}_{i,a}}s_{i,j}}
$$
回归`Regression`：采用加权和相同的公式，但是不使用原始打分$r_{a,j}$，而是使用基于线性回归的近似值$r_{a,j}^{\prime}$。假设目标 `item`$i$的打分向量为$\vec{\mathbf{r}}_{\cdot,i}$，相似 `item`$j$的修正打分为：$\vec{\mathbf{r}}_{\cdot,j}=\alpha_{i,j}\vec{\mathbf{r}}_{\cdot,i}+\beta_{i,j}$。然后最小化来求解参数$\alpha_{i,j},\beta_{i,j}$。最终有：
$$
P_{a,i} =\sum_{j\in\mathbb{S}_{i,a}}\omega_{ij}\times (\alpha_{i,j}\times r_{a,j}+\beta_{i,j})\\
\omega_{i,j} = \frac{s_{i,j}}{\sum_{\mathbb{S}_{i,a}}s_{i,j}}
$$

###### 基于模型的协同过滤

通过定义一个参数模型来描述用户和物品、用户和用户、物品和物品之间的关系，然后通过已有的用户-物品评分矩阵来优化求解得到参数。例如矩阵分解、隐语义模型`LFM`等。

![](../../picture/1/233.png)

矩阵填充可以用经典的`SVD`解决，一般来说`SVD`求解可以分为三步：对 M 矩阵的缺失值填充为0；求解`SVD`问题，得到`U`矩阵和`V`矩阵；利用`U`和`V`矩阵的低秩`k`维矩阵来估计。

对于第二中的`SVD`求解问题等价于以下的最优化问题
$$
\text{argmin}_{U,\Sigma, V}(Y-U\Sigma V^T)^2=\text{arg min}_{U,\Sigma,V}\sum_{i=1}^{m}\sum_{j=1}^{n}(y_{ij}-(U\Sigma V^T)_{ij})^2
$$


其中$y_{ij}$为用户$i$对物品$j$的真实评分，也就是 label，$U$和$V$为模型预估值，求解矩阵$U$和$V$的过程就是最小化用户真实评分矩阵和预测矩阵误差的过程。

这种`SVD`求解方法存在以下问题：缺失数据和可观测数据权重一样；最小化过程没有正则化，容易产生过拟合。

##### `MF`模型（矩阵分解）

矩阵分解模型提出的模型如下：$\hat{y}_{ui} = \mathbf{v}_u^T\mathbf{v}_i$。

`MF`模型的核心思想可以分成两步：将用户$u$对物品$i$的打分分解成用户的隐向量$\mathbf{v}_u$，以及物品的隐向量$\mathbf{v}_i$；用户$u$和物品$i$的向量点积得到的值，可以用来代表用户$u$对物品$i$的喜好程度，分数越高代表该物品推荐给用户的概率就越大。同时，`MF`模型引入了$l_2$正则来解决过拟合问题。
$$
L=\sum_{u}\sum_{i}w_{ui}(y_{ui}-\hat{y}_{ui})^2+\lambda(\sum_u||\mathbf{v}_u||^2+\sum_i||\mathbf{v}_i||^2)
$$

##### `FISM`模型

将用户喜欢过的物品作为用户的表达来刻画用户，用数据公式表示如下：
$$
\hat{y}_{ui} = (\sum_{j \in \mathcal{R}_u}\mathbf{q}_j)^T\mathbf{v}_i
$$
其中$\mathcal{R}_u$是用户$u$交互过的商品的集合，对于每一个物品，都有两个嵌入向量$\mathbf{q}$和$\mathbf{v}$，$\mathbf{q}$是商品作为交互历史中的物品时的嵌入向量，$\mathbf{v}$时物品是预测的物品时的嵌入向量。

用户表达不再是独立的隐向量，而是用用户喜欢过的所有物品的累加求和得到作为用户的表达；而 物品本身的隐向量$\mathbf{v}_i$是另一套表示，两者最终同样用向量内积表示。

##### SVD++模型

`MF`模型可以看成是 user-based 的 CF 模型，直接将用户id映射成隐向量，而`FISM`模型可以看成是 item-based 的`CF`模型，将用户交户过的 item 的集合映射成隐向量。`SVD++`正是这两者的结合，数学表达如下：
$$
\hat{y}_{ui} = (\mathbf{v}_u+\sum_{j \in \mathcal{R}_u}\mathbf{q}_j)^T\mathbf{v}_i
$$
每个用户表达分成两个部分，左边$\mathbf{v}_u$表示用户 id 映射的隐向量，右边是用户交互过的 item 集合的求和。User 和 item 的相似度还是用向量点击来表达。

#### Generic feature-based的方法

上述的方法中都只是利用了用户和商品的交互信息，而对于大量的 side information 信息没有利用到。因此，下面将介绍如何利用这些特征，去构造 feature-based 的 model。特征体系三模块：用户信息、物品信息、交互信息

![](../../picture/1/234.png)

##### `POLY2`模型

`LR` 模型只考虑特征之间的线性关系，而`POLY2` 模型考虑了特征之间的非线性关系。捕获非线性特征的一个常用方法是采用核技巧，但是核技巧存在计算量大，通过多项式映射 `polynomially mapping` 数据的方式来提供非线性特征，在达到接近核技巧效果的情况下大幅度降低内存和计算量。仅仅考虑使用一个多项式映射，则我们得到
$$
\phi(\vec{\mathbf{x}}) = [1,x_1,\cdots,x_n,x_1^2,\cdots,x_n^2,x_1x_2,\cdots,x_{n-1}x_n]^T
$$
结合`LR` 模型，则得到 `POLY2` 模型：
$$
z(\vec{\mathbf{x}})=\omega_0+\sum_{i=1}^p\omega_ix_i+\sum_{i=1}^{K}\sum_{j=i+1}^{K}\omega_{ij}\times x_i\times x_j\\
y(vec{\mathbf{x}})=\frac{1}{1+\exp(-z(\vec{\mathbf{x}}))}
$$

优点：除了线性特征之外，还能够通过特征组合自动捕获二阶特征交叉产生的非线性特征。

缺点：参数太多导致计算量和内存需求发生爆炸性增长。数据稀疏导致二次项参数训练困难，非常容易过拟合。

##### FM模型

将样本为：$\vec{\mathbf{x}}=(x_1,x_2,\cdots,x_K)^T$

则 `FM` 模型为：$\hat{y}(\mathbf{x})=\omega_0+\sum_{i=1}^K\omega_ix_i+\sum_{i=1}^{K}\sum_{j=i+1}^{K}\hat{w}_{i,j}x_ix_j$

其中$\hat{w}_{i,j}$是交叉特征的参数，它由一组参数定义：$\hat{w}_{i,j}=<\vec{\mathbf{x}}_i,\vec{\mathbf{x}}_j>=\sum_{l=1}^dv_{i,l}\times v_{j,l}$
$$
\hat{\mathbf{W}} = \left[\begin{array}{cccc}\hat{w}_{1,1}&\hat{w}_{1,2}&\cdots&\hat{w}_{1,K}\\
\hat{w}_{2,1}&\hat{w}_{2,2}&\cdots&\hat{w}_{2,K}\\
\cdot&\cdot&\cdots&\cdot\\
\hat{w}_{K,1}&\hat{w}_{K,2}&\cdots&\hat{w}_{K,K}\end{array}\right]=\mathbf{V}^T\mathbf{V}=\left[\begin{array}{cccc}\vec{\mathbf{v}}_1^T\\
\vec{\mathbf{v}}_2^T\\
\cdot\\ \cdot \\ \cdot\\\vec{\mathbf{v}}_K^T
\end{array}\right]\left[\begin{array}{cccc}\vec{\mathbf{v}}_1^T  &  \vec{\mathbf{v}}_2^T&\cdots&\vec{\mathbf{v}}_K^T
\end{array}\right]
$$
模型待求解的参数为：$\omega_0\in \mathbb{R},\vec{\mathbf{w}}\in \mathbb{R}^n, \mathbf{V}=(\vec{\mathbf{v}}_i,\cdots,\vec{\mathbf{x}}_K)\in \mathbb{R}^{d\times K}$

对于每个输入特征$x_i$，模型都需要学习一个低维的隐向量表达$\mathbf{v}_i$。
$$
\\
\begin{array}{l}\sum_{i=1}^{K}\sum_{j=i+1}^{K}\hat{w}_{i,j}x_ix_j &=\sum_{i=1}^{K}\sum_{j=i+1}^{K}(\sum_{l=1}^dv_{il}v_{j_l})x_ix_j\\
&=\sum_{l=1}^d[\sum_{i=1}^{K}\sum_{j=i+1}^{K}(v_{il}x_i)(v_{jl}x_j)]\\
&=\sum_{l=1}^d\frac{1}{2}[(\sum_{i=1}^{K}v_{il}x_i)^2-\sum_{i=1}^{K}(v_{il}x_i)^2]
\end{array}
$$
因此有：$\hat{y}(\vec{\mathbf{x}})=\omega_0+\sum_{i=1}^K\omega_ix_i+\sum_{l=1}^d\frac{1}{2}[(\sum_{i=1}^{K}v_{il}x_i)^2-\sum_{i=1}^{K}(v_{il}x_i)^2]$

`FM` 模型可以用于求解分类问题 ，也可以用于求解回归问题。

- 对于回归问题，其损失函数为`MSE` 均方误差

$$
\mathcal{L} = \sum_{(\vec{\mathbf{x}},y)\in\mathbb{S}}(\hat{\vec{\mathbf{x}}}-y)^2+\sum_{\theta\in\Theta}\lambda_\theta\times\theta^2
$$

- 对于二分类问题，其损失函数为交叉熵：

$$
p(\hat{y}=y|\vec{\mathbf{x}})=\frac{1}{1+\exp(-y\phi(\vec{\mathbf{x}}))}\\
\mathcal{L} = -\sum_{(\vec{\mathbf{x}},y)\in\mathbb{S}}\log p(\hat{y}=y|\vec{\mathbf{x}}) + \sum_{\theta\in\Theta}\lambda_\theta\times\theta^2
$$

其中$\mathbf{\Theta}=\{w_0,\vec{\mathbf{w}},\mathbf{V}\}$，其中$\lambda_\theta$为参数$\theta$的正则化系数

###### `ALS`优化算法

$$
\hat{y}(\vec{\mathbf{x}};\mathbf{\Theta})=\omega_0+\sum_{i=1}^K\omega_ix_i+\sum_{l=1}^d\frac{1}{2}[(\sum_{i=1}^{K}v_{il}x_i)^2-\sum_{i=1}^{K}(v_{il}x_i)^2]
$$

对每个$\theta$，可以将$\hat{y}$分解为$\theta$的线性部分和偏置部分：$\hat{y}(\vec{\mathbf{x}};\mathbf{\Theta})=h_\theta(\vec{\mathbf{x}})\times \theta +g_\theta(\vec{\mathbf{x}})$。其中$h_\theta(\vec{\mathbf{x}}),g_\theta(\vec{\mathbf{x}})$与$\theta$无关。

对于$w_0$有：
$$
h_{w_{0}}=1\\
g_{w_0} = \sum_{i=1}^K\omega_ix_i+\sum_{l=1}^d\frac{1}{2}[(\sum_{i=1}^{K}v_{il}x_i)^2-\sum_{i=1}^{K}(v_{il}x_i)^2]
$$
对于$w_i,i=1,2,\cdots,K$有：
$$
h_{w_i} = x_i\\
g_{w_i}=\omega_0+\sum_{j=1,j\ne i}^K\omega_jx_j+\sum_{l=1}^d\frac{1}{2}[(\sum_{j=1}^{K}v_{jl}x_j)^2-\sum_{j=1}^{K}(v_{jl}x_j)^2]
$$
对于$v_{i,l},i=1,2,\cdots,K;l=1,2.\cdots,d$有：
$$
h_{v_{i,l}} = \sum_{j=1,j\ne i}^Kv_{j,l}\times x_j\times x_i\\
g_{v_{i,l}} = \omega_0+\sum_{i=1}^K\omega_ix_i+\sum_{i^{\prime}=1}^K\sum_{j^{\prime}=i^{\prime}+1}^K\sum_{l^{\prime}=1,(l^{\prime},i^{\prime},j^{\prime})\ne (l,i,i)}v_{i^{\prime},l^{\prime}}\times v_{j^{\prime},l^{\prime}}\times x_{i^{\prime}}\times x_{j^{\prime}}
$$
考虑均方误差损失函数，最小值点的偏导数为 0 ，则有：
$$
\theta = -\frac{\sum_{(\vec{\mathbf{x}},y)\in\mathbb{S}}(g_\theta(\vec{\mathbf{x}})-y)\times h_\theta(\vec{\mathbf{x}})}{\sum_{(\vec{\mathbf{x}},y)\in\mathbb{S}}h^2_\theta(\vec{\mathbf{x}})+\lambda_\theta}
$$
`ALS` 通过多轮次、轮流迭代求解$\theta\in\mathbf{\Theta}$即可得到模型的最优解。在迭代之前初始化参数，其中：$w_0,\vec{\mathbf{w}}$通过零初始化，$\mathbf{V}$的每个元素通过均值为0、方差为$\sigma$的正太分布随机初始化。每一轮迭代时：首先求解$w_0,\vec{\mathbf{w}}$，因为相对于二阶交叉的高阶特征，低阶特征有更多的数据来估计其参数，因此参数估计更可靠。然后求解$\mathbf{V}$。这里按照维度优先的准确来估计：先估计所有`representation` 向量的第 `1` 维度，然后是第 `2` 维，... 最后是第 `d` 维。

##### `FFM`模型

每个特征$x_i$的隐向量应该有多个，当该特征与不同域的特征做交叉时，应该使用对应的隐向量，这样可以做到更精细地刻画交叉特征的权重。所以每个特征应该有和field数量相同的隐向量个数。

| 域   | 性别 |      | 年龄 |      |      |      | 价格 |      |       |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ----- |
| 特征 | 男   | 女   | 15   | 25   | 50   | >50  | 500  | 1000 | >1000 |
| 样本 | 1    | 0    | 0    | 0    | 1    | 0    | 0    | 1    | 0     |

在 `FM` 模型中，每个特征的表示向量只有一个。`FFM` 中每个特征的 表示向量有多个，用于捕捉该特征在不同`field` 中的含义。

假设样本共有$F$个`field`，特征$x_i,x_j$分别属于域$f_i,f_j$，$x_i$有$F$个隐向量$(\mathbf{v}_{i,1},\cdots,\mathbf{v}_{i,F})$，$\mathbf{v}_i$是一个$d$维向量。
$$
\hat{y}(\vec{\mathbf{x}})=\omega_0+\sum_{i=1}^K\omega_ix_i+\sum_{i=1}^K\sum_{j=i+1}^K\hat{\omega}_{ij}\times x_i\times x_j\\
\hat{\omega}_{ij}=<\vec{\mathbf{v}}_{i,f_j},\vec{\mathbf{v}}_{j,f_i}>
$$

和 `FM` 相同，`FFM` 模型也可以用于求解分类问题，也可以用于求解回归问题 

`FFM` 模型需要为每个特征分配一个 `field` ，离散型特征：通常对离散型特征进行 `one-hot` 编码，编码后的所有二元特征都属于同一个 `field` 。数值型特征：数值型特征有两种处理方式：不做任何处理，简单的每个特征分配一个`field` 。数值特征离散化之后，按照离散型特征分配 `field` 。

##### `GBDT+LR`

其本质上是通过Boosting Tree模型本身的特征组合能力来替代原先算法工程师们手动组合特征的过程。GBDT等这类Boosting Tree模型本身具备了特征筛选能力（每次分裂选取增益最大的分裂特征与分裂点）以及高阶特征组合能力（树模型天然优势），因此通过GBDT来自动生成特征向量就成了一个非常自然的思路。虽然是两个模型的组合，但实际并非是端到端的模型，而是两阶段的、解耦的，即先通过GBDT训练得到特征向量后，再作为下游LR的输入，LR的在训练过程中并不会对GBDT进行更新。

![](../../picture/1/303.png)



通过手动设计交叉特征以及特征离散化等方式，赋予LR这样的线性模型对数据集的非线性学习能力，高维离散特征+手动交叉特征构成了CTR预估的基础特征。