##### 异常点检测

In data mining, anomaly detection is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.
Anomalies can be broadly categorized as:

Point anomalies: A single instance of data is anomalous if it's too far off from the rest. Business use case: Detecting credit card fraud based on "amount spent."

Contextual anomalies: The abnormality is context specific. This type of anomaly is common in time-series data. Business use case: Spending $100 on food every day during the holiday season is normal, but may be odd otherwise.

Collective anomalies: A set of data instances collectively helps in detecting anomalies.

- Anomaly detection is similar to — but not entirely the same as — noise removal and novelty detection. 

- Novelty detection is concerned with identifying an unobserved pattern in new observations not included in training data

- Noise removal is the process of removing noise from an otherwise meaningful signal. 

###### 1. Anomaly Detection Techniques

#### Simple Statistical Methods

The simplest approach to identifying irregularities in data is to flag the data points that deviate from common statistical properties of a distribution, including mean, median, mode, and quantiles. Let's say the definition of an anomalous data point is one that deviates by a certain standard deviation from the mean. Traversing mean over time-series data isn't exactly trivial, as it's not static. You would need a rolling window to compute the average across the data points. Technically, this is called a ***rolling average or a moving average***, and it's intended to smooth short-term fluctuations and highlight long-term ones. Mathematically, an n-period simple moving average can also be defined as a ***"low pass filter."***

#### Challenges with Simple Statistical Methods

The low pass filter allows you to identify anomalies in simple use cases, but there are certain situations where this technique won't work. Here are a few:  

- The data contains noise which might be similar to abnormal behavior, because the boundary between normal and abnormal behavior is often not precise. 

- The definition of abnormal or normal may frequently change, as malicious adversaries constantly adapt themselves. Therefore, the threshold based on moving average may not always apply.

- The pattern is based on seasonality. This involves more sophisticated methods, such as decomposing the data into multiple trends in order to identify the change in seasonality.

###### 2. Machine Learning-Based Approaches

###### a.Density-Based Anomaly Detection 

Density-based anomaly detection is based on the k-nearest neighbors algorithm.

Assumption: Normal data points occur around a dense neighborhood and abnormalities are far away. 

The nearest set of data points are evaluated using a score, which could be Eucledian distance or a similar measure dependent on the type of the data (categorical or numerical). They could be broadly classified into two algorithms:

***K-nearest neighbor***: k-NN is a simple, non-parametric lazy learning technique used to classify data based on similarities in distance metrics such as Eucledian, Manhattan, Minkowski, or Hamming distance.

***Relative density of data***: This is better known as local outlier factor (LOF). This concept is based on a distance metric called reachability distance.

###### b.Clustering-Based Anomaly Detection 

Clustering is one of the most popular concepts in the domain of unsupervised learning.

Assumption: Data points that are similar tend to belong to similar groups or clusters, as determined by their distance from local centroids.

***K-means*** is a widely used clustering algorithm. It creates 'k' similar clusters of data points. Data instances that fall outside of these groups could potentially be marked as anomalies.

c.Support Vector Machine-Based Anomaly Detection 

- A support vector machine is another effective technique for detecting anomalies. 
- A SVM is typically associated with supervised learning, but there are extensions (OneClassCVM, for instance) that can be used to identify anomalies as an unsupervised problems (in which training data are not labeled). 
- The algorithm learns a soft boundary in order to cluster the normal data instances using the training set, and then, using the testing instance, it tunes itself to identify the abnormalities that fall outside the learned region.
- Depending on the use case, the output of an anomaly detector could be numeric scalar values for filtering on domain-specific thresholds or textual labels (such as binary/multi labels).


In this jupyter notebook we are going to take the credit card fraud detection as the case study for understanding this concept in detail using the following Anomaly Detection Techniques namely

Now it is time to start building the model .The types of algorithms we are going to use to try to do anomaly detection on this dataset are as follows

2. Local Outlier Factor(LOF) Algorithm

The LOF algorithm is an unsupervised outlier detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outlier samples that have a substantially lower density than their neighbors.

The number of neighbors considered, (parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.

　第一类是基于统计学的方法来处理异常数据，这种方法一般会构建一个概率分布模型，并计算对象符合该模型的概率，把具有低概率的对象视为异常点；第二类是基于聚类的方法来做异常点检测；第三类是基于专门的异常点检测算法来做。这些算法不像聚类算法，检测异常点只是一个赠品，它们的目的就是专门检测异常点的。

###### One Class `SVM`算法

假设产生的超球体参数为中心$ o$和对应的超球体半径$ r>0$，超球体体积 $V(r)$被最小化，中心$o$是支持向量的线性组合；跟传统$SVM$方法相似，可以要求所有训练数据点 $x_i $到中心的距离严格小于$ r$，但同时构造一个惩罚系数为$ C$的松弛变量$ ξ_i$，优化问题如下所示：
$$
\begin{array}{c}{\underbrace{\min }_{r, o} V(r)+C \sum_{i=1}^{m} \xi_{i}} \\ {\left\|x_{i}-o\right\|_{2} \leq r+\xi_{i}, \quad i=1,2, \ldots m} \\ {\xi_{i} \geq 0, \quad i=1,2, \ldots m}\end{array}
$$

###### Isolation Forest算法

The algorithm is based on the fact that anomalies are data points that are few and different. As a result of these properties, anomalies are susceptible to a mechanism called isolation.

This method is highly useful and is fundamentally different from all existing methods. It introduces the use of isolation as a more effective and efficient means to detect anomalies than the commonly used basic distance and density measures. Moreover, this method is an algorithm with a low linear time complexity and a small memory requirement. It builds a good performing model with a small number of trees using small sub-samples of fixed size, regardless of the size of a data set.

Typical machine learning methods tend to work better when the patterns they try to learn are balanced, meaning the same amount of good and bad behaviors are present in the dataset.

How Isolation Forests Work

The Isolation Forest algorithm isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The logic argument goes: isolating anomaly observations is easier because only a few conditions are needed to separate those cases from the normal observations. On the other hand, isolating normal observations require more conditions. Therefore, an anomaly score can be calculated as the number of conditions required to separate a given observation.

The way that the algorithm constructs the separation is by first creating isolation trees, or random decision trees. Then, the score is calculated as the path length to isolate the observation.

第一步训练构建随机森林对应的多颗决策树，这些决策树一般叫$iTree$，第二步计算需要检测的数据点$x$最终落在任意第$t$颗$iTree$的层数$h_t(x)$。然后我们可以得出$x$在每棵树的高度平均值$h(x)$。第三步根据$h(x)$判断$x$是否是异常点。首先采样决策树的训练样本时，普通的随机森林要采样的样本个数等于训练集个数。但是$iForest$不需要采样这么多，一般来说，采样个数要远远小于训练集个数。原因是我们的目的是异常点检测，只需要部分的样本我们一般就可以将异常点区别出来了。另外就是在做决策树分裂决策时，由于我们没有标记输出，所以没法计算基尼系数或者和方差之类的划分标准。这里我们使用的是**随机选择划分特征**，然后在基于这个特征**再随机选择划分阈值**，进行决策树的分裂。直到树的深度达到限定阈值或者样本数只剩一个。第二步计算要检测的样本点在每棵树的高度平均值$h(x)$。首先需要遍历每一颗$iTree$，得到检测的数据点$x$最终落在任意第t颗$iTree$的数层数$h_t(x)$$h_t(x)$代表的是树的深度，也就是离根节点越近，则$h_t(x)$越小，越靠近底层，则$h_t(x)$越大，根节点的高度为$0$。第三步是据$h(x)$判断$x$是否是异常点。我们一般用下面的公式计算$x$的异常概率分值：
$$
s(x, m)=2^{-\frac{n(x)}{c(m)}}\\
c(m)=2 \ln (m-1)+\xi-2 \frac{m-1}{m}
$$
$ξ$是欧拉常数，$\text{IForest}$具有线性时间复杂度。因为是随机森林的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。由于每棵树都是互相独立生成的，因此可以部署在大规模分布式系统上来加速运算。但是有缺$\text{IForest}$点，不适用于特别高维的数据。另外仅对即$\text{IForest}$全局稀疏点敏感，不擅长处理局部的相对稀疏点 ，这样在某些局部的异常点较多的时候检测可能不是很准。而$\text{OneClassSVM}$对于中小型的数据分析，尤其是训练样本不是特别海量的时候用起来经常会比$\text{IForest}$顺手，因此比较适合做原型分析。

##### `Wassertein`距离

 假设从未知数据分布$p(x)$中观测到一些随机变量$x$，我们想要找到一个模型$q(x|θ)$能作为$p(x)$的一个很好的近似。 因为KL散度可以度量两个分布的距离，所以只需要最小化$KL(q‖p)$就可以了。尽管 KL 散度有很广泛的应用，在某些情况下，KL 散度则会失效。不妨考虑一下如下图所示的离散分布：

![](D:/MarkDown/picture/work/62.png)

 KL散度假设这两个分布共享相同的支撑集，也就是说，它们被定义在同一个点集上。因此，我们不能为上面的例子计算KL散度。

######  **移动概率质量函数** 

 不妨把离散的概率分布想象成空间中分散的点的质量。我们可以观测这些带质量的点从一个分布移动到另一个分布需要做多少功，如下图所示： 

![](D:/MarkDown/picture/work/63.png)

 接着，我们可以定义另一个度量标准，用以衡量移动做所有点所需要做的功。要想将这个直观的概念形式化定义下来，首先，我们可以通过引入一个耦合矩阵 P，它表示要从`p(x)`支撑集中的一个点上到`q(x)`支撑集中的一个点需要分配多少概率质量。对于均匀分布，我们规定每个点都具有$\frac{1}{4}$的概率质量。 
$$
\mathbf{P}=\left(\begin{array}{cccc}{0} & {0} & {0} & {\frac{1}{4}} \\ {0} & {0} & {\frac{1}{4}} & {0} \\ {0} & {\frac{1}{4}} & {0} & {0} \\ {\frac{1}{4}} & {0} & {0} & {0}\end{array}\right)
$$
 也就是说，$p(x)$支撑集中点1的质量被分配给了$q(x)$支撑集中的点4，$p(x)$支撑集中点2的质量被分配给了$q(x)$支撑集中的点3，以此类推，如上图中的箭头所示。 

 为了算出质量分配的过程需要做多少功，我们将引入第二个矩阵：距离矩阵。该矩阵中的每个元素$C_{ij}$表示将$p(x)$支撑集中的点移动到$q(x)$支撑集中的点上的成本。点与点之间的欧几里得距离是定义这种成本的一种方式，它也被称为`ground distance`。如果我们假设$p(x)$的支撑集和$q(x)$的支撑集分别为$\{1,2,3,4\}$和$\{5,6,7,8\}$，成本矩阵即为： 
$$
\mathbf{C}=\left(\begin{array}{llll}{4} & {5} & {6} & {7} \\ {3} & {4} & {5} & {6} \\ {2} & {3} & {4} & {5} \\ {1} & {2} & {3} & {4}\end{array}\right)
$$
 根据上述定义，总的成本可以通过$P$和$C$之间的`Frobenius`内积来计算： $\langle\mathbf{C}, \mathbf{P}\rangle=\sum_{i j} \mathbf{C}_{i j} \mathbf{P}_{i j}$

实际上有很多种方法可以把点从一个支撑集移动到另一个支撑集中，每一种方式都会得到不同的成本。上面给出的只是一个示例，但是我们感兴趣的是最终能够让成本较小的分配方式。这就是两个离散分布之间的「最优传输」问题，该问题的解是所有耦合矩阵上的最低成本 L_C。由于不是所有矩阵都是有效的耦合矩阵，最后一个条件会引入了一个约束。对于一个耦合矩阵来说，其所有列都必须要加到带有 q(x) 概率质量的向量中。
$$
\begin{aligned} \mathrm{L}_{\mathrm{C}} &=\min _{\mathbf{P}}\langle\mathbf{C}, \mathbf{P}\rangle \\ \text { subject to } \mathbf{P} \mathbf{1} &=\mathbf{a} \\ \mathbf{P}^{\top} \mathbf{1} &=\mathbf{b} \end{aligned}
$$

######  熵正则化和`Sinkhorn`迭代 

 矩阵的熵定义如下： $H(\mathbf{P})=-\sum_{i j} \mathbf{P}_{i j} \log \mathbf{P}_{i j}$

 我们可以将正则化系数 ε 引入最优传输问题，从而得到更平滑的耦合矩阵： 
$$
\begin{aligned} \mathrm{L}_{\mathrm{C}} &=\min _{\mathbf{P}}\langle\mathbf{C}, \mathbf{P}\rangle-\varepsilon H(\mathbf{P}) \\ \text { subject to } \mathbf{P} \mathbf{1} &=\mathbf{a} \\ \mathbf{P}^{\top} \mathbf{1} &=\mathbf{b} \end{aligned}
$$
 通过增大 ε，最终得到的耦合矩阵将会变得更加平滑；而当 ε 趋近于零时，耦合矩阵会更加稀疏，同时最终的解会更加趋近于原始最优运输问题。  通过引入这种熵正则化，该问题变成了一个凸优化问题，并且可 以通过使用`Sinkhorn iteration`求解。解可以被写作$P=diag(u)Kdiag(v)$，在迭代过程中交替更新$u$和$v$： 
$$
\begin{aligned} \mathbf{u}^{(k+1)} &=\frac{\mathbf{a}}{\mathbf{K} \mathbf{v}^{(k)}} \\ \mathbf{v}^{(k+1)} &=\frac{\mathbf{b}}{\mathbf{K}^{\top} \mathbf{u}^{(k+1)}} \end{aligned}
$$
 其中$K$是一个用$C$计算的核矩阵。 

#### 最优化

最优化问题的一般提法是要选择一组参数，在满足一系列有关的限制条件下，使设计指标达到最优值

最优化问题分为函数优化问题和组合优化问题两大类，其中函数优化的对象是一定区间的连续变量，而组合优化的对象则是解空间中的离散状态。

最优化算法有三要素：变量、约束条件和目标函数。最优化算法，其实就是一种搜索过程或规则，它是基于某种思想和机制，通过一定的途径或规则来得到满足用户要求的问题的解。

![](../../picture/1/126.png)

领域搜索算法。从任一解出发，对其领域的不断搜索和当前解的替换来实现优化。根据搜索行为，它又可分为局部搜索法和指导性搜索法。
局部领域搜索法（也称爬山法）。以局部优化策略在当前解的领域中贪婪搜索，如只接受优于当前解的状态作为下一当前解的爬山法；接受当前邻域中的最好解作为下一当前解的最陡下降法等。
指导性搜索法。利用一些指导规则来指导整个解空间中优良解的探索，如SA、GA、EP、ES和TS等.

爬山算法
算法思想：从当前的节点开始，和周围的邻居节点的值进行比较。如果当前节点是最大的，那么返回当前节点，作为最大值(即山峰最高点)；反之就用最高的邻居节点替换当前节点，从而实现向山峰的高处攀爬的目的。

禁忌算法(Tabu Search，TS)
基本思想：基于爬山算法的改进，标记已经解得的局部最优解或求解过程，并在进一步的迭代中避开这些局部最优解或求解过程。局部搜索的缺点在于，太过于对某一局部区域以及其邻域的搜索，导致一叶障目。为了找到全局最优解，禁忌搜索就是对于找到的一部分局部最优解，有意识地避开它，从而或得更多的搜索区域
模拟退火(simulated annealing，SA)
模拟退火算法作为局部搜索算法的扩展，在每一次修改模型的过程中，随机产生一个新的状态模型，然后以一定的概率选择邻域中能量值大的状态．这种接受新模型的方式使其成为一种全局最优算法，并得到理论证明和实际应用的验证．SA虽然在寻优能力上不容置疑，但它是以严密的退火计划为保证的，具体地讲，就是足够高的初始温度、缓慢的退火速度、大量的迭代次数及同一温度下足够的扰动次数。

###### 粒子群优化算法

鸟群有三个决定其搜索结果的参数：$C_1$:自我学习因子，$C_2$:全局学习因子。对于每只鸟，有两个属性：

位置：$\mathbf{X}_i^t = (x_{i,1}^T,x_{1,2}^t,\dots, x_{i,D}^t)$

速度：$\mathbf{V}_i^t = (v_{i,1}^T,v_{1,2}^t,\dots, v_{i,D}^t)$

其中$t$表示第$t$次迭代,$i$表是这只鸟的序号是$i$，$D$表示搜索空间的维度.
$$
v_{i,d}^{t+1} = v_{i,d}^t +r_1C_1(P_{i,d}^t - x_{i,d}^t) + r_2C_2(G_{i,d}^t-x_{i,d}^t)\\
x_{i,d}^{t+1} = x_{i,d}^t + v_{i,d}^{t+1}
$$

###### 差分进化算法

差分进化算法中，每一个个体的基因表示待求问题的一个候选解。每次迭代将先进行变异操作，选择一个或多个个体的基因作为基，然后选择不同的个体的差分来构成差分基因，最后将作为基的基因与差分基因相加来得出新的个体。交叉操作将新的个体将于父代的对应个体交叉，然后进行选择操作，比较交叉后的个体与父代的对应个体，选择较优的个体保留至下一代。在迭代完成之后将选择种群中最优个体的基因作为解。

差分变异：$U_i = X_{r1} +\alpha\times (X_{r2}-X_{r3})$

交叉：$v_{i,d} = u_{i,d} \text{ or } x_{i,d}$

###### 人工蜂群算法

　采蜜蜂：也有叫雇佣蜂，蜜源的发现者，发现蜜源后会去招募观察蜂小伙伴来一同开采这个蜜源。
 　观察蜂：也有叫非雇佣蜂、跟随蜂等，在等来数只采蜜蜂来招募时，观察蜂会在众多的采蜜蜂中选择一只跟随去开采采蜜蜂所发现的蜜源，直到该蜜源被开采完。
 　侦查蜂：不跟随任何其他蜜蜂，自己寻找蜜源，找到之后则会转变为采蜜蜂去招募观察蜂。
 　每只蜜蜂第t代的位置如下，每一个位置都代表一个蜜源，蜜源值越优对蜜蜂的吸引越大

三种蜜蜂之间可以相互转化。

 　采蜜蜂->观察蜂：有观察蜂在采蜜过程中发现了比当前采蜜蜂更好的蜜源，则采蜜蜂放弃当前蜜源转而变成观察蜂跟随优质蜜源，同时该观察蜂转变为采蜜蜂。
 　采蜜蜂->观察蜂：当该采蜜蜂所发现的蜜源被开采完后，它会转变为观察蜂去跟随其他采蜜蜂。
 　采蜜蜂->侦查蜂：当所有的采蜜蜂发现的蜜源都被开采完后，采蜜蜂将会变为侦查蜂，观察蜂也会变成侦查蜂，因为大家都无蜜可采。
 　侦查蜂->采蜜蜂、观察蜂：侦查蜂随机搜索蜜源，选择较好的数个蜜源位置的蜜蜂为采蜜蜂，其他蜜蜂为观察蜂。

###### 杜鹃搜索算法

杜鹃有两种产卵的策略：

   　1.    列维飞行，杜鹃将在通过列维飞行所找到的与之前的寄生巢对比，选择较优的寄生巢作为下一代的寄生巢
            　2.    随机选择，每个寄生巢的主人都有一定的几率发现自己的巢被寄生。发现后，杜鹃将随机选择一个新的鸟巢作为自己的寄生巢。

在D维解空间内每个鸟巢的位置为: $X=(x_1,x_2,\cdots,x_d)$

第t+1代时，杜鹃将根据第t代的寄生巢的位置，结合列维飞行求得新的寄生巢的位置，飞行公式如下

$x_{i,d}^{t+1} = x_{i,d}^t + \alpha \times Levy(\lambda)$

实际的飞行公式如下：$x_{i,d}^{t+1} = x_{i,d}^t + \alpha \times Levy(\lambda)\times (x_{best,d}^t-x_{i,d}^t)$

Levy 过程直观上讲,可以看做连续时间的随机游动 .它的特征是有平稳 独立的增量, 重要的 Levy 过程有 Brown 运动, Poisson 过程, Cauchy 过程等

###### 萤火虫算法

一句话简述萤火虫算法流程：**每只萤火虫都向着看上去比自己更亮的萤火虫飞行。**

在D维解空间内每个萤火虫的位置为

萤火虫之间的相对吸引度由以下公式:$\beta(r) = \beta_0e^{-\sigma r^2}$

$\beta_0$为其初始吸引度，即两只萤火虫之间距离为0时的吸引度，r为两只萤火虫之间的距离。

算法运行过程中，每只萤火虫将会朝着所有亮度比自己高的所有萤火虫移动

$X_i^\prime = X_i + \beta(r)(X_i-X_j) + \alpha\times rand$

其中![X_i](https://math.jianshu.com/math?formula=X_i)表示一个比第i个个体亮度更高的萤火虫的位置，r表示第i个萤火虫与第j个萤火虫之间的距离。rand()为一个随机扰动，![\alpha](https://math.jianshu.com/math?formula=%5Calpha)为扰动的步长因子。一般rand()取值为[-0.5,0.5]范围内的均匀分布或者U(0,1)的标准正态分布a取值为[0,1]之间。

###### 鲸鱼算法

在鲸鱼群捕猎过程中，每只鲸鱼有两种行为，一种是包围猎物，所有的鲸鱼都向着其他鲸鱼前进；另一种是汽包网，鲸鱼环形游动喷出气泡来驱赶猎物。在每一代的游动中，鲸鱼们会随机选择这两种行为来进行捕猎。在鲸鱼进行包围猎物的行为中，鲸鱼将会随机选择是向着最优位置的鲸鱼游去还是随机选择一只鲸鱼作为自己的目标，并向其靠近。

向最优：$X_i^{t+1} = X_{best}^t - A|C\times X_{best}^t-X_i^t|$

向随机：$X_i^{t+1} = X_{rand}^t - A|C\times X_{rand}^t-X_i^t|$

其中![X_{best}](https://math.jianshu.com/math?formula=X_%7Bbest%7D) 为当前最优的鲸鱼的位置，A的每一维为均匀分布在（-a,a）内的随机数，a的初始值为2,随着迭代次数线性递减至0；C为均匀分布在（0,2）内的随机数。||表示数的绝对值，即![|C*X_{best}^{t}-X_i^t|](https://math.jianshu.com/math?formula=%7CC*X_%7Bbest%7D%5E%7Bt%7D-X_i%5Et%7C)每一维的值都是非负数。

当 ![|A<1|](https://math.jianshu.com/math?formula=%7CA%3C1%7C)时，鲸鱼选择向着最优个体游动。注意A是一个D维的向量，所以是A的模小于1时，鲸鱼向着最优个体游动。
 　当 ![|A\geq 1|](https://math.jianshu.com/math?formula=%7CA%5Cgeq%201%7C)时，鲸鱼选择向着随机个体游动。

气泡网：$X_i^{t+1} = X_{best}^t+ |X_{best}^t-X_i^t|\times e^{bl}\times \cos(2\pi l)$

每次行动之前，每只鲸鱼都会抛个硬币，来决定是选择包围猎物还是使用气泡网来驱赶猎物。

###### 烟花算法

每个火星的振幅 可由下式计算得出：

![A_i=A_{max}\frac{f_{max}-f(X_i)+\xi}{\sum_{k=1}^N {(f_{max}-f(X_i)}+\xi},f_{max}=f_{best}(2)](https://math.jianshu.com/math?formula=A_i%3DA_%7Bmax%7D%5Cfrac%7Bf_%7Bmax%7D-f(X_i)%2B%5Cxi%7D%7B%5Csum_%7Bk%3D1%7D%5EN%20%7B(f_%7Bmax%7D-f(X_i)%7D%2B%5Cxi%7D%2Cf_%7Bmax%7D%3Df_%7Bbest%7D(2))

式（1）为适应度值越小越优的情况，而式（2）则是适应度值越大越优的情况。 ![\xi](https://math.jianshu.com/math?formula=%5Cxi)为一个极小的值，以保证分母不为0。
 　每个火星产生的正常火星数量也由其适应度值来决定。
 ![S_i=S_{max}\frac{f_{max}-f(X_i)+\xi}{\sum_{k=1}^N {(f_{max}-f(X_i)}+\xi},f_{min}=f_{best}(3)](https://math.jianshu.com/math?formula=S_i%3DS_%7Bmax%7D%5Cfrac%7Bf_%7Bmax%7D-f(X_i)%2B%5Cxi%7D%7B%5Csum_%7Bk%3D1%7D%5EN%20%7B(f_%7Bmax%7D-f(X_i)%7D%2B%5Cxi%7D%2Cf_%7Bmin%7D%3Df_%7Bbest%7D(3))
 　其中![S_{i}](https://math.jianshu.com/math?formula=S_%7Bi%7D)表示第i个火星将要产生的正常火星数，![S_{all}](https://math.jianshu.com/math?formula=S_%7Ball%7D)是产生正常火星的总数为一个常数，从式（3）,（4）可以看出适应度值越好的火星能够产生更多的正常火星，反之，火星适应度越差，能够产生的火星数越少。
 　由于式（3），（4）计算出的值为小数，烟花算法中使用式（5）将其转化为整数。
 ![S_i= \begin{cases} round(a*S_{all}),S_i<a*S_{all} \\ round(b*S_{all}),S_i>b*S_{all} ,a<b<1 \\ round(S_{i}),a*S_{all} \leq S_i\leq b*S_{all} ,a<b<1 \\ \end{cases} (5)](https://math.jianshu.com/math?formula=S_i%3D%20%5Cbegin%7Bcases%7D%20round(a*S_%7Ball%7D)%2CS_i%3Ca*S_%7Ball%7D%20%5C%5C%20round(b*S_%7Ball%7D)%2CS_i%3Eb*S_%7Ball%7D%20%2Ca%3Cb%3C1%20%5C%5C%20round(S_%7Bi%7D)%2Ca*S_%7Ball%7D%20%5Cleq%20S_i%5Cleq%20b*S_%7Ball%7D%20%2Ca%3Cb%3C1%20%5C%5C%20%5Cend%7Bcases%7D%20(5))

当前火星每次爆炸会从D维搜索空间内随机选择z维进行更新从而产生新的火星。正常火星的位置由如下公式产生。
 ![X_{i,j}^{k+1}= \begin{cases} A_i*rand(-1,1)+X_{i,j}^k,j\in \{ d_1,d_2,...,d_z \} ,1\leq z\leq d \\ X_{i,j}^k,j\notin \{ d_1,d_2,...,d_z \} \\ \end{cases} (6)](https://math.jianshu.com/math?formula=X_%7Bi%2Cj%7D%5E%7Bk%2B1%7D%3D%20%5Cbegin%7Bcases%7D%20A_i*rand(-1%2C1)%2BX_%7Bi%2Cj%7D%5Ek%2Cj%5Cin%20%5C%7B%20d_1%2Cd_2%2C...%2Cd_z%20%5C%7D%20%2C1%5Cleq%20z%5Cleq%20d%20%5C%5C%20X_%7Bi%2Cj%7D%5Ek%2Cj%5Cnotin%20%5C%7B%20d_1%2Cd_2%2C...%2Cd_z%20%5C%7D%20%5C%5C%20%5Cend%7Bcases%7D%20(6))
 　其中z为取值1-D的均匀随机正整数，rand(-1,1)表示-1到1内的均匀随机数。从式(6)中可以看出，正常火星的位置与其振幅有直接关系，振幅越大产生的新火星距当前火星的距离约远。

每次迭代过程中，会产生m个特别的火星，即在这N个火星中随机选择m个火星，每个火星产生一个特别的火星。特别的火星的由下面的公式产生：

![X_{i,j}^{k+1}= \begin{cases} randGauss(1,1)*X_{i,j}^k,j\in \{ d_1,d_2,...,d_z \} ,1\leq z\leq d \\ X_{i,j}^k,j\notin \{ d_1,d_2,...,d_z \} \\ \end{cases}](https://math.jianshu.com/math?formula=X_%7Bi%2Cj%7D%5E%7Bk%2B1%7D%3D%20%5Cbegin%7Bcases%7D%20randGauss(1%2C1)*X_%7Bi%2Cj%7D%5Ek%2Cj%5Cin%20%5C%7B%20d_1%2Cd_2%2C...%2Cd_z%20%5C%7D%20%2C1%5Cleq%20z%5Cleq%20d%20%5C%5C%20X_%7Bi%2Cj%7D%5Ek%2Cj%5Cnotin%20%5C%7B%20d_1%2Cd_2%2C...%2Cd_z%20%5C%7D%20%5C%5C%20%5Cend%7Bcases%7D)

每次会先从 ![S_{all}+m+N](https://math.jianshu.com/math?formula=S_%7Ball%7D%2Bm%2BN)个火星中选择最优的火星保留至下一代，然后再从中选择N-1个火星。选择某个火星的概率如下：
 ![p{(X_i)}=\frac{R(X_i)}{\sum_{k=1}^{S_{all}+m+N} R(X_k)},(8)](https://math.jianshu.com/math?formula=p%7B(X_i)%7D%3D%5Cfrac%7BR(X_i)%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BS_%7Ball%7D%2Bm%2BN%7D%20R(X_k)%7D%2C(8))
 ![R{(X_i)}=\sum_{j} R(X_j),(9)](https://math.jianshu.com/math?formula=R%7B(X_i)%7D%3D%5Csum_%7Bj%7D%20R(X_j)%2C(9))
 其中R(X)表示该火星距其他所有火星的距离之和，即距其它火星越远的火星，被选择保留至下一代的概率较大。





