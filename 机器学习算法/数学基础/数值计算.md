

##### 数值稳定性

下溢出`underflow`：当接近零的数字四舍五入为零时，发生下溢出。上溢出`overflow`：当数值非常大，超过了计算机的表示范围时，发生上溢出。

`Conditioning`刻画了一个函数的如下特性：当函数的输入发生了微小的变化时，函数的输出的变化有多大。对于`Conditioning`较大的函数，在数值计算中可能有问题。因为函数输入的舍入误差可能导致函数输出的较大变化。对于方阵$\mathbf{A} \in \mathbb{R}^{n \times n}$，其条件数`condition number`为：
$$
\text{condition number} =\max _{1 \leq i, j \leq n, i \neq j}\left|\frac{\lambda_{i}}{\lambda_{j}}\right|
$$
其中$\lambda_{i}, i=1,2, \cdots, n $ 为$\mathbf{A}$的特征值。当方阵的条件数很大时，矩阵的求逆将对误差特别敏感。条件数是矩阵本身的特性，它会放大那些包含矩阵求逆运算过程中的误差。

##### 梯度下降法

对于函数：$f:\mathbb{R}^n\to\mathbb{R}$，假设输入$\vec{\mathbf{x}}=(x_1,\cdots,x_n)^T$，则定义梯度
$$
\nabla_{\vec{\mathbf{x}}}f(\vec{\mathbf{x}})=\left(\frac{\part}{\part x_1}f(\vec{\mathbf{x}}),\cdots,\frac{\part}{\part x_n}f(\vec{\mathbf{x}})\right)
$$
根据梯度下降法，为了寻找$f$的最小点，迭代过程为：$\vec{\mathbf{x}}^{\prime}=\vec{\mathbf{x}}-\epsilon\nabla_{\vec{\mathbf{x}}}f(\vec{\mathbf{x}})$。其中：$\epsilon$为学习率，它是一个正数，决定了迭代的步长。迭代结束条件为：梯度向量$\nabla_{\vec{\mathbf{x}}}f(\vec{\mathbf{x}})$的每个成分为零或者非常接近零。

选择学习率有多种方法：

- 选择$\epsilon$为一个小的、正的常数；
- 给定多个$\epsilon$，然后选择使得$f(\vec{\mathbf{x}}-\epsilon\nabla_{\vec{\mathbf{x}}}f(\vec{\mathbf{x}}))$最小的那个值作为本次迭代的学习率；
- 求得使$f(\vec{\mathbf{x}}-\epsilon\nabla_{\vec{\mathbf{x}}}f(\vec{\mathbf{x}}))$取极小值的$\epsilon$，即求解最优化问题：

$$
\epsilon^*=\arg\min_{\epsilon,\epsilon>0}f(\vec{\mathbf{x}}-\epsilon\nabla_{\vec{\mathbf{x}}}f(\vec{\mathbf{x}}))
$$
这种方法也称作最速下降法。当目标函数是凸函数时，梯度下降法的解是全局最优的。通常情况下，梯度下降法的解不保证是全局最优的。

##### 海森矩阵与学习率

当函数输入为多维时，定义海森矩阵$\mathbf{H}(f)(\vec{\mathbf{x}})$的第$i$行$j$列元素为：$\mathbf{H}_{i,j}=\frac{\part}{\part x_i\part x_j}f(\vec{\mathbf{x}})$。当二阶偏导是连续时，海森矩阵是对称阵，即有：$\mathbf{H}=\mathbf{H}^T$。在深度学习中大多数海森矩阵都是对称阵。将$f(\vec{\mathbf{x}})$在$\vec{\mathbf{x}}_{0}$处泰勒展开：
$$
f(\vec{\mathbf{x}}) \approx f\left(\vec{\mathbf{x}}_{0}\right)+\left(\vec{\mathbf{x}}-\vec{\mathbf{x}}_{0}\right)^{T} \vec{\mathbf{g}}+\frac{1}{2}\left(\vec{\mathbf{x}}-\vec{\mathbf{x}}_{0}\right)^{T} \mathbf{H}\left(\vec{\mathbf{x}}-\vec{\mathbf{x}}_{0}\right)
$$
其中：$\vec{\mathbf{g}}$为$\vec{\mathbf{x}}_0$处的梯度；$\mathbf{H}$为$\vec{\mathbf{x}}_0$处的海森矩阵。根据梯度下降法：$\vec{\mathbf{x}}^{\prime}=\vec{\mathbf{x}}-\epsilon\nabla_{\vec{\mathbf{x}}}f(\vec{\mathbf{x}})$

应用在点$\vec{\mathbf{x}}_0$，有：
$$
f\left(\vec{\mathbf{x}}_{0}-\epsilon \vec{\mathbf{g}}\right) \approx f\left(\vec{\mathbf{x}}_{0}\right)-\epsilon \vec{\mathbf{g}}^{T} \vec{\mathbf{g}}+\frac{1}{2} \epsilon^{2} \vec{\mathbf{g}}^{T} \mathbf{H} \vec{\mathbf{g}}
$$
如果$\frac{1}{2} \epsilon^{2} \vec{\mathbf{g}}^{T} \mathbf{H} \vec{\mathbf{g}}$较大时，可能会导致：沿着负梯度的方向，函数值反而增加。

如果$\vec{\mathbf{g}}^{T} \mathbf{H} \vec{\mathbf{g}}\le 0$，则无论$\epsilon$取多大的值， 可以保证函数值是减小的。

如果$\vec{\mathbf{g}}^{T} \mathbf{H} \vec{\mathbf{g}}> 0$， 则学习率$\epsilon$不能太大。若$\epsilon$太大则函数值增加。考虑最速下降法，选择使得$f$下降最快的$\epsilon$，有：$\epsilon^*=\frac{\vec{\mathbf{g}}^{T} \vec{\mathbf{g}}}{\vec{\mathbf{g}}^{T} \mathbf{H} \vec{\mathbf{g}}}$。

由于海森矩阵为实对称阵，因此它可以进行特征值分解。假设其特征值从大到小排列为：$\lambda_1\ge\lambda_2\ge\cdots\lambda_n$。海森矩阵的瑞利商为：$R(\vec{\mathbf{x}})=\frac{\vec{\mathbf{x}}^T\mathbf{H}\vec{\mathbf{x}}}{\vec{\mathbf{x}}^T\vec{\mathbf{x}}}$。可以证明：

$$
\begin{array}{cc}\lambda_n\le R(\vec{\mathbf{x}})\le \lambda_1\\
\lambda_1=\max_{\vec{\mathbf{x}}\ne\vec{0}}R(\vec{\mathbf{x}})\\
\lambda_n=\min_{\vec{\mathbf{x}}\ne\vec{0}}R(\vec{\mathbf{x}})\end{array}
$$
根据$\epsilon^*=\frac{\mathbf{g}^{T} \vec{\mathbf{g}}}{\vec{\mathbf{g}}^{T} \mathbf{H} \vec{\mathbf{g}}}=\frac{1}{R(\vec{\mathbf{g}})}$可知：海森矩阵决定了学习率的取值范围。最坏的情况下，梯度$\vec{\mathbf{g}}$与海森矩阵最大特征值$\lambda_1$对应的特征向量平行，则此时最优学习率为$\frac{1}{\lambda_1}$。

##### 牛顿法

梯度下降法有个缺陷：它未能利用海森矩阵的信息。当海森矩阵的条件数较大时，不同方向的梯度的变化差异很大。在某些方向上，梯度变化很快；在有些方向上，梯度变化很慢。梯度下降法未能利用海森矩阵，也就不知道应该优先搜索导数长期为负或者长期为正的方向。当海森矩阵的条件数较大时，也难以选择合适的步长。

牛顿法结合了海森矩阵。考虑泰勒展开式：
$$
f(\vec{\mathbf{x}}) \approx f\left(\vec{\mathbf{x}}_{0}\right)+\left(\vec{\mathbf{x}}-\vec{\mathbf{x}}_{0}\right)^{T} \vec{\mathbf{g}}+\frac{1}{2}\left(\vec{\mathbf{x}}-\vec{\mathbf{x}}_{0}\right)^{T} \mathbf{H}\left(\vec{\mathbf{x}}-\vec{\mathbf{x}}_{0}\right)
$$
如果$\vec{\mathbf{x}}$为极值点，则有：$\frac{\partial}{\partial \vec{\mathbf{x}}} f(\vec{\mathbf{x}})=\vec{0}$，则有：$\vec{\mathbf{x}}^{*}=\vec{\mathbf{x}}_{0}-\mathbf{H}^{-1} \vec{\mathrm{g}}$。当$f$是个正定的二次型，则牛顿法直接一次就能到达最小值点。当$f$不是正定的二次型，则可以在局部近似为正定的二次型，那么则采用多次牛顿法即可到达最小值点。

目标是$\nabla f(\vec{\mathbf{x}})=0$。在一维情况下就是求解$f^{\prime}(x)=0$。牛顿法的方法是：以$x=x^k$做$y=f^{\prime}(x)$切线，该切线过点$(x^k,f^{\prime}(x^k))$。该切线在$x$轴上的交点就是：
$$
x^{k+1}=x^k-\frac{f^{\prime}(x^k)}{f^{\prime\prime}(x^k)}
$$
推广到多维情况下就是：$\vec{\mathbf{x}}^{k+1}=\vec{\mathbf{x}}^k-\mathbf{H}_k^{-1}\vec{\mathbf{g}}_k$。

##### 拟牛顿法

###### 原理

在牛顿法的迭代中，需要计算海森矩阵的逆矩阵$\mathbf{H}^{-1}$，这一计算比较复杂。可以考虑用一个$n$阶矩阵$\mathbf{G}_{k}=G(\vec{\mathbf{x}}^{k})$来近似代替$\mathbf{H}_{k}^{-1}=H^{-1}(\vec{\mathbf{x}}^{k})$。

先看海森矩阵满足的条件：$\vec{\mathbf{g}}_{k+1}-\vec{\mathbf{g}}_{k}=\mathbf{H}_{k}(\vec{\mathbf{x}}^{k+1}-\vec{\mathbf{x}}^{k})$。令$\vec{\mathbf{y}}_{k}=\vec{\mathbf{g}}_{k+1}-\vec{\mathbf{g}}_{k}, \vec{\delta}_{k}=\vec{\mathbf{x}}^{k+1}-\vec{\mathbf{x}}^{k}$。则有：$\vec{\mathbf{y}}_{k}=\mathbf{H}_{k} \vec{\delta}_{k}$，或者$\mathbf{H}_{k}^{-1} \vec{\mathbf{y}}_{k}=\vec{\delta}_{k}$。这称为拟牛顿条件。

根据牛顿法的迭代：$\vec{\mathbf{x}}^{k+1}=\vec{\mathbf{x}}^{k}-\mathbf{H}_{k}^{-1} \vec{\mathbf{g}}_{k}$，将$f(\vec{\mathbf{x}})$在$\vec{\mathbf{x}}^{k}$的一阶泰勒展开：
$$
\begin{array}{l}{f\left(\vec{\mathbf{x}}^{k+1}\right)=f\left(\vec{\mathbf{x}}^{k}\right)+f^{\prime}\left(\vec{\mathbf{x}}^{k}\right)\left(\vec{\mathbf{x}}^{k+1}-\vec{\mathbf{x}}^{k}\right)} \\ {=f\left(\vec{\mathbf{x}}^{k}\right)+\vec{\mathbf{g}}_{k}^{T}\left(-\mathbf{H}_{k}^{-1} \vec{\mathbf{g}}_{k}\right)=f\left(\vec{\mathbf{x}}^{k}\right)-\vec{\mathbf{g}}_{k}^{T} \mathbf{H}_{k}^{-1} \vec{\mathbf{g}}_{k}}\end{array}
$$
当$\mathbf{H}_K$是正定矩阵时，总有$f(\vec{\mathbf{x}}^{k+1})<f(\vec{\mathbf{x}}^{k})$，因此每次都是沿着函数递减的方向迭代。

如果选择$\mathbf{G}_k$作为$\mathbf{H}_k^{-1}$的近似时， 同样要满足两个条件：$\mathbf{G}_k$必须是正定的。$\mathbf{G}_k$满足拟牛顿条件：$\mathbf{G}_{k+1} \vec{\mathbf{y}}_{k}=\vec{\delta}_{k}$。因为$\mathbf{G}_0$是给定的初始化条件，所以下标从$k+1$开始。按照拟牛顿条件，在每次迭代中可以选择更新矩阵$\mathbf{G}_{k+1}=\mathbf{G}_{k}+\Delta \mathbf{G}_{k}$。

###### `DFP`算法

`DFP`算法选择$\mathbf{G}_{k+1}$的方法是：假设每一步迭代中$\mathbf{G}_{k+1}$是由$\mathbf{G}_{k}$加上两个附加项构成：$\mathbf{G}_{k+1}=\mathbf{G}_{k}+\mathbf{P}_{k}+\mathbf{Q}_{k}$，其中$\mathbf{P}_{k},\mathbf{Q}_{k}$是待定矩阵。此时有：
$$
\mathbf{G}_{k+1} \vec{\mathbf{y}}_{k}=\mathbf{G}_{k}\vec{\mathbf{y}}_{k}+\mathbf{P}_{k}\vec{\mathbf{y}}_{k}+\mathbf{Q}_{k}\vec{\mathbf{y}}_{k}
$$
为了满足拟牛顿条件，可以取：$\mathbf{P}_{k}\vec{\mathbf{y}}_{k}=\vec{\delta}_{k},\mathbf{Q}_{k}\vec{\mathbf{y}}_{k}=-\mathbf{G}_{k}\vec{\mathbf{y}}_{k}$。

这样的$\mathbf{P}_{k},\mathbf{Q}_{k}$不止一个。例如取
$$
\mathbf{P}_{k}=\frac{\vec{\delta}_{k}\vec{\delta}_{k}^T}{\vec{\delta}_{k}^T\vec{\mathbf{y}}_{k}},\mathbf{Q}_{k}=-\frac{\mathbf{G}_{k}\vec{\mathbf{y}}_{k}\vec{\mathbf{y}}_{k}^T\mathbf{G}_{k}}{\vec{\mathbf{y}}_{k}^T\mathbf{G}_{k}\vec{\mathbf{y}}_{k}}
$$
则迭代公式为：

$$
\mathbf{G}_{k+1}=\mathbf{G}_{k}+\frac{\vec{\delta}_{k}\vec{\delta}_{k}^T}{\vec{\delta}_{k}^T\vec{\mathbf{y}}_{k}}-\frac{\mathbf{G}_{k}\vec{\mathbf{y}}_{k}\vec{\mathbf{y}}_{k}^T\mathbf{G}_{k}}{\vec{\mathbf{y}}_{k}^T\mathbf{G}_{k}\vec{\mathbf{y}}_{k}}
$$
可以证明：如果初始矩阵$\mathbf{G}_{0}$是正定的，则迭代过程中每个矩阵$\mathbf{G}_{k}$都是正定的。

###### `BFGS`算法

`DFP`算法中，用$\mathbf{G}_{k}$逼近$\mathbf{H}^{-1}$。换个角度看，可以用矩阵$\mathbf{B}_{k}$逼近海森矩阵$\mathbf{H}$。此时对应的拟牛顿条件为：$\mathbf{B}_{k+1}\vec{\delta}_{k}=\vec{\mathbf{y}}_{k}$。令：$\mathbf{B}_{k+1}=\mathbf{B}_{k}+\mathbf{P}_{k}+\mathbf{Q}_{k}$，有：
$$
\mathbf{B}_{k+1}\vec{\delta}_{k}=\mathbf{B}_{k}\vec{\delta}_{k}+\mathbf{P}_{k}\vec{\delta}_{k}+\mathbf{Q}_{k}\vec{\delta}_{k}
$$
可以取$\mathbf{P}_{k}\vec{\delta}_{k}=\vec{\mathbf{y}}_{k},\mathbf{Q}_{k}\vec{\delta}_{k}=-\mathbf{B}_{k}\vec{\delta}_{k}$。寻找合适的$\mathbf{P}_{k},\mathbf{Q}_{k}$，可以得到 `BFGS` 算法矩阵的 的迭代公式：
$$
\mathbf{G}_{k+1}=\mathbf{G}_{k}+\frac{\vec{\mathbf{y}}_{k}\vec{\mathbf{y}}_{k}^T}{\vec{\mathbf{y}}_{k}^T\vec{\delta}_{k}}-\frac{\mathbf{B}_{k}\vec{\delta}_{k}\vec{\delta}_{k}^T\mathbf{B}_{k}}{\vec{\delta}_{k}^T\mathbf{B}_{k}\vec{\delta}_{k}}
$$
可以证明：如果初始矩阵$\mathbf{B}_{0}$是正定的，则迭代过程中每个矩阵$\mathbf{B}_{k}$都是正定的。

##### 约束最优化

假设$f(\vec{\mathbf{x}}), c_{i}(\vec{\mathbf{x}}), h_{j}(\vec{\mathbf{x}})$是定义在$\mathbb{R}^{n}$上的连续可微函数。考虑约束最优化问题：
$$
\begin{array}{cc}\operatorname{min}_{\vec{x} \in \mathbb{R}^{n}} f(\vec{\mathbf{x}}) \\ \text {s.t. } \quad c_{i}(\vec{\mathbf{x}}) \leq 0, i=1,2, \cdots, k\\
\quad h_{j}(\vec{\mathbf{x}})=0, j=1,2, \cdots, l\end{array}
$$
可行域有等式和不等式共同确定$\mathrm{S}=\left\{\vec{\mathbf{x}} | c_{i}(\vec{\mathbf{x}}) \leq 0, i=1,2, \cdots, k ; \quad h_{j}(\vec{\mathbf{x}})=0, j=1,2, \cdots, l\right\}$

引入拉格朗日函数：
$$
L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})=f(\vec{\mathbf{x}})+\sum_{i=1}^{k} \alpha_{i} c_{i}(\vec{\mathbf{x}})+\sum_{j=1}^{l} \beta_{j} h_{j}(\vec{\mathbf{x}})
$$
这里$\vec{\mathbf{x}}=\left(x_{1}, x_{2}, \cdots, x_{n}\right)^{T} \in \mathbb{R}^{n}, \alpha_{i}, \beta_{j}$是拉格朗日乘子，$\alpha_{i} \geq 0$。定义函数：
$$
\theta_{P}(\vec{\mathbf{x}})=\max _{\vec{\alpha}, \vec{\beta} : \alpha_{i} \geq 0}L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})
$$
其中下标$P$表示原始问题。则有：
$$
\theta_{P}(\vec{\mathbf{x}})=\left\{\begin{array}{ll}{f(\vec{\mathbf{x}}),} & {\text { if } \vec{\mathbf{x}} \text { statisfy original problem's constraint }} \\ {+\infty,} & {\text { or else. }}\end{array}\right.
$$
若$\vec{\mathbf{x}}$满足原问题的约束，则很容易证明$L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})=f(\vec{\mathbf{x}})+\sum_{i=1}^{k} \alpha_{i} c_{i}(\vec{\mathbf{x}}) \leq f(\vec{\mathbf{x}})$，等号在$\alpha_i =0$时取到。若$\vec{\mathbf{x}}$不满足原问题的约束：若不满足$c_{i}(\vec{\mathbf{x}}) \leq 0$：设违反的为$c_{i_{0}}(\vec{\mathbf{x}})>0$，则令$\vec{\alpha}_{i_{0}} \rightarrow \infty$，有：$L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})=f(\vec{\mathbf{x}})+\sum_{i=1}^{k} \alpha_{i} c_{i}(\vec{\mathbf{x}}) \rightarrow \infty$。若不满足$h_{j}(\vec{\mathbf{x}})=0$： 设违反的为$h_{j_{0}}(\vec{\mathbf{x}}) \neq 0$，则令$\vec{\beta}_{j_{0}} h_{j_{0}}(\vec{\mathbf{x}}) \rightarrow \infty$，有：$L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})=f(\vec{\mathbf{x}})+\sum_{i=1}^{k} \alpha_{i} c_{i}(\vec{\mathbf{x}})+\vec{\beta}_{j_{0}} h_{j_{0}}(\vec{\mathbf{x}}) \rightarrow \infty$。

考虑极小化问题：
$$
\min _{\vec{\mathbf{x}}} \theta_{P}(\vec{\mathbf{x}})=\min _{\vec{\mathbf{x}}} \max _{\vec{\alpha}, \vec{\beta} : \alpha_{i} \geq 0} L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})
$$
则该问题是与原始最优化问题是等价的，即他们有相同的问题。
$$
\min _{\vec{\mathbf{x}}} \max _{\vec{\alpha}, \vec{\beta} : \alpha_{i} \geq 0 }L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})
$$
称为广义拉格朗日函数的极大极小问题。为了方便讨论，定义原始问题的最优值为：$p^{*}=\min _{\vec{\mathbf{x}}} \theta_{P}(\vec{\mathbf{x}})$。

定义$\theta_{D}(\vec{\alpha}, \vec{\beta})=\min _{\vec{\mathbf{x}}} L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})$，考虑极大化$\theta_{D}(\vec{\alpha}, \vec{\beta})$，即：
$$
\max _{\vec{\alpha}, \vec{\beta} : \alpha_{i} \geq 0} \theta_{D}(\vec{\alpha}, \vec{\beta})=\max _{\vec{\alpha}, \vec{\beta} : \alpha_{i} \geq 0}\min _{\vec{\mathbf{x}}} L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})
$$
问题$\max _{\vec{\alpha}, \vec{\beta} : \alpha_{i} \geq 0}\min _{\vec{\mathbf{x}}} L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})$称为广义拉格朗日函数的极大极小问题。它可以表示为约束最优化问题：
$$
\begin{array}{c}{\max _{\vec{\alpha}, \vec{\beta} : \alpha_{i} \geq 0} \theta_{D}(\vec{\alpha}, \vec{\beta})=\max _{\vec{\alpha}, \vec{\beta} : \alpha_{i} \geq 0} \min _{\vec{\mathbf{x}}} L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})} \\ {\text { s.t. } \alpha_{i} \geq 0, i=1,2, \cdots, k}\end{array}
$$
称为原始问题的对偶问题。为了方便讨论，定义对偶问题的最优值为：$d^{*}=\max _{\vec{\alpha}, \overline{\beta} : \alpha_{i} \geq 0}  \theta_{D}(\vec{\alpha}, \vec{\beta})$。

定理一：若原问题和对偶问题具有最优值，则：
$$
d^{*}=\max _{\vec{\alpha}, \vec{\beta} : \vec{\alpha}_{i} \geq 0} \min _{\vec{\mathbf{x}}} L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta}) \leq \min _{\vec{\mathbf{x}}} \max _{\vec{\alpha}, \vec{\beta} : \vec{\alpha}_{i} \geq 0} L(\vec{\mathbf{x}}, \vec{\alpha}, \vec{\beta})=p^{*}
$$
推论一：设$\vec{\mathbf{x}}^*$为原始问题的可行解，且$\theta_P(\vec{\mathbf{x}}^*)$的值为$p^*$；$\vec{\alpha}^{*}, \vec{\beta}^{*}$为对偶问题的可行解，$\theta_{D}\left(\vec{\alpha}^{*}, \vec{\beta}^{*}\right)$值为$d^*$。如果有$p^{*}=d^{*}$，则$\vec{\mathbf{x}}^{*}, \vec{\alpha}^{*}, \vec{\beta}^{*}$分别为原始问题和对偶问题的最优解。

定理二：假设函数$f(\vec{\mathbf{x}})$和$c_{i}(\vec{\mathbf{x}})$为凸函数，$h_{j}(\vec{\mathbf{x}})$是仿射函数；并且假设不等式约束$c_{i}(\vec{\mathbf{x}})$是严格可行的，即存在$\vec{\mathbf{x}}$，对于所有$i$有$c_{i}(\vec{\mathbf{x}})<0$。则存在$\vec{\mathbf{x}}^{*}, \vec{\alpha}^{*}, \vec{\beta}$，使得：$\vec{\mathbf{x}}^*$是原始问题$\min _{\vec{\mathbf{x}}} \theta_{P}(\vec{x})$的解，$\vec{\alpha}^{*}, \vec{\beta}^{*}$是对偶问题$d^{*}=\max _{\vec{\alpha}, \overline{\beta} : \alpha_{i} \geq 0}  \theta_{D}(\vec{\alpha}, \vec{\beta})$的解，并且$p^{*}=d^{*}=L\left(\vec{\mathbf{x}}^{*}, \vec{\alpha}^{*}, \vec{\beta}^{*}\right)$。

定理三：假设函数$f(\vec{\mathbf{x}})$和$c_{i}(\vec{\mathbf{x}})$为凸函数，$h_{j}(\vec{\mathbf{x}})$是仿射函数；并且假设不等式约束$c_{i}(\vec{\mathbf{x}})$是严格可行的，即存在$\vec{\mathbf{x}}$，对于所有$i$有$c_{i}(\vec{\mathbf{x}})<0$。则存在$\vec{\mathbf{x}}^{*}, \vec{\alpha}^{*}, \vec{\beta}$，使得：$\vec{\mathbf{x}}^*$是原始问题$\min _{\vec{x}} \theta_{P}(\vec{x})$的解，$\vec{\alpha}^{*}, \vec{\beta}^{*}$是对偶问题$d^{*}=\max _{\vec{\alpha}, \overline{\beta} : \alpha_{i} \geq 0} \theta_{D}(\vec{\alpha}, \vec{\beta})$的解的充要条件是：$\vec{\mathbf{x}}^{*}, \vec{\alpha}^{*}, \vec{\beta}$满足下面的 `Karush-kuhn-Tucker(KKT)`条件：
$$
\begin{array}{c}{\nabla_{\vec{\mathbf{x}}} L\left(\vec{\mathbf{x}}^{*}, \vec{\alpha}^{*}, \vec{\beta}^{*}\right)=0} \\ {\nabla_{\vec{\alpha}} L\left(\vec{\mathbf{x}}^{*}, \vec{\alpha}^{*}, \vec{\beta}^{*}\right)=0} \\ {\nabla_{\vec{\beta}} L\left(\vec{\mathbf{x}}^{*}, \vec{\alpha}^{*}, \vec{\beta}^{*}\right)=0} \\ {\vec{\alpha}_{i}^{*} c_{i}\left(\vec{\mathbf{x}}^{*}\right)=0, i=1,2, \cdots, k} \\ {\vec{\alpha}_{i}\left(\vec{\mathbf{x}}^{*}\right) \leq 0, i=1,2, \cdots, k} \\ {\vec{\alpha}_{i}^{*} \geq 0, i=1,2, \cdots, k} \\ {h_{j}\left(\vec{\mathbf{x}}^{*}\right)=0, j=1,2, \cdots, l}\end{array}
$$
仿射函数：仿射函数即由`1`阶多项式构成的函数。一般形式为$f(\vec{\mathbf{x}})=\mathbf{A} \vec{\mathbf{x}}+b_{\mathrm{o}}$。这里：$\mathbf{A}$是一个$m \times k$矩阵，$\vec{\mathbf{x}}$是一个$k$维列向量，$b$是一个$m$维列向量。它实际上反映了一种从$k$维到$m$维的空间线性映射关系。

凸函数：设$f$为定义在区间$\mathcal{X}$上的函数，若对$\mathcal{X}$上的任意两点$\vec{\mathbf{x}}_{1}, \vec{\mathbf{x}}_{2}$和任意的实数$\lambda \in(0,1)$，总有$f\left(\lambda \vec{\mathbf{x}}_{1}+(1-\lambda) \vec{\mathbf{x}}_{2}\right) \geq \lambda f\left(\vec{\mathbf{x}}_{1}\right)+(1-\lambda) f\left(\vec{\mathbf{x}}_{2}\right)$，则$f$称为$\mathcal{X}$上的凸函数 