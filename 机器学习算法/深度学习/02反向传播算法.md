#### 反向传播算法

多层前馈神经网络可以看作是一个非线性复合函数$φ : R^d → R^{d′}$，将输入$x ∈ R^d $映射到输出$φ(x) ∈ R^{d′}$。因此，多层前馈神经网络也可以看成是一种特征转换方法，其输出$φ(x)$作为分类器的输入进行分类。给定一个训练样本$ (\mathbf{x}, y)$，先利用多层前馈神经网络将$\mathbf{x} $映射到$φ(x)$，然后再将$φ(x)$输入到分类器$g(·)$。
$$
\hat{y}=g(\varphi(\mathbf{x}), \theta)
$$
对第$l$ 层中的参数$W^{(l)}$ 和$b^{(l)}$ 计算偏导数
$$
\begin{aligned} \frac{\partial \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})}{\partial W_{i j}^{(l)}} &=\left(\frac{\partial \mathbf{z}^{(l)}}{\partial W_{i j}^{(l)}}\right)^{\mathrm{T}} \frac{\partial \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{z}^{(l)}} \\ \frac{\partial \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{b}^{(l)}} &=\left(\frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{b}^{(l)}}\right)^{\mathrm{T}} \frac{\partial \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{z}^{(l)}} \end{aligned}
$$
计算偏导数$\frac{\partial \mathbf{z}^{(l)}}{\partial W_{i j}^{(l)}}$
$$
\begin{aligned}\frac{\partial \mathbf{z}^{(l)}}{\partial W_{i j}^{(l)}}&=\frac{\partial\left(W^{(l)} \mathbf{a}^{(l-1)}+\mathbf{b}^{(l)}\right)}{\partial W_{i j}^{(l)}}\\
&\triangleq \mathbb{I}_{i}\left(a_{j}^{(l-1)}\right)
\end{aligned}
$$


计算误差项$\frac{\partial \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{z}^{(l)}}$
$$
\delta^{(l)}=\frac{\partial \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{z}^{(l)}} \in \mathbb{R}^{(l)}
$$

$$
\begin{aligned} \delta^{(l)} & \triangleq \frac{\partial \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{z}^{(l)}} \\ &=\frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}} \cdot \frac{\partial \mathbf{z}^{(l+1)}}{\partial \mathbf{a}^{(l)}} \cdot \frac{\partial \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{z}^{(l+1)}} \\ &=\operatorname{diag}\left(f_{l}^{\prime}\left(\mathbf{z}^{(l)}\right)\right) \cdot\left(W^{(l+1)}\right)^{T} \cdot \delta^{(l+1)}\\&={f_{l}^{\prime}\left(\mathbf{z}^{(l)}\right) \odot\left(\left(W^{(l+1)}\right)^{\mathrm{T}} \delta^{(l+1)}\right)} \end{aligned}
$$

$$
\frac{\partial \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})}{\partial W_{i j}^{(l)}}=\mathbb{I}_{i}\left(a_{j}^{(l-1)}\right)^{\mathrm{T}} \delta^{(l)}=\delta_{i}^{(l)} a_{j}^{(l-1)}
$$

$$
\frac{\partial \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})}{\partial W^{(l)}}=\delta^{(l)}\left(\mathbf{a}^{(l-1)}\right)^{\mathrm{T}}
$$

$$
\frac{\partial \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})}{\partial \mathbf{b}^{(l)}}=\delta^{(l)}
$$

神经网络的参数学习比线性模型要更加困难，主要原因有两点：非凸优化问题和梯度消失问题。

计算图：图中的每个节点代表一个变量。操作：为一个或者多个变量的简单函数。多个操作组合在一起可以描述一个更复杂的函数。一个操作仅返回单个输出变量。如果变量$\mathbf{y}$是变量$\mathbf{x}$通过一个操作计算得到，则在图中绘制一条从$\mathbf{y}$到$\mathbf{x}$的有向边。

在多维情况下，设：$\overrightarrow{\mathbf{x}} \in \mathbb{R}^{m}, \overrightarrow{\mathbf{y}} \in \mathbb{R}^{n}$，$g$为$\mathbb{R}^m到$$\mathbb{R}^n$的映射且满足$\overrightarrow{\mathbf{y}}=g(\overrightarrow{\mathbf{x}})$，$f$为$\mathbb{R}^n$到$\mathbb{R}$的映射且满足$z=f(\overrightarrow{\mathbf{y}})$。则有：$\frac{\partial z}{\partial x_{i}}=\sum_{j=1}^{n} \frac{\partial z}{\partial y_{j}} \frac{\partial y_{j}}{\partial x_{i}}, \quad i=1,2, \cdots, m$。使用向量记法，可以等价地写作：$\nabla_{\overrightarrow{\mathbf{x}}} z=\left(\frac{\partial \overrightarrow{\mathbf{y}}}{\partial \overrightarrow{\mathbf{x}}}\right)^{T} \nabla_{\overrightarrow{\mathbf{y}}} z$。使用向量记法，可以等价地写作：$\nabla_{\overrightarrow{\mathbf{x}}} z=\left(\frac{\partial \overrightarrow{\mathbf{y}}}{\partial \overrightarrow{\mathbf{x}}}\right)^{T} \nabla_{\overrightarrow{\mathbf{y}}} z$。其中：$\frac{\partial \overrightarrow{\mathbf{y}}}{\partial \overrightarrow{\mathbf{x}}}$为$g$的$n\times m$阶雅可比矩阵，$\nabla_{\overrightarrow{\mathbf{x}}} z$为$z$对$\vec{\mathbf{x}}$的梯度，$\nabla_{\overrightarrow{\mathbf{y}}} z$为$z$对$\vec{\mathbf{y}}$的梯度：

$$
\nabla_{\overrightarrow{\mathbf{x}}} z=\left[\begin{array}{c}{\frac{\partial z}{\partial x_{1}}} \\ {\frac{\partial z}{\partial x_{2}}} \\ {\vdots} \\ {\frac{\partial z}{\partial x_{m}}}\end{array}\right] \quad \nabla_{\overrightarrow{\mathbf{y}}} z=\left[\begin{array}{c}{\frac{\partial z}{\partial y_{1}}} \\ {\frac{\partial z}{\partial y_{2}}} \\ {\vdots} \\ {\frac{\partial z}{\partial y_{n}}}\end{array}\right]\frac{\partial \overrightarrow{\mathbf{y}}}{\partial \overrightarrow{\mathbf{x}}}=\left[\begin{array}{cccc}{\frac{\partial y_{1}}{\partial x_{1}}} & {\frac{\partial y_{1}}{\partial x_{2}}} & {\cdots} & {\frac{\partial y_{1}}{\partial x_{m}}} \\ {\frac{\partial y_{2}}{\partial x_{1}}} & {\frac{\partial y_{2}}{\partial x_{2}}} & {\cdots} & {\frac{\partial y_{2}}{\partial x_{m}}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial y_{n}}{\partial x_{1}}} & {\frac{\partial y_{n}}{\partial x_{2}}} & {\cdots} & {\frac{\partial y_{n}}{\partial x_{m}}}\end{array}\right]
$$
链式法则不仅可以作用于向量，也可以应用于张量：首先将张量展平为一维向量。然后计算该向量的梯度。然后将该梯度重新构造为张量。记$\nabla_{\mathbf{X}} z$为$z$对张量$\mathbf{X}$的梯度。现在有多个索引，可以使用单个变量$i$来表示$\mathbf{X}$的索引元组。这就与向量中的索引方式完全一致：$\left(\nabla_{\mathbf{X}} z\right)_{i}=\frac{\partial z}{\partial x_{i}}$。

设$\mathbf{Y}=g(\mathbf{X}), z=f(\mathbf{Y})$，用单个变量$j$来表示$\mathbf{Y}$的索引元组。则张量的链式法则为：
$$
\frac{\partial z}{\partial x_{i}}=\sum_{j} \frac{\partial z}{\partial y_{j}} \frac{\partial y_{j}}{\partial x_{i}} \Rightarrow \nabla_{\mathbf{X}} z=\sum_{j}\left(\nabla_{\mathbf{x}} y_{j}\right) \frac{\partial z}{\partial y_{j}}
$$

