给定包含$N$篇文档的语料库$\mathbb{D}=\{\mathcal{D}_1,\mathcal{D}_2,\cdots,\mathcal{D}_N\}$，所有的单词来自于包含$V$个词汇的词汇表$\mathbb{V}=\{\text{word}_1,\text{word}_2,\cdots,\text{word}_V\}$，其中$V$表示词汇表的大小 。每篇文档$\mathbb{D}_i$包含单词序列$\{\text{word}_{w_1^i},\text{word}_{w_2^i},\cdots,\text{word}_{w_{n_i}^i}\}$，其中$w_i^j\in\{1,2,\cdots,V\}$表示第$i$篇文档的第$j$个单词在词汇表中的编号，$n_i$表示第$i$篇文档包含$n_i$个单词。词的表达任务的问题是如何表示每个词汇$\text{word}_v$。

最简单的表示方式是`one-hot` 编码：对于词汇表中第$v$个单词$\text{word}_v$，将其表示为$\text{word}_v\to(0,\cdots,1,\cdots,0)^T$，即第$v$位取值为`1`，剩余位取值为`0` 。这种表示方式有两个主要缺点：无法表达单词之间的关系，任何两个单词之间的距离一样；向量维度过高。

`BOW:Bag of Words`：词在文档中不考虑先后顺序，这称作词袋模型。

###### `TF-IDF`

`TF-IDF`是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词。`TF-IDF`有两层意思，一层是词频，另一层是逆文档频率。当有`TF`和`IDF`后，将这两个词相乘，就能得到一个词的`TF-IDF`的值。某个词在文章中的`TF-IDF`越大，那么一般而言这个词在这篇文章的重要性会越高，由大到小排序，排在最前面的几个词，就是该文章的关键词。

计算词频：词频`TF` = $\frac{\text{某个词在文章中的出现次数}}{\text{文章的总词数}}$

计算逆文档频率：逆文档频率`IDF`= $\log(\frac{\text{语料库的文档总数}}{\text{包含该词的文档数}+1})$

计算`TF-IDF`：`TF-IDF`=$\text{TF}\times \text{IDF}$

#### 向量空间模型

向量空间模型假设单词和单词之间是相互独立的，每个单词代表一个独立的语义单元。实际上该假设很难满足：①文档中的单词和单词之间存在一定关联性，单词和其前面几个单词、后面几个单词可能存在语义上的相关性，而向量空间模型忽略了这种上下文的作用；②文档中存在很多的一词多义和多词同义的现象，每个单词并不代表一个独立的语义单元。

给定语料库$\mathbb{D}$和词汇表$\mathbb{V}$，定义`文档-单词` 矩阵为：

![](../../../picture/1/357.png)

令矩阵为$\mathbf{D}$，则：$D(i,j)=1$表示文档$\mathcal{D}_i$中含有单词$\text{word}_j$；$D(i,j)=0$表示文档$\mathcal{D}_i$中不含单词$\text{word}_j$。文档的上述表达并未考虑单词的顺序，也未考虑单词出现的次数。一种改进策略是考虑单词出现的次数，从而赋予`文档-单词` 矩阵以不同的权重：
$$
\mathbf{D}=\left[\begin{array}{c}w_{1,1} & w_{1,2} & \cdots & w_{1,V}\\
w_{2,1} & w_{2,2} & \cdots & w_{2,V}\\
\cdot & \cdot & \cdots & \cdot\\
w_{N,1} & w_{N,2} & \cdots & w_{N,V}
\end{array}

\right]
$$
其中$w_{i,j}$表示单词$\text{word}_j$在文档$\mathcal{D}_i$中的权重。如果单词$\text{word}_j$在文档$\mathcal{D}_i$中未出现，则权重$w_{i,j}=0$。如果单词$\text{word}_j$在文档$\mathcal{D}_i$中出现，则权重$w_{i,j}\ne0$。

权重$w_{i,j}$有两种常用的选取方法：

- 单词权重等于单词出现的频率`TF`：$w_{i,j}=TF(\mathcal{D}_i,\text{word}_j)$，返回单词$\text{word}_j$在文档$\mathcal{D}_i$中出现的频数；其缺点是：一些高频词以较大的权重出现在每个文档中，这意味着对每篇文档这些高频词是比较重要的。

- 单词权重等于单词的`TF-IDF`：

给定 `文档-单词` 矩阵，得到文档的向量表达：$\mathcal{D}_i\to(w_{i,1},\cdots,w_{i,V})^T$。给定文档$\mathcal{D}_i,\mathcal{D}_j$，则文档的相似度为：
$$
\text{similar}(\mathcal{D}_i,\mathcal{D}_j)=\cos(\vec{\mathbf{w}}_i,\vec{\mathbf{w}}_j)=\frac{\vec{\mathbf{w}}_i\cdot\vec{\mathbf{w}}_j}{||\vec{\mathbf{w}}_i||\cdot||\vec{\mathbf{w}}_j||}
$$

##### `LSA`

基本假设是：如果两个词多次出现在同一篇文档中，则这两个词具有语义上的相似性。

给定`文档-单词` 矩阵$\mathbf{D}$，其中$w_{i,j}$表示单词$\text{word}_j$在文档$\mathcal{D}_i$中的权重，可以为：单词$\text{word}_j$在文档$\mathcal{D}_i$中是否出现的`0/1`值、单词$\text{word}_j$在文档$\mathcal{D}_i$中出现的频次、单词$\text{word}_j$在文档$\mathcal{D}_i$中的`TF-IDF`值

给定$\vec{\mathbf{v}}_j = (w_{1,j},\cdots,w_{N,j})^T$，它为矩阵$\mathbf{D}$的第$j$列，代表单词$\text{word}_j$的`单词-文档向量`向量，描述了该单词和所有文档的关系。向量内积$\vec{\mathbf{v}}_p\cdot \vec{\mathbf{v}}_q$描述了单词$\text{word}_p$和单词$\text{word}_q$在文档集合中的相似性。矩阵乘积$\mathbf{D}^T\mathbf{D}\in \mathbb{R}^{V\times V}$包含了所有词向量内积的结果。

给定$\vec{\mathbf{d}}_i = (w_{i,1},\cdots,w_{i,v})^T$，它为矩阵$\mathbf{D}$的第$i$行，代表文档$\mathcal{D}_i$的`单词-文档向量`向量，描述了该单词和所有文档的关系。向量内积$\vec{\mathbf{d}}_s\cdot \vec{\mathbf{d}}_t$描述了文档$\mathcal{D}_s$和文档$\mathcal{D}_t$的相似性。矩阵乘积$\mathbf{D}\mathbf{D}^T\in \mathbb{R}^{N\times N}$包含了所有文档向量内积的结果。

对矩阵$\mathbf{D}$进行`SVD` 分解。假设矩阵$\mathbf{D}$可以分解为：$\mathbf{D}=\mathbf{P}\mathbf{\Sigma}\mathbf{Q}^T$。其中：

$\mathbf{P}\in \mathbb{R}^{N\times N} \mathbf{Q}\in \mathbb{R}^{V\times V}$为单位正交矩阵。$\mathbf{\Sigma} \in \mathbb{R}^{N\times V}$为广义对角矩阵。

`SVD` 分解的物理意义为：将文档按照主题分解。所有的文档一共有$r$个主题，每个主题的强度分别为：$\sigma_1,\cdots,\sigma_r$。

第$i$篇文档$\mathcal{D}_i$由这$r$个主题组成，文档的主题概率分布（称作`文档-主题向量`）为：
$$
\vec{\mathbf{p}}^{i}=(P(i,1),\cdots, P(i,r))^T
$$
第$t$个主题由$V$个单词组成，主题的单词概率分布（称作`主题-单词向量` ）为：
$$
\vec{\mathbf{q}}^{t}=(Q(t,1),\cdots, Q(t,V))^T
$$
第$j$个单词由$r$个主题组成，单词的主题概率分布（称作 `单词-主题` 向量）为：
$$
\vec{\mathbf{v}}^{j}=(Q(1,j),\cdots, Q(r,j))^T
$$
根据$\mathbf{D}=\mathbf{P}\mathbf{\Sigma}\mathbf{Q}^T$有：则该分解的物理意义为：`文档-单词` 矩阵 = `文档-主题` 矩阵 x `主题强度` x `主题-单词` 矩阵。

得到了文档的主题分布、单词的主题分布之后，可以获取文档的相似度和单词的相似度。文档$\mathcal{D}_i$和文档$\mathcal{D}_j$的相似度：
$$
\text{similar}(\mathcal{D}_i,\mathcal{D}_j)==\frac{\vec{\mathbf{p}}^i\cdot\vec{\mathbf{p}}^j}{||\vec{\mathbf{p}}^i||\cdot||\vec{\mathbf{p}}^j||}
$$
单词$\text{word}_i$和单词$\text{word}_j$的相似度：
$$
\text{similar}(\text{word}_i,\text{word}_i)==\frac{\vec{\mathbf{v}}^i\cdot\vec{\mathbf{v}}^j}{||\vec{\mathbf{v}}^i||\cdot||\vec{\mathbf{v}}^j||}
$$
`LSA` 可以应用在以下几个方面：

- 通过对文档的`文档-主题向量` 进行比较，从而进行基于主题的文档聚类或者分类。
- 通过对单词的`单词-主题向量`进行比较，从而用于同义词、多义词进行检测。
- 通过将`query` 映射到主题空间，进而进行信息检索。

#### `Word2Vec`

##### `CBOW`模型

在一个单词上下文的`CBOW` 模型中：输入是前一个单词，输出是后一个单词，输入为输出的上下文。由于只有一个单词作为输入，因此称作一个单词的上下文。一个单词上下文的`CBOW` 模型如下

![](../../../picture/1/411.png)

其中：$N$为隐层的大小，即隐向量$\vec{\mathbf{h}}=(h_1,\cdots,h_N)^T$。网络输入$\vec{\mathbf{x}}=(x_1,\cdots,x_V)^T\in\mathbb{R}^V$，它是输入单词的 `one-hote` 编码，其中只有一位为 1，其他位都为 0 。网络输出$\vec{\mathbf{y}}=(y_1,\cdots,y_V)^T\in\mathbb{R}^V$，它是输出单词为词汇表各单词的概率。相邻层之间为全连接：输入层和隐层之间的权重矩阵为$\mathbf{W}\in\mathbb{R}^{V\times N}$。隐层和输出层之间的权重矩阵为$\mathbf{W}^{\prime}\in\mathbb{R}^{N\times V}$

假设没有激活函数，没有偏置项。给定输入$\vec{\mathbf{x}}\in\mathbb{R}^V$，则其对应的隐向量$\vec{\mathbf{h}}\in\mathbb{R}^N$为：$\vec{\mathbf{h}}=\mathbf{W}^T\vec{\mathbf{x}}$。$\mathbf{W}=(\vec{\mathbf{w}}_1^T,\cdots,\vec{\mathbf{w}}_V^T)^T$。给定隐向量$\vec{\mathbf{h}}$，其对应的输出向量$\vec{\mathbf{u}}\in\mathbb{R}^V$为：$\vec{\mathbf{u}}=\mathbf{W}^{\prime T}\vec{\mathbf{h}}$。令：
$$
\mathbf{W}^{\prime}=[\vec{\mathbf{w}}_1^{\prime},\cdots,\vec{\mathbf{w}}_V^{\prime}]
$$
则有：$u_j=\vec{\mathbf{w}}_j\cdot \vec{\mathbf{h}}$。$\vec{\mathbf{u}}$之后接入一层 `softmax` 层，则有：
$$
y_j=p(\text{word}_j|\vec{\mathbf{x}})=\frac{\exp(u_j)}{\sum_{i=1}^V\exp(u_i)}
$$
假设输入单词为$\text{word}_I$，观测到它的下一个单词为$\text{word}_O$。令输入单词的对应的网络输入为$\vec{\mathbf{x}}$，其隐向量为$\vec{\mathbf{w}}_I$，输入输出单词对应的输出单元为$j^*$，则采用交叉熵的损失函数为：
$$
E(\text{word}_I,\text{word}_O)=-\log\frac{\exp(u_j^*)}{\sum_{i=1}^V\exp(u_i)}=-\vec{\mathbf{w}}_{j^*}^{\prime}\cdot \vec{\mathbf{w}}_I+\log\sum_{i=1}^V\exp(\vec{\mathbf{w}}_{i}^{\prime}\cdot \vec{\mathbf{w}}_I)
$$
考虑语料库$\mathbb{D}$中所有的样本，则整体经验损失函数为：
$$
\mathcal{L}=\sum_{(\text{word}_I,\text{word}_O)\in\mathbb{D}}E(\text{word}_I,\text{word}_O)
$$
则网络的优化目标为：
$$
\min\mathcal{L}=\min_{\mathbf{W},\mathbf{W}^{\prime}}\sum_{(\text{word}_I,\text{word}_O)\in\mathbb{D}}\left(-\vec{\mathbf{w}}_{j^*}^{\prime}\cdot \vec{\mathbf{w}}_I+\log\sum_{i=1}^V\exp(\vec{\mathbf{w}}_{i}^{\prime}\cdot \vec{\mathbf{w}}_I)\right)
$$
考虑输入为目标单词前后的多个单词，输入为$C$个单词：$\vec{\mathbf{x}}_1,\cdots,\vec{\mathbf{x}}_C$。对于每个输入单词，其权重矩阵都为$\mathbf{W}$，这称作权重共享。这里的权重共享隐含着：每个单词的表达是固定的、唯一的，与它的上下文无关。隐向量为所有输入单词映射结果的均值：
$$
\vec{\mathbf{h}}=\frac{1}{C}\mathbf{W}^T(\vec{\mathbf{x}}_1+\cdots+\vec{\mathbf{x}}_C)=\frac{1}{C}(\vec{\mathbf{x}}_{I_1}+\cdots+\vec{\mathbf{x}}_{I_C})
$$
![](../../../picture/1/412.png)

假设给定一个单词序列$\text{word}_{I_1},\cdots,\text{word}_{I_C}$，观测到它的下一个单词为$\text{word}_{O}$。$\text{word}_{O}$对应的网络输出编号为$j^*$。定义损失函数为交叉熵：
$$
E=-u_{j^*}+\log\sum_{i=1}^V\exp(u_i)=-\vec{\mathbf{w}}_{j^*}^{\prime}\cdot \vec{\mathbf{h}}+\log\sum_{i=1}^V\vec{\mathbf{w}}_{i}^{\prime}\cdot \vec{\mathbf{h}}
$$
则网络的优化目标为：
$$
\min\mathcal{L}=\min_{\mathbf{W},\mathbf{W}^{\prime}}\sum_{(\text{word}_{I_1},\cdots,\text{word}_{I_C},\text{word}_O)\in\mathbb{D}}\left(-\vec{\mathbf{w}}_{j^*}^{\prime}\cdot \vec{\mathbf{w}}_I+\log\sum_{i=1}^V\exp(\vec{\mathbf{w}}_{i}^{\prime}\cdot \vec{\mathbf{w}}_I)\right)
$$


与`一个单词上下文`中推导的结果相同，这里给出参数更新规则：

- 更新 ：

  

  其中 。

- 更新 ：

  

  其中 ：

  -  ，它是词汇表 中所有单词的输出向量的加权和，其权重为 。
  -  为第 个输入单词在词表 中的编号。

在更新 时，如果有相同的输入单词（如： )，则在参数更新时认为它们是不同的。最终的效果就是在 中多次减去同一个增量 。

你也可以直接减去 ， 其中 为词汇表中单词 在输入中出现的次数。

##### `Skip-Gram`

`Skip-Gram` 模型是根据一个单词来预测其前后附近的几个单词。`Skip-Gram` 网络模型

![](../../../picture/1/413.png)

其中：

- 网络输入 ，它是输入单词的 `one-hote` 编码，其中只有一位为 1，其他位都为 0 。

- 网络输出 ，其中 是第 个输出单词为词汇表各单词的概率。

- 对于网络的每个输出 ，其权重矩阵都相同，为 。这称作权重共享。

  这里的权重共享隐含着：每个单词的输出向量是固定的、唯一的，与其他单词的输出无关。