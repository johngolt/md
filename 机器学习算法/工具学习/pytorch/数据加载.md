`Tensors`

`tensors`除了`run on GPUs or other hardware accelerators`跟`ndarrays`不同之外，其他方面相似。 tensors and `NumPy` arrays can often share the same underlying memory, eliminating the need to copy data

By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using `.to` method 

```python
# We move our tensor to the GPU if available
if torch.cuda.is_available():
  tensor = tensor.to('cuda')
```

对于只有一个元素的`tensor`可以使用`item()`

```python
agg = tensor.sum()
agg_item = agg.item()
```

```python
t = torch.ones(5)
n = t.numpy()
t = torch.from_numpy(n)
```

##### 数据加载

`Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples.

自定义的`Dataset`class需要实现三个函数：`__init__`,`__len__`,`__getitem__`。

the FashionMNIST images are stored in a directory `img_dir`, and their labels are stored separately in a CSV file `annotations_file`.

```python
import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
```

The `__getitem__` function loads and returns a sample from the dataset at the given index `idx`.

```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)
```

##### 创建模型

`torch.nn`amespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the `nn.Module`

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using {} device".format(device))

# Define model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
print(model)
```

Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s `parameters()` or `named_parameters()` methods.

In a forward pass, autograd does two things simultaneously:

- run the requested operation to compute a resulting tensor
- maintain the operation’s *gradient function* in the DAG.

The backward pass kicks off when `.backward()` is called on the DAG root. `autograd` then:

- computes the gradients from each `.grad_fn`,
- accumulates them in the respective tensor’s `.grad` attribute
- using the chain rule, propagates all the way to the leaf tensors.

##### 模型参数训练

Mathematically, if you have a vector valued function$\vec{\mathbf{y}}=f(\vec{\mathbf{x}})$, then the gradient of  $\vec{\mathbf{y}}$ with respect to $\vec{\mathbf{x}}$​ is a Jacobian matrix :
$$
J=\left[\begin{array}{cccc}\frac{\part y_1}{\part x_1}&\cdots&\frac{\part y_1}{\part x_n}\\
\cdot&\cdots&\cdot\\
\frac{\part y_m}{\part x_1}&\cdots&\frac{\part y_m}{\part x_m}
\end{array}
\right]
$$
`torch.autograd` is an engine for computing vector-Jacobian product. That is, given any vector $\vec{\mathbf{v}}$, compute the product $J^T\vec{\mathbf{v}}$

if $\vec{\mathbf{v}}$ happens to be the gradient fo a scalar function $l=g(\vec{\mathbf{y}})$
$$
\vec{\mathbf{v}}=(\frac{\part l}{\part y_1},\cdots,\frac{\part l}{\part y_m})^T
$$
then by the chain rule, the vector-Jacobian product would be the gradient of $l$ with respect to $\vec{\mathbf{x}}$
$$
J^T\vec{\mathbf{v}}=\left[\begin{array}{cccc}\frac{\part y_1}{\part x_1}&\cdots&\frac{\part y_1}{\part x_n}\\
\cdot&\cdots&\cdot\\
\frac{\part y_m}{\part x_1}&\cdots&\frac{\part y_m}{\part x_m}
\end{array}
\right]^T\left[\begin{array}{cccc}\frac{\part l}{\part y_1}\\
\cdot\\
\frac{\part l}{\part y_m}\end{array}
\right]=\left[\begin{array}{cccc}\frac{\part l}{\part x_1}\\
\cdot\\
\frac{\part l}{\part x_n}\end{array}\right]
$$
`external_grad` represents $\vec{\mathbf{v}}$

```python
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)

def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
            
def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
```

```python
epochs = 5
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train(train_dataloader, model, loss_fn, optimizer)
    test(test_dataloader, model, loss_fn)
print("Done!")
```

##### 模型保存和加载

```python
torch.save(model.state_dict(), "model.pth")
print("Saved PyTorch Model State to model.pth")
```

```python
model = NeuralNetwork()
model.load_state_dict(torch.load("model.pth"))

classes = [
    "T-shirt/top",
    "Trouser",
    "Pullover",
    "Dress",
    "Coat",
    "Sandal",
    "Shirt",
    "Sneaker",
    "Bag",
    "Ankle boot",
]

model.eval()
x, y = test_data[0][0], test_data[0][1]
with torch.no_grad():
    pred = model(x)
    predicted, actual = classes[pred[0].argmax(0)], classes[y]
    print(f'Predicted: "{predicted}", Actual: "{actual}"')
```

