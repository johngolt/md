The flowers dataset contains 5 sub-directories, one per class. Each directory contains images of that type of flower.

```
flowers_photos/
  daisy/
  dandelion/
  roses/
  sunflowers/
  tulips
 
image_count = len(list(data_dir.glob('*/*.jpg')))
```

```python
batch_size = 32
img_height = 180
img_width = 180
```

##### `tf.keras`

```python
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
```

```python
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
```

###### `standardize the data`

```
normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)
```

There are two ways to use this layer. You can apply it to the dataset by calling map

```python
normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
# Notice the pixels values are now in `[0,1]`.
```

Or, you can include the layer inside your model definition to simplify deployment.



##### `tf.data`

```python
list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'), shuffle=False)
list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)
```

The tree structure of the files can be used to compile a `class_names` list.

```python
class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.name != "LICENSE.txt"]))
```

```python
val_size = int(image_count * 0.2)
train_ds = list_ds.skip(val_size)
val_ds = list_ds.take(val_size)
```

###### Configure dataset for performance

```python
def configure_for_performance(ds):
  ds = ds.cache()
  ds = ds.shuffle(buffer_size=1000)
  ds = ds.batch(batch_size)
  ds = ds.prefetch(buffer_size=AUTOTUNE)
  return ds

train_ds = configure_for_performance(train_ds)
val_ds = configure_for_performance(val_ds)
```

###### Continue training the model

```python
model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=3
)
```