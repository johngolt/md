#### 朴素贝叶斯

朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。对给定的训练集：首先基于特征条件独立假设学习输入、输出的联合概率分布。然后基于此模型，对给定的输入$\vec{\mathbf{x}}$，利用贝叶斯定理求出后验概率最大的输出$y$。朴素贝叶斯法不是贝叶斯估计，贝叶斯估计是最大后验估计。

设输入空间$\mathcal{X} \subseteq \mathbb{R}^{n}$为$n$维向量的集合，输出空间为类标记集合$\mathcal{Y}=\left\{c_{1}, c_{2}, \cdots, c_{k}\right\}$。令$\vec{\mathbf{x}}=\left(x_{1}, x_{2}, \cdots, x_{n}\right)^{T}$为定义在$\mathcal{X}$上的随机向量，$y$为定义在$\mathcal{Y}$上的随机变量。

令$p(\vec{\mathbf{x}}, y)$为$\vec{\mathbf{x}}$和$y$的联合概率分布，假设训练数据集$\mathbb{D}=\left\{\left(\vec{\mathbf{x}}_{1}, \tilde{y}_{1}\right), \cdots,\left(\vec{\mathbf{x}}_{N}, \tilde{y}_{N}\right)\right\}$由$p(\vec{\mathbf{x}}, y)$独立同分布产生。朴素贝叶斯法通过训练数据集学习联合概率分布$p(\vec{\mathbf{x}}, y)$。

具体学习：先验概率分布：$p(y)$。条件概率分布：$p(\vec{\mathbf{x}} | y)=p\left(x_{1}, \cdots, x_{n} | y\right)$。朴素贝叶斯法对条件概率做了特征独立性假设：$p(\vec{\mathbf{x}} | y)=p\left(x_{1}, \cdots, x_{n} | y\right)=\prod_{j=1}^{n} p\left(x_{j} | y\right)$。这意味着在分类确定的条件下，用于分类的特征是条件独立的。根据贝叶斯定理：
$$
p(y | \vec{\mathbf{x}})=\frac{p(\vec{\mathbf{x}} | y) p(y)}{\sum_{y^{\prime}} p\left(\vec{\mathbf{x}} | y^{\prime}\right) p\left(y^{\prime}\right)}
$$
考虑分类特征的条件独立假设有：
$$
p(y | \vec{\mathbf{x}})=\frac{p(y) \prod_{i=1}^{n} p\left(x_{i} | y\right)}{\sum_{y} p\left(\vec{\mathbf{x}} | y^{\prime}\right) p\left(y^{\prime}\right)}
$$
则朴素贝叶斯分类器表示为：
$$
f(\vec{\mathbf{x}})=\arg \max _{y \in \mathcal{Y}} \frac{p(y) \prod_{i=1}^{n} p\left(x_{i} | y\right)}{\sum_{y} p\left(\vec{\mathbf{x}} | y^{\prime}\right) p\left(y^{\prime}\right)}
$$

###### 期望风险最小化

朴素贝叶斯分类器是后验概率最大化，等价于期望风险最小化。令损失函数为：
$$
\begin{array}{c}L(y, f(\vec{\mathbf{x}}))=\left\{\begin{array}{l}{1,} & {y \neq f(\vec{\mathbf{x}})} \\ {0,} & {y=f(\vec{\mathbf{x}})}\end{array}\right.\\R_{e x p}(f)=\mathbb{E}[L(y, f(\vec{\mathbf{x}}))]=\sum_{\vec{\mathbf{x}} \in \mathcal{X}} \sum_{y \in \mathcal{Y}} L(y, f(\vec{\mathbf{x}})) p(\vec{\mathbf{x}}, y)
\end{array}
$$
根据$p(\vec{\mathbf{x}}, y)=p(\vec{\mathbf{x}}) p(y | \vec{\mathbf{x}})$有：
$$
R_{e x p}(f)=\mathbb{E}[L(y, f(\vec{\mathbf{x}}))]=\sum_{\vec{\mathbf{x}} \in \mathcal{X}} \sum_{y \in \mathcal{Y}} L(y, f(\vec{\mathbf{x}})) p(\vec{\mathbf{x}}, y)=\mathbb{E}_{X}\left[\sum_{y \in \mathcal{Y}} L(y, f(\vec{\mathbf{x}})) p(y | \vec{\mathbf{x}})\right]
$$
为了使得期望风险最小化，只需要对$\mathbb{E}_{X}$中的元素极小化。令$\hat{y}=f(\vec{\mathbf{x}})$，则有：
$$
\begin{array}{l}{\arg \min _{\hat{y}} \sum_{y \in \mathcal{Y}} L(y, \hat{y}) p(y | \vec{\mathbf{x}})=\arg \min _{\hat{y}} \sum_{y \in \mathcal{Y}} p(y \neq \hat{y} | \vec{\mathbf{x}})} \\ {=\arg \min _{\hat{y}}(1-p(\hat{y} | \vec{\mathbf{x}}))=\arg \max _{\hat{y}} p(\hat{y} | \vec{\mathbf{x}})}\end{array}
$$
即：期望风险最小化，等价于后验概率最大化。

在朴素贝叶斯法中，学习意味着估计概率：$p(y)$，$p(x_i|y)$。可以用极大似然估计相应概率。先验概率$p(y)$的极大似然估计为：
$$
p(y=c_k)=\frac{1}{N}\sum_{i=1}^N\mathbf{I}(\tilde{y}_i=c_k)
$$
设第$j$个特征$x_j$可能的取值为$\{a_{j,1},\cdots,a_{j,s_j}\}$，则条件概率 的极大似然估计为：
$$
p(x_j=a_{j,l}|y=c_k) = \frac{\sum_{i=1}^NI(x_{i,j}=a_{j,l},\tilde{y}_i=c_k)}{\sum_{i=1}^N\mathbf{I}(\tilde{y}_i=c_k)}
$$
其中：$I(\cdot)$为示性函数，$x_{i,j}$表示第$i$个样本的第$j$个特征。

###### 贝叶斯估计

在估计概率$p(x_i|y)$的过程中，分母$\sum_{i=1}^NI(\tilde{y}_i=c_k)$可能为 0 。这是由于训练样本太少才导致$c_k$的样本数为 0 。而真实的分布中，$c_k$的样本并不为 0 。解决的方案是采用贝叶斯估计。

假设第$j$个特征$x_j$可能的取值为$\{a_{j,1},\cdots,a_{j,s_j}\}$，贝叶斯估计假设在每个取值上都有一个先验的计数 。即：
$$
\begin{equation}\begin{aligned}
p_{\lambda}\left(x_{j}=a_{j, l} | y=c_{k}\right) &=\frac{\sum_{i=1}^{N} I\left(x_{i, j}=a_{j, l}, \tilde{y}_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N} I\left(\tilde{y}_{i}=c_{k}\right)+s_{j} \lambda} \\
j &=1,2, \cdots, n ; l=1,2, \cdots, s_{j} ; k=1,2, \cdots, K
\end{aligned}\end{equation}
$$
它等价于在$x_j$的各个取值的频数上赋予了一个正数$\lambda$。若$c_k$的样本数为0，则它假设特征$x_j$每个取值的概率为$\frac{1}{s_j}$，即等可能的。采用贝叶斯估计后， 的贝叶斯估计调整为:
$$
\begin{equation}\begin{aligned}
p_{\lambda}\left(y=c_{k}\right) &=\frac{\sum_{i=1}^{N} I\left( \tilde{y}_{i}=c_{k}\right)+\lambda}{N+K \lambda}
\end{aligned}\end{equation}
$$

- 当$\lambda=0$时，为极大似然估计；当$\lambda=1$时，为拉普拉斯平滑
- 若$c_k$的样本数为 0，则假设赋予它一个非零的概率$\frac{\lambda}{N+K\lambda}$

朴素贝叶斯分类器的优点：性能相当好，它速度快，可以避免维度灾难。支持大规模数据的并行学习，且天然的支持增量学习。

朴素贝叶斯分类器的缺点：无法给出分类概率，因此难以应用于需要分类概率的场景。

##### 半朴素贝叶斯分类器

半朴素贝叶斯分类器原理：适当考虑一部分特征之间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较强的特征依赖关系。

###### 独依赖估计OED

独依赖估计是半朴素贝叶斯分类器最常用的一种策略。它假设每个特征在类别之外最多依赖于一个其他特征，即
$$
\begin{equation}p(\vec{\mathbf{x}} | y)=p\left(x_{1}, x_{2}, \cdots, x_{n} | y\right)=\prod_{j=1}^{n} p\left(x_{j} | y, x_{j}^{P}\right)\end{equation}
$$
其中$x_j^P$为特征$x_j$所依赖的特征，称作的$x_j$父特征。如果父属性已知，那么可以用贝叶斯估计来估计概率值$p\left(x_{j} | y, x_{j}^{P}\right)$。现在的问题是：如何确定每个特征的父特征？不同的做法产生不同的独依赖分类器。

###### `SPODE`

最简单的做法是：假设所有的特征都依赖于同一个特征，该特征称作超父。然后通过交叉验证等模型选择方法来确定超父特征。这就是`SPODE:Super-Parent ODE`方法。假设节点 `Y` 代表输出变量$y$，节点 $X_j$代表属性$x_j$。下图给出了超父特征为$x_1$时的 `SPODE` 。

![](../../picture/2/280.png)

###### `TAN`

`TAN:Tree Augmented naive Bayes`是在最大带权生成树算法基础上，通过下列步骤将特征之间依赖关系简化为如下图所示的树型结构：

- 计算任意两个特征之间的条件互信息。记第$i$个特征$x_i$代表的结点为$\mathbf{X}_i$，标记代表的节点为$\mathbf{Y}$则有:
  $$
  \begin{equation}I\left(\mathbf{X}_{i}, \mathbf{X}_{j} | \mathbf{Y}\right)=\sum_{y} \sum_{x_{i}} \sum_{x_{j}} p\left(x_{i}, x_{j} | y\right) \log \frac{p\left(x_{i}, x_{j} | y\right)}{p\left(x_{i} | y\right) p\left(x_{j} | y\right)}\end{equation}
  $$
  如果两个特征$x_i,x_j$相互条件独立，则$p\left(x_{i}, x_{j} | y\right)=p\left(x_{i} | y\right) p\left(x_{j} | y\right)$。则有条件互信息$I\left(\mathbf{X}_{i}, \mathbf{X}_{j} | \mathbf{Y}\right)$，则在图中这两个特征代表的结点没有边相连。

- 以特征为结点构建完全图，任意两个结点之间边的权重设为条件互信息$I\left(\mathbf{X}_{i}, \mathbf{X}_{j} | \mathbf{Y}\right)$。

- 构建此完全图的最大带权生成树，挑选根结点，将边置为有向边。

- 加入类别结点$\mathbf{Y}$，增加$\mathbf{Y}$到每个特征的有向边。因为所有的条件概率都是以$y$为条件的。

#### K近邻

$k$近邻法是一种基本的分类与回归方法。

- 分类问题：对新的样本，根据其$k$个最近邻的训练样本的类别，通过多数表决等方式进行预测。
- 回归问题：对新的样本，根据其$k$个最近邻的训练样本标签值的均值作为预测值。

近邻模型具有非常高的容量，这使得它在训练样本数量较大时能获得较高的精度。它的缺点有：计算成本很高。因为需要构建一个$N\times N$的距离矩阵，其计算量为$O(N^2)$，其中$N$为训练样本的数量。在训练集较小时，泛化能力很差，非常容易陷入过拟合。无法判断特征的重要性。

近邻法的三要素： $k$值选择、距离度量、决策规则。

###### K值选择

$k$值的选择会对$k$近邻法的结果产生重大影响。

- 若$k$值较小，则相当于用较小的邻域中的训练样本进行预测，"学习"的偏差减小。只有与输入样本较近的训练样本才会对预测起作用，预测结果会对近邻的样本点非常敏感。即： 值的减小意味着模型整体变复杂，易发生过拟合。
- 若$k$值较大，则相当于用较大的邻域中的训练样本进行预测。这时输入样本较远的训练样本也会对预测起作用，使预测偏离预期的结果。即： 值增大意味着模型整体变简单。

应用中，$k$值一般取一个较小的数值。通常采用交叉验证法来选取最优的$k$值。

###### 距离度量

特征空间中两个样本点的距离是两个样本点的相似程度的反映。$k$近邻模型的特征空间一般是$n$维实数向量空间$\mathbb{R}^n$,其距离一般为欧氏距离，也可以是一般的$L_p$距离。
$$
L_p(\vec{\mathbf{x}}_i,\vec{\mathbf{x}}_j)=\left(\sum_{l=1}^n|x_{i,l}-x_{j,l}|^p\right)^{1/p}
$$
不同的距离度量所确定的最近邻点是不同的。

###### 决策规则

分类决策通常采用多数表决，也可以基于距离的远近进行加权投票：距离越近的样本权重越大。多数表决等价于经验风险最小化。设分类的损失函数为$0-1$损失函数，分类函数为$f:\mathbb{R}^n\rightarrow\{c_1,\cdots,c_K\}$。给定样本$\vec{\mathbf{x}}\in\mathcal{X}$，其最邻近的$k$个训练点构成集合$\mathcal{N}_k(\vec{\mathbf{x}})$。设涵盖$\mathcal{N}_k(\vec{\mathbf{x}})$区域的类别为$c_m$，则损失函数为：
$$
\begin{equation}L=\frac{1}{k} \sum_{\vec{\mathbf{x}}_{i} \in \mathcal{N}_{k}(\mathbf{x})} I\left(\tilde{y}_{i} \neq c_{m}\right)=1-\frac{1}{k} \sum_{\vec{\mathbf{x}}_{i} \in \mathcal{N}_{i}(\vec{\mathbf{x}})} I\left(\tilde{y}_{i}=c_{m}\right)\end{equation}
$$
$L$就是训练数据的经验风险。要使经验风险最小，则使得$\sum_{\vec{\mathbf{x}}_{i} \in \mathcal{N}_{i}(\vec{\mathbf{x}})} I\left(\tilde{y}_{i}=c_{m}\right)$最大。即多数表决$c_m=\text{argmax}_{c_m}\sum_{\vec{\mathbf{x}}_{i} \in \mathcal{N}_{i}(\vec{\mathbf{x}})} I\left(\tilde{y}_{i}=c_{m}\right)$。

回归决策通常采用均值回归，也可以基于距离的远近进行加权投票：距离越近的样本权重越大。均值回归等价于经验风险最小化。设回归的损失函数为均方误差。给定样本$\vec{\mathbf{x}}\in\mathcal{X}$，其最邻近的$k$个训练点构成集合$\mathcal{N}_k(\vec{\mathbf{x}})$。设涵盖$\mathcal{N}_k(\vec{\mathbf{x}})$区域的类别为$\hat{y}$，则损失函数为：
$$
\begin{equation}L=\frac{1}{k} \sum_{\vec{\mathbf{x}}_{i} \in \mathcal{N}_{k}(\mathbf{x})} (\tilde{y}_i-\hat{y}_i)^2\end{equation}
$$
$L$​就是训练数据的经验风险。要使经验风险最小，则有：$\hat{y}=\frac{1}{k} \sum_{\vec{\mathbf{x}}_{i} \in \mathcal{N}_{k}(\mathbf{x})} \tilde{y}_i$​。即：均值回归。

##### `kd`树

$kd$树是一种对$k$维空间中的样本点进行存储以便对其进行快速检索的树型数据结构。它是二叉树，表示对$k$维空间的一个划分。构造$kd$树的过程相当于不断的用垂直于坐标轴的超平面将$k$维空间切分的过程。$kd$树的每个结点对应于一个$k$维超矩形区域。

![](../../picture/2/48.png)

$kd$树搜索算法

![](../../picture/2/49.png)

