集成学习`ensemble learning`是通过构建并结合多个学习器来完成学习任务。其一般结构为：

- 先产生一组“个体学习器”（`individual learner`) 。个体学习器通常由一种或者多种现有的学习算法从训练数据中产生。如果个体学习器都是从某一种学习算法从训练数据中产生，则称这样的集成学习是同质的`homogenerous`。此时的个体学习器也称作基学习器`base learner`，相应的学习算法称作基学习算法。如果个体学习器是从某几种学习算法从训练数据中产生，则称这样的集成学习是异质的`heterogenous` 。
- 再使用某种策略将它们结合起来。集成学习通过将多个学习器进行组合，通常可以获得比单一学习器显著优越的泛化性能。

根据个体学习器的生成方式，目前的集成学习方法大概可以分作两类：

- 个体学习器之间存在强依赖关系、必须串行生成的序列化方法，每一轮迭代产生一个个体学习器。其中以`Boosting`为代表。
- 个体学习器之间不存在强依赖关系、可同时生成的并行化方法。其中以`Bagging`和随机森林`Random Forest`为代表。

##### `Boosting`

`Boosting` 就是一族可以将弱学习器提升为强学习器的算法。这族算法的工作原理类似：

- 先从初始训练集训练出一个基学习器。
- 再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注。
- 然后基于调整后的样本分布来训练下一个基学习器。
- 如此重复，直到基学习器数量达到事先指定的值`M` 。
- 最终将这`M`个基学习器进行加权组合。

###### `AdaBoost`算法

`AdaBoot`算法两个核心步骤：

- 每一轮中如何改变训练数据的权值？`AdaBoost`算法提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。于是那些没有得到正确分类的数据由于权值的加大而受到后一轮的弱分类器的更大关注。

- 最后如何将一系列弱分类器组合成一个强分类器？`AdaBoost` 采用加权多数表决的方法：加大分类误差率较小的弱分类器的权值，使得它在表决中起较大作用。减小分类误差率较大的弱分类器的权值，使得它在表决中起较小的作用。


`AdaBoost`算法有两个特点：

不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同作用。

- 因此`AdaBoost`要求基本学习器能够对特定的数据分布进行学习，这一般是在学习的时候为每个训练样本赋予一个权重。
- 对于无法接受带权样本的基本学习算法，则可以通过“重采样法”来处理：即在每一轮学习中，根据样本分布对训练集重新采样，再用重采样的样本集对基本学习器进行训练。

利用基本分类器的线性组合$f(\vec{\mathbf{x}})=\sum_{m=1}^M\alpha_mh_m(\vec{\mathbf{x}})$构成最终分类器：
$$
H(\vec{\mathbf{x}})=\text{sign}(f(\vec{\mathbf{x}}))=\text{sign}(\sum_{m=1}^M\alpha_mh_m(\vec{\mathbf{x}}))
$$
其中：$f(\vec{\mathbf{x}})$的符号决定实例$\vec{\mathbf{x}}$的分类。$f(\vec{\mathbf{x}})$的绝对值表示分类的确信度。

`AdaBoost` 算法可以认为是：模型为加法模型、损失函数为指数函数、学习算法为前向分步算法的二类分类学习方法。其中指数损失函数为：
$$
L(\tilde{y},f(\vec{\mathbf{x}}))=e^{-\tilde{y}f(\vec{\mathbf{x}})}
$$
考虑加法模型$\hat{y}=f(\vec{\mathbf{x}})=\sum_{m=1}^M\beta_mb(\vec{\mathbf{x}};\gamma_m)$，其中$b(\vec{\mathbf{x}};\gamma_m)$为基函数、$\gamma_m$为基函数的参数、$\beta_m$为基函数的系数。

给定训练数据以及损失函数$L(\tilde{y},\hat{y})$的条件下，根据经验风险极小化（损失函数极小化）准测来学习加法模型$f(\vec{\mathbf{x}})$：
$$
\min_{\beta_m,\gamma_m}\sum_{i=1}^NL(\tilde{y},\sum_{m=1}^M\beta_mb(\vec{\mathbf{x}};\gamma_m))
$$
前向分步算法求解这一优化问题的思想是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数，则可以简化优化的复杂度。具体地，每一步只需要优化如下的损失函数：
$$
(\beta_m,\gamma_m)=\arg\min_{\beta,\gamma}\sum_{i=1}^NL(\tilde{y},f_{m-1}(\vec{\mathbf{x}})+\beta b(\vec{\mathbf{x}};\gamma))
$$

##### `Bagging`

`Bagging`直接基于自助采样法`bootstrap sampling`。自助采样法的步骤是：给定包含$N$个样本的数据集：

- 先随机取出一个样本放入采样集中，再把该样本放回原始数据集。
- 这样经过$N$次随机采样操作，得到包含$N$个样本的采样集。

自助采样法给`Bagging`算法带来的优点：由于每个基学习器只用初始训练集中约 63.2% 的样本来训练，剩下的约 36.8% 的样本可用作验证集来对泛化性能进行包外估计。

`Bagging`的基本流程：经过$M$轮自助采样，可以得到$M$个包含$N$个训练样本的采样集。然后基于每个采样集训练出一个基学习器。最后将这$M$个基学习器进行组合，得到集成模型。

在使用 `Bagging`学习器进行预测时：

- 分类任务采取简单投票法，取每个基学习器的预测类别的众数。
- 回归任务使用简单平均法，取每个基学习器的预测值的平均。

从`偏差-方差分解`的角度来看：

- `Bagging`主要关注降低方差，它能平滑强学习器的方差。因此它在非剪枝决策树、神经网络等容易受到样本扰动的学习器上效果更为明显。
- `Boosting` 主要关注降低偏差，它能将一些弱学习器提升为强学习器。因此它在`SVM` 、`knn` 等不容易受到样本扰动的学习器上效果更为明显。

###### `Random Forest`

随机森林在以决策树为基学习器构建`Bagging`集成模型的基础上，进一步在决策树的训练过程中引入了随机属性选择。

- 传统决策树在选择划分属性时，是在当前结点的属性集合（假定有$n$个属性）中选择一个最优属性。
- 随机森林中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含$k$个属性的子集，然后再从这个子集中选择一个最优属性用于划分。
  - 如果$k=n$，则基决策树的构建与传统决策树相同。
  - 如果$k=1$，则随机选择一个属性用于划分。
  - 通常建议$k=\log_2n$。

随机森林的优点：训练效率较高。因为随机森林使用的决策树只需要考虑所有属性的一个子集。随机森林简单、容易实现、计算开销小。随机森林在很多现实任务中展现出强大的性能，被称作 “代表集成学习技术水平的方法”。

##### 集成策略

假定集成包含$M$个基学习器$h_1,h_2,\cdots,h_M$。

###### 平均法

平均法通常用于回归任务中。简单平均法：$H(\vec{\mathbf{x}})=\frac{1}{M}\sum_{i=1}^Mh_i(\vec{\mathbf{x}})$。加权平均法：
$$
H(\vec{\mathbf{x}})=\frac{1}{M}\sum_{i=1}^M\omega_ih_i(\vec{\mathbf{x}})\\
\omega_i\ge0, \sum_{i=1}^m\omega_i=1
$$
其中学习器$h_i$的权重$\omega_i$是从训练数据中学的。现实任务中训练样本通常不充分或者存在噪声，这就使得学得的权重不完全可靠。尤其是对于规模比较大的集成学习，要学习的权重比较多，很容易出现过拟合。通常如果个体学习器性能相差较大时，适合使用加权平均法；个体学习器性能相差较近时，适合使用简单平均法。

###### 投票法

投票法通常用于分类任务中。绝大多数投票法：若某个标记得票数过半，则预测为该标记；否则拒绝预测。相对多数投票法：选取得票最多的标记作为预测值： 
$$
H(\vec{\mathbf{x}})=\arg\max_{c_j}\sum_{i=1}^MI(h_i(\vec{\mathbf{x}})=c_j)
$$
加权投票法：类似于加权平均法，其中学习器$h_i$的权重$\omega_i$是从训练数据中学的：
$$
H(\vec{\mathbf{x}})=\arg\max_{c_j}\sum_{i=1}^M\omega_iI(h_i(\vec{\mathbf{x}})=c_j)
$$

###### 学习法

学习法中，个体学习器的分类结果通过与另一个学习器来组合。此时称个体学习器为初级学习器，用于组合的学习器称作次级学习器或者元学习器`meta_learner`。

学习法的典型代表就是`stacking`集成算法。`stacking` 集成算法中：

- 首先从初始数据集训练出初级学习器。
- 然后将初级学习器的预测结果作为一个新的数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样本输入特征；初始样本的标记仍被视作标记。

若直接使用初级学习器的输出来产生次级训练集，则容易发生过拟合。一般是通过使用交叉验证，使用训练初级学习器时未使用的样本来产生次级学习器的训练样本。

次级学习器的输入属性表示和次级学习算法对`stacking`集成算法的泛化性能有很大影响。通常推荐：

- 次级学习器的输入特征是以初级学习器的输出类概率为特征。
- 次级学习算法采用多响应线性回归`Multi-response Linear Regression:MLR` 。