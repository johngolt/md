#### 支持向量机

支持向量机的目的是用训练数据集的间隔最大化（泛化能力好）找到一个最优分离超平面。当训练数据集线性可分时，存在无穷个分离超平面可以将两类数据正确分开。

- 感知机利用误分类最小的策略，求出分离超平面。但是此时的解有无穷多个。
- 线性可分支持向量机利用间隔最大化求得最优分离超平面，这样的解只有唯一的一个。

支持向量机学习基本思想：求解能够正确划分训练数据集并且几何间隔最大的分离超平面。几何间隔最大化又称作硬间隔最大化。支持向量机支持处理线性可分数据集、非线性可分数据集。

- 当训练数据线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机（也称作硬间隔支持向量机）。
- 当训练数据近似线性可分时，通过软间隔最大化，学习一个线性分类器，即线性支持向量机（也称为软间隔支持向量机）。
- 当训练数据不可分时，通过使用核技巧以及软间隔最大化，学习一个非线性分类器，即非线性支持向量机。

可以将一个点距离分离超平面的远近来表示分类预测的可靠程度：

- 一个点距离分离超平面越远，则该点的分类越可靠。
- 一个点距离分离超平面越近，则该点的分类则不那么确信。

可以用$y(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}+b)$来表示分类的正确性以及确信度，就是函数间隔的概念。符号决定了正确性。范数决定了确信度。

对于给定的训练数据集$\mathbb{D}$和超平面$(\vec{\mathbf{w}},b)$ 

- 定义超平面$(\vec{\mathbf{w}},b)$关于样本点$(\vec{\mathbf{x}}_i,\tilde{y}_i)$ 的函数间隔为：$\hat{\gamma}_i=\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b)$  
- 定义超平面$(\vec{\mathbf{w}},b)$关于训练集$\mathbb{D}$的函数间隔为：超平面$(\vec{\mathbf{w}},b)$关于$\mathbb{D}$中所有样本点  的函数间隔之最小值：$\hat{\gamma}=\min_{\mathbb{D}}\hat{\gamma}_i$。

对于给定的训练数据集$\mathbb{D}$和超平面$(\vec{\mathbf{w}},b)$ 

- 定义超平面$(\vec{\mathbf{w}},b)$关于样本点$(\vec{\mathbf{x}}_i,\tilde{y}_i)$ 的几何间隔为：$\gamma_i=\tilde{y}_i(\frac{\vec{\mathbf{w}}}{||\vec{\mathbf{w}}||_2}\cdot\vec{\mathbf{x}}_i+\frac{b}{||\vec{\mathbf{w}}||_2})$  
- 定义超平面$(\vec{\mathbf{w}},b)$关于训练集$\mathbb{D}$的几何间隔为：超平面$(\vec{\mathbf{w}},b)$关于$\mathbb{D}$中所有样本点  的几何间隔之最小值：$\gamma=\min_{\mathbb{D}}\gamma_i$。

当超平面参数$\vec{\mathbf{w}},b$等比例改变时：超平面并没有变化；函数间隔也按比例改变；几何间隔保持不变。

支持向量机学习基本思想：求解能够正确划分训练数据集并且几何间隔最大的分离超平面。几何间隔最大化又称作硬间隔最大化。几何间隔最大化的物理意义：不仅将正负实例点分开，而且对于最难分辨的实例点，也有足够大的确信度来将它们分开。

求解几何间隔最大的分离超平面可以表示为约束的最优化问题：
$$
\max_{\vec{\mathbf{w}},b}\gamma\\
\tilde{y}_i(\frac{\vec{\mathbf{w}}}{||\vec{\mathbf{w}}||_2}\cdot\vec{\mathbf{x}}_i+\frac{b}{||\vec{\mathbf{w}}||_2})\ge \gamma, i=1,\cdots,N
$$
考虑几何间隔和函数间隔的关系，改写问题为：
$$
\max_{\vec{\mathbf{w}},b}\frac{\hat{\gamma}}{||\vec{\mathbf{w}}||_2}\\
\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b)\ge\hat{\gamma}, i=1,\cdots,N
$$
函数间隔$\hat{\gamma}$的大小并不影响最优化问题的解。假设将$\vec{\mathbf{w}},b$按比例的改变为$\lambda\vec{\mathbf{w}},\lambda b$，此时函数间隔变成 $\lambda\hat{\gamma}$：这一变化对求解最优化问题的不等式约束没有任何影响。这一变化对最优化目标函数也没有影响。因此取$\hat{\gamma}=1$，则最优化问题改写为：

$$
\max_{\vec{\mathbf{w}},b}\frac{1}{||\vec{\mathbf{w}}||_2}\\
\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b)\ge1, i=1,\cdots,N
$$
由于$\max_{\vec{\mathbf{w}},b}\frac{1}{||\vec{\mathbf{w}}||_2}$和$\min\frac{1}{2}||\vec{\mathbf{w}}||_2^2$是等价的，于是最优化问题改写为：
$$
\min_{\vec{\mathbf{w}},b}\frac{1}{2}{||\vec{\mathbf{w}}||_2^2}\\
\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b)-1\ge0, i=1,\cdots,N
$$
这是一个凸二次规划问题。凸优化问题 ，指约束最优化问题：
$$
\min_{\vec{\mathbf{w}}}f(\vec{\mathbf{w}})\\
g_j(\vec{\mathbf{w}})\le0, j=1,\cdots,J\\
h_k(\vec{\mathbf{w}})=0, k=1,2,\cdots,K
$$
其中：目标函数$f(\vec{\mathbf{w}})$和约束函数$g_j(\vec{\mathbf{w}})$都是$\mathbb{R}^n$上的连续可微的凸函数。约束函数$h_k(\vec{\mathbf{w}})$是$\mathbb{R}^n$上的仿射函数。$h(\vec{\mathbf{x}})$称为仿射函数，如果它满足$h(\vec{\mathbf{x}})=\vec{\mathbf{a}}\cdot\vec{\mathbf{x}}+b$。当目标函数$f(\vec{\mathbf{w}})$是二次函数且约束函数$g_j(\vec{\mathbf{w}})$是仿射函数时，上述凸最优化问题成为凸二次规划问题。

在训练数据集线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量。支持向量是使得约束条件等号成立的点，即
$$
\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b)-1=0
$$
支持向量是使得约束条件等号成立的点，即$\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b)-1=0$。在决定分离超平面时，只有支持向量起作用，其他的实例点并不起作用。如果移动支持向量，将改变所求的解。如果在间隔边界以外移动其他实例点，甚至去掉这些点，则解是不变的。

###### 对偶算法

将线性可分支持向量机的最优化问题作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。这就是线性可分支持向量机的对偶算法。定义拉格朗日函数：
$$
L(\vec{\mathbf{w}},b,\vec{\mathbf{\alpha}})=\frac{1}{2}||\vec{\mathbf{w}}||^2_2-\sum_{i=1}^N\alpha_i[\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b)-1]+\sum_{i=1}^N\alpha_i
$$
其中$\vec{\alpha}=(\alpha_1,\cdots,\alpha_n)^T$为拉格朗日乘子向量。根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：
$$
\max_{\vec{\alpha}}\min_{\vec{\mathbf{w}},b}L(\vec{\mathbf{w}},b,\vec{\mathbf{\alpha}})
$$
先求$\min_{\vec{\mathbf{w}},b}L(\vec{\mathbf{w}},b,\vec{\mathbf{\alpha}})$。拉格朗日函数分别为$\vec{\mathbf{w}},b$求偏导数，并令其等于0
$$
\nabla_{\vec{\mathbf{w}}}L(\vec{\mathbf{w}},b,\vec{\mathbf{\alpha}})=\vec{\mathbf{w}}-\sum_{i=1}^N\alpha_i\tilde{y}_i\vec{\mathbf{x}}_i=0\\
\nabla_bL(\vec{\mathbf{w}},b,\vec{\mathbf{\alpha}})=\sum_{i=1}^N\alpha_i\tilde{y}_i=0
$$
代入拉格朗日函数：
$$
L(\vec{\mathbf{w}},b,\vec{\mathbf{\alpha}})=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j\tilde{y}_i\tilde{y}_j(\vec{\mathbf{x}}_i\cdot\vec{\mathbf{x}}_j)+\sum_{i=1}^N\alpha_i
$$
对偶问题极大值为：
$$
\min_{\vec{\alpha}}\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j\tilde{y}_i\tilde{y}_j(\vec{\mathbf{x}}_i\cdot\vec{\mathbf{x}}_j)-\sum_{i=1}^N\alpha_i\\
\sum_{i=1}^N\alpha_i\tilde{y}_i=0\\
\alpha_i\ge0, i=1,2,\cdots,N
$$
设对偶最优化问题的$\vec{\alpha}$的解为$\vec{\alpha}^*=(\alpha_1^*, \cdots,\alpha_N^*)^T$，则根据 `KKT` 条件有：
$$
\nabla_{\vec{\mathbf{w}}}L(\vec{\mathbf{w}}^*,b^*,\vec{\mathbf{\alpha}}^*)=\vec{\mathbf{w}}^*-\sum_{i=1}^N\alpha_i^*\tilde{y}_i\vec{\mathbf{x}}_i=0\\
\nabla_bL(\vec{\mathbf{w}}^*,b^*,\vec{\mathbf{\alpha}}^*)=\sum_{i=1}^N\alpha_i^*\tilde{y}_i=0\\
\alpha_i^*[\tilde{y}_i(\vec{\mathbf{w}}^*\cdot\vec{\mathbf{x}}_i+b^*)-1]=0,i=1,\cdots,N\\
\tilde{y}_i(\vec{\mathbf{w}}^*\cdot\vec{\mathbf{x}}_i+b^*)-1\ge0,i=1,\cdots,N\\
\alpha_i^*\ge0, i=1,\cdots,N
$$
于是分离超平面写作：$\sum_{i=1}^N\alpha_i^*\tilde{y}_i(\vec{\mathbf{x}}\cdot\vec{\mathbf{x}}_i)+b^*=0$。$\vec{\mathbf{w}}^*,b^*$只依赖于$\alpha_i^*>0$对应的样本点$\vec{\mathbf{x}}_i,\tilde{y}_i$，而其他的样本点对于$\vec{\mathbf{w}}^*,b^*$没有影响。将训练数据集里面对应于$\alpha_i^*>0$的样本点对应的实例$\vec{\mathbf{x}}_i$称为支持向量。对于$\alpha_i^*>0$的样本点，根据$\alpha_i^*[\tilde{y}_i(\vec{\mathbf{w}}^*\cdot\vec{\mathbf{x}}_i+b^*)-1]=0$，有：$\tilde{y}_i(\vec{\mathbf{w}}^*\cdot\vec{\mathbf{x}}_i+b^*)=1$。即$\vec{\mathbf{x}}_i$一定在间隔边界上。这与原始问题给出的支持向量的定义一致。

##### 线性支持向量机

对于线性不可分训练数据，线性支持向量机不再适用，但可以想办法将它扩展到线性不可分问题。假设训练数据集不是线性可分的，这意味着某些样本点$(\vec{\mathbf{x}}_i,\tilde{y}_i)$不满足函数间隔大于等于1的约束条件。对每个样本点$$(\vec{\mathbf{x}}_i,\tilde{y}_i)$$引进一个松弛变量$\zeta_i$，使得函数间隔加上松弛变量大于等于 1。即约束条件变成了：$\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b)\ge1-\zeta_i$。对每个松弛变量$\zeta_i$，支付一个代价$\zeta_i$。目标函数变成：

$$
\min_{\vec{\mathbf{w}},b,\vec{\zeta}}\frac{1}{2}{||\vec{\mathbf{w}}||_2^2}+C\sum_{i=1}^N\zeta_i
$$
于是线性不可分的线性支持向量机的学习问题变成了凸二次规划问题：
$$
\min_{\vec{\mathbf{w}},b,\vec{\zeta}}\frac{1}{2}{||\vec{\mathbf{w}}||_2^2}+C\sum_{i=1}^N\zeta_i\\
\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b)\ge1-\zeta_i, i=1,\cdots,N\\
\zeta_i\ge0, i=1,\cdots,N
$$
因为这是个凸二次规划问题，因此解存在。$\vec{\mathbf{w}}$的解是唯一的；$b$的解不是唯一的，$b$的解存在于一个区间。

###### 对偶问题

定义拉格朗日函数为：
$$
L(\vec{\mathbf{w}},b,\vec{\zeta},\vec{\mathbf{\alpha}},\vec{\mu})=\frac{1}{2}||\vec{\mathbf{w}}||^2_2+C\sum_{i=1}^N\zeta_i-\sum_{i=1}^N\alpha_i[\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b)-1+\zeta_i]+\sum_{i=1}^N\mu_i\zeta_i
$$
先求$L(\vec{\mathbf{w}},b,\vec{\zeta},\vec{\mathbf{\alpha}},\vec{\mu})$对$\vec{\mathbf{w}},b,\vec{\zeta}$的极小。根据偏导数为0：
$$
\nabla_{\vec{\mathbf{w}}}L(\vec{\mathbf{w}},b,\vec{\zeta},\vec{\mathbf{\alpha}},\vec{\mu})=\vec{\mathbf{w}}-\sum_{i=1}^N\alpha_i\tilde{y}_i\vec{\mathbf{x}}_i=0\\
\nabla_bL(\vec{\mathbf{w}},b,\vec{\zeta},\vec{\mathbf{\alpha}},\vec{\mu})=\sum_{i=1}^N\alpha_i\tilde{y}_i=0\\
\nabla_{\zeta_i}L(\vec{\mathbf{w}},b,\vec{\zeta},\vec{\mathbf{\alpha}},\vec{\mu})=C-\alpha_i-\mu_i=0
$$
再求极大问题：将上面三个等式代入拉格朗日函数：于是得到对偶问题：
$$
\min_{\vec{\alpha}}\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j\tilde{y}_i\tilde{y}_j(\vec{\mathbf{x}}_i\cdot\vec{\mathbf{x}}_j)-\sum_{i=1}^N\alpha_i\\
\sum_{i=1}^N\alpha_i\tilde{y}_i=0\\
C\ge\alpha_i\ge0, i=1,2,\cdots,N
$$
根据 `KKT` 条件：
$$
\nabla_{\vec{\mathbf{w}}}L(\vec{\mathbf{w}}^*,b^*,\vec{\zeta}^*,\vec{\mathbf{\alpha}}^*,\vec{\mu}^*)=\vec{\mathbf{w}}^*-\sum_{i=1}^N\alpha_i^*\tilde{y}_i\vec{\mathbf{x}}_i=0\\
\nabla_bL(\vec{\mathbf{w}}^*,b^*,\vec{\zeta}^*,\vec{\mathbf{\alpha}}^*,\vec{\mu}^*)=\sum_{i=1}^N\alpha_i^*\tilde{y}_i=0\\
\nabla_{\zeta_i}L(\vec{\mathbf{w}}^*,b^*,\vec{\zeta}^*,\vec{\mathbf{\alpha}}^*,\vec{\mu}^*)=C-\alpha_i^*-\mu_i^*=0\\
\alpha_i^*[\tilde{y}_i(\vec{\mathbf{w}}^*\cdot\vec{\mathbf{x}}_i+b^*)-1+\zeta_i^*]=0\\
\tilde{y}_i(\vec{\mathbf{w}}^*\cdot\vec{\mathbf{x}}_i+b^*)-1+\zeta_i^*\ge0\\
\mu_i^*\zeta_i^*=0,\zeta_i^*\ge0,\mu_i^*\ge0\\
C\ge\alpha_i^*\ge0\\
i=1,\cdots,N
$$
在线性不可分的情况下，对偶问题的解$\vec{\alpha}^*=(\alpha_1^*,\cdots,\alpha_N^*)^T$中，对应于$\alpha_i^*>0$的样本点$(\vec{\mathbf{x}}_i,\tilde{y}_i)$的实例点$\vec{\mathbf{x}}_i$称作支持向量，它是软间隔的支持向量。

根据$\nabla_{\zeta_i}L(\vec{\mathbf{w}}^*,b^*,\vec{\zeta}^*,\vec{\mathbf{\alpha}}^*,\vec{\mu}^*)=0$，以及$\mu_i^*\zeta_i^*=0$，则：

- 若$\alpha_i^*<C$，则$\mu_i^*>0$， 则松弛量$\zeta_i^*=0$。支持向量恰好落在了间隔边界上。
- 若$\alpha_i^*=C$， 则$\mu_i^*=0$，于是$\zeta_i^*$可能为任何正数：
  - 若$<0\zeta_i^*<1$，则支持向量落在间隔边界与分离超平面之间，分类正确。
  - 若$\zeta_i^*=1$，则支持向量落在分离超平面上。
  - 若$\zeta_i^*>1$，则支持向量落在分离超平面误分类一侧，分类错误。

##### 非线性支持向量机

用线性分类方法求解非线性分类问题分两步：首先用一个变换将原空间的数据映射到新空间。再在新空间里用线性分类学习方法从训练数据中学习分类模型。这一策略称作核技巧

核函数替代法，等价于：首先经过映射函数$\phi$将原来的输入空间变换到一个新的特征空间。然后将输入空间中的内积$\vec{\mathbf{x}}_i\cdot \vec{\mathbf{x}}_j$变换为特征空间中的内积$\phi(\vec{\mathbf{x}}_i)\cdot\phi(\vec{\mathbf{x}}_j)$。最后在新的特征空间里从训练样本中学习线性支持向量机。

##### `SVDD`

一类分类的策略是：训练出一个最小的超球面把正类数据包起来。识别一个新的数据点时，如果这个数据点落在超球面内，则属于正类；否则不是。

给定训练集$\mathbb{D}=\{\vec{\mathbf{x}}_1,\cdots,\vec{\mathbf{x}}_N\}$，这些样本都是属于同一类。`SVDD` 的的优化目标是：求一个中心为$\vec{\mathbf{o}}$，半径为$R$的最小球面，使得$\mathbb{D}$中的样本都在该球面中。类似`SVR`，`SVDD` 允许一定程度上的放松，引入松弛变量。对松弛变量$\xi$，其代价为$C\xi_i$。
$$
L(R,\vec{\mathbf{o}},\vec{\xi})=R^2+C\sum_{i=1}^N\xi_i\\
||\vec{\mathbf{x}}_i-\vec{\mathbf{o}}||^2_2\le R^2+\xi_i\\
\xi_i\ge0, i=1,\cdots,N
$$
`SVDD` 的求解也是采用拉格朗日乘子法：
$$
L(R,\vec{\mathbf{o}},\vec{\alpha},\vec{\xi},\vec{\gamma})=R^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^N\alpha_i( R^2+\xi_i-||\vec{\mathbf{x}}_i-\vec{\mathbf{o}}||^2_2)-\sum_{i=1}^N\gamma_i\xi_i
$$
先求极小问题：根据$L(R,\vec{\mathbf{o}},\vec{\alpha},\vec{\xi},\vec{\gamma})$对$R,\vec{\mathbf{o}},\vec{\xi}$偏导数为零可得：
$$
\sum_{i=1}^N\alpha_i=1\\
\vec{\mathbf{o}}=\frac{\sum_{i=1}^N\alpha_i\vec{\mathbf{x}}_i}{\sum_{i=1}^N\alpha_i}=\sum_{i=1}^N\alpha_i\vec{\mathbf{x}}_i\\
C-\alpha_i-\gamma_i=0, i=1,\cdots,N
$$
代入拉格朗日函数有：
$$
L=\sum\alpha_i(\vec{\mathbf{x}}_i\cdot\vec{\mathbf{x}}_i)-\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j(\vec{\mathbf{x}}_i\cdot\vec{\mathbf{x}}_j)\\
0\le\alpha_i\le C\\
\sum_{i=1}^N\alpha_i=1
$$
引入核函数：
$$
L=\sum\alpha_iK(\vec{\mathbf{x}}_i\cdot\vec{\mathbf{x}}_i)-\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jK(\vec{\mathbf{x}}_i\cdot\vec{\mathbf{x}}_j)\\
0\le\alpha_i\le C\\
\sum_{i=1}^N\alpha_i=1
$$
判断一个新的数据点$\vec{\mathbf{z}}$是否属于这个类，主要看它是否在训练出来的超球面内：若$||\vec{\mathbf{z}}-\vec{\mathbf{o}}||^2_2\le R^2$，则判定为属于该类。