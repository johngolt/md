给定样本$\vec{\mathbf{x}}$，其中$\vec{\mathbf{x}}=\{x_1,\cdots,x_n\}^T$，$x_i$为样本$\vec{\mathbf{x}}$的第$i$个特征。线性模型的形式为：
$$
f(\vec{\mathbf{x}})=\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}+b
$$
其中$\vec{\mathbf{w}}$为每个特征对应的权重生成的权重向量。线性模型的优点是：模型简单；可解释性强，权重向量$\vec{\mathbf{w}}$直观地表达了各个特征在预测中的重要性。

##### 线性回归

给定数据集$\mathbb{D}=\{(\vec{\mathbf{x}}_1,\tilde{y}_1),\cdots,(\vec{\mathbf{x}}_N,\tilde{y}_N)\}$，其中$\vec{\mathbf{x}}_i=(x_{i,1},\cdots,x_{i,n})^T\in\mathcal{X},y_i\in \mathcal{Y}$。线性回归问题试图学习模型：$f(\vec{\mathbf{x}})=\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}+b$。对于每个$\vec{\mathbf{x}}_i$，其预测值为$\hat{y}_i=f(\vec{\mathbf{x}}_i)=\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b$。采用平方损失函数，则在训练集 上，模型的损失函数为：

$$
L(f)=\sum_{i=1}^N(\hat{y}_i-\tilde{y}_i)^2=\sum_{i=1}^N(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b-\tilde{y}_i)^2
$$
优化目标是损失函数最小化，即：
$$
(\vec{\mathbf{w}}^*,b^*)=\arg\min_{\vec{\mathbf{w}},b}\sum_{i=1}^N(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b-\tilde{y}_i)^2
$$
可以用梯度下降法来求解上述最优化问题的数值解，但是实际上该最优化问题可以通过最小二乘法获得解析解

当$\mathbf{X}^T\mathbf{X}$为满秩矩阵时，可得：$\vec{\mathbf{w}}^*=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\vec{\mathbf{y}}$。当$\mathbf{X}^T\mathbf{X}$不是满秩矩阵。此时存在多个解析解，他们都能使得均方误差最小化。究竟选择哪个解作为输出，由算法的偏好决定。

##### 广义线性模型

考虑单调可微函数$g(\cdot)$，令$g(y)=\vec{\mathbf{w}}^T\vec{\mathbf{x}}+b$，这样得到的模型称作广义线性模型。其中函数$g(\cdot)$称作联系函数。

如果给定$\vec{\mathbf{x}}$和$\vec{\mathbf{w}}$之后，$y$的条件概率分布$p(y | \vec{\mathbf{x}} ; \vec{\mathbf{w}})$服从指数分布族，则该模型称作广义线性模型。指数分布族的形式为：$p(y ; \eta)=b(y) * \exp (\eta T(y)-a(\eta))$。$\eta$是$\vec{\mathbf{x}}$的线性函数：$\eta=\vec{\mathbf{w}}^{T} \vec{\mathbf{x}}$ 。$b(y),T(y)$为$y$的函数。$\alpha(\eta)$为$\eta$的函数.

###### 高斯分布

假设$y$服从高斯正态分布，则可以写成如下的指数分布族形式：
$$
\begin{array}{c}p(y)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(y-\mu)^{2}}{2 \sigma^{2}}\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{y^{2}}{2 \sigma^{2}}\right) \exp \left(\frac{\mu}{\sigma^{2}} \times y-\frac{\mu^{2}}{2 \sigma^{2}}\right) \\
{c}
b(y)=\frac{1}{\sqrt{2 \pi} \sigma} \times \exp \left(-\frac{y^{2}}{2 \sigma^{2}}\right) \\
T(y)=y \\
\eta=\frac{\mu}{\sigma^{2}} \\
a(\eta)=\frac{\mu^{2}}{2 \sigma^{2}}
\end{array}
$$

###### 多元伯努利分布

如果$y$服从多元伯努利分布，假设有$K$个分类，样本标记$\tilde{y} \in\{1,2, \cdots, K\}$。每种分类对应的概率为$\phi_{1}, \phi_{2}, \cdots, \phi_{K}$。则根据全概率公式，有
$$
\begin{array}{c}{\sum_{i=1}^{K} \phi_{i}=1} \\ {\phi_{K}=1-\sum_{i=1}^{K-1} \phi_{i}}\end{array}
$$
定义$T(y)$为一个$K-1$维的列向量：
$$
T(1)=\left[ \begin{array}{c}{1} \\ {0} \\ {0} \\ {\vdots} \\ {0}\end{array}\right], T(2)=\left[ \begin{array}{c}{0} \\ {1} \\ {0} \\ {\vdots} \\ {0}\end{array}\right], \cdots, T(K-1)=\left[ \begin{array}{c}{0} \\ {0} \\ {0} \\ {\vdots} \\ {1}\end{array}\right], T(K)=\left[ \begin{array}{c}{0} \\ {0} \\ {0} \\ {\vdots} \\ {0}\end{array}\right]
$$
定义示性函数 : $I(y=i)$表示属于$i$分类；$I(y \neq i)$表示不属于$i$分类。则有：$T(y)_{i}=I(y=i)$。构建概率密度函数为：

$$
\begin{array}{l}p(y ; \phi)=\phi_{1}^{I(y=1)} \times \phi_{2}^{I(y=2)} \times \cdots \times \phi_{K}^{I(y=K)}\\
=\phi_{1}^{T(y)_{1}} \times \phi_{2}^{T(y)_{2}} \times \cdots \times \phi_{K}^{1-\sum_{i-1}^{K-1} T(y)_{i}}\\
{=\exp \left(T(y)_{1} \times \ln \phi_{1}+T(y)_{2} \times \ln \phi_{2}+\cdots+\left(1-\sum_{i=1}^{K-1} T(y)_{i}\right) \times \ln \phi_{K}\right)} \\ {=\exp \left(T(y)_{1} \times \ln \frac{\phi_{1}}{\phi_{K}}+T(y)_{2} \times \ln \frac{\phi_{2}}{\phi_{K}}+\cdots+T(y)_{K-1} \times \ln \frac{\phi_{K-1}}{\phi_{K}}+\ln \phi_{K}\right)}\\
\text{令}：\eta=\left(\ln \frac{\phi_{1}}{\phi_{K}}, \ln \frac{\phi_{2}}{\phi_{K}}, \cdots, \ln \frac{\phi_{K-1}}{\phi_{K}}\right)^{T}\\
\text{则有}:p(y ; \phi)=\exp \left(\eta \cdot T(y)+\ln \phi_{K}\right)\\
\text{令}b(y)=1, a(\eta)=-\ln \phi_{K}，\text{则满足广义线性模型}\\
\phi_{i}=\left\{\begin{array}{ll}{\frac{e^{\eta_{i}}}{1+\sum_{j=1}^{K-1} e^{\eta_{j}}},} & {i=1,2, \cdots, K-1} \\ {\frac{1}{1+\sum_{j=1}^{K-1} e^{\eta_{j}}},} & {i=K}\end{array}\right.
\end{array}
$$

###### 伯努利分布

$y$服从伯努利分布（$y$为 0 或者 1，取 1的概率为$\phi$）`logistic` 回归属于伯努利分布的广义形式。
$$
\begin{array}{c}p(y ; \phi)=\phi^{y}(1-\phi)^{1-y}=\exp \left(y \ln \frac{\phi}{1-\phi}+\ln (1-\phi)\right)\\
b(y)=1 \\
\eta=\ln \frac{\phi}{1-\phi} \\
T(y)=y \\
a(\eta)=-\ln (1-\phi)
\end{array}
$$

##### 逻辑回归

考虑二分类问题。给定数据集
$$
\mathbb{D}=\{(\vec{\mathbf{x}}_1,\tilde{y}_1),\cdots,(\vec{\mathbf{x}}_N,\tilde{y}_N)\},\vec{\mathbf{x}}_i\in \mathcal{X},y_i\in \mathcal{Y}=\{0,1\}
$$
考虑到$\vec{\mathbf{w}}\cdot \vec{\mathbf{x}}+b$取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率$p(y=1|\vec{\mathbf{x}})$，因为概率的取值也是连续的。但是对于$\vec{\mathbf{w}}\ne \vec{\mathbf{0}}$， $\vec{\mathbf{w}}\cdot \vec{\mathbf{x}}+b$取值是$\mathbb{R}$，不符合概率取值为$[0,1]$，因此考虑采用广义线性模型。最理想的是单位阶跃函数：
$$
\begin{equation}p(y=1 | \vec{\mathbf{x}})=\left\{\begin{array}{ll}
0, & z<0 \\
0.5, & z=0 \\
1, & z>0
\end{array}, z=\vec{\mathbf{w}} \cdot \vec{\mathbf{x}}+b\right.\end{equation}
$$
但是阶跃函数不满足单调可微的性质，不能直接用作$g(\cdot)$。对数几率函数就是这样的一个替代函数：
$$
\begin{equation}p(y=1 | \vec{\mathbf{x}})=\frac{1}{1+e^{-z}} \quad, z=\vec{\mathbf{w}} \cdot \vec{\mathbf{x}}+b\end{equation}
$$
这样的模型称作对数几率回归模型。由于$p(y=0|\vec{\mathbf{x}})=1-p(y=1|\vec{\mathbf{x}})$，则有：
$$
\begin{equation}\ln \frac{P(y=1 | \vec{\mathbf{x}})}{P(y=0 | \vec{\mathbf{x}})}=z=\vec{\mathbf{w}} \cdot \vec{\mathbf{x}}+b\end{equation}
$$
其优点：直接对分类的可能性进行建模，无需事先假设数据分布，这就避免了因为假设分布不准确带来的问题；不仅预测出来类别，还得到了近似概率的预测，这对许多需要利用概率辅助决策的任务有用；对数函数是任意阶可导的凸函数，有很好的数学性质，很多数值优化算法都能直接用于求取最优解。

###### 参数估值

给定数据集
$$
\mathbb{D}=\{(\vec{\mathbf{x}}_1,\tilde{y}_1),\cdots,(\vec{\mathbf{x}}_N,\tilde{y}_N)\},\vec{\mathbf{x}}_i\in \mathbb{R}^n,y_i\in \{0,1\}
$$
可以用极大似然估计法估计模型参数，为了便于讨论，将参数$b$吸收进$\vec{\mathbf{x}}$中。从而得出模型。
$$
\begin{array}{c}P(y=1 | \vec{\mathbf{x}})=\pi(\vec{\mathbf{x}})=\frac{\exp(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}})}{1+\exp(\mathbf{w}\cdot\vec{\mathbf{x}})}\\
P(y=0 | \vec{\mathbf{x}})=1-\pi(\vec{\mathbf{x}})\end{array}
$$
则似然函数为：$\prod_{i=1}^N[\pi(\vec{\mathbf{x}}_i)]^{\tilde{y}_i}[1-\pi(\vec{\mathbf{x}})]^{1-\tilde{y}_i}$。

对数似然函数为：
$$
\begin{array}{c}L(\vec{\mathbf{w}})=\sum_{i=1}^N\left[\tilde{y}_i\log\frac{\pi(\vec{\mathbf{x}})}{1-\pi(\vec{\mathbf{x}})}+\log(1-\pi(\vec{\mathbf{x}})\right]\\
L(\vec{\mathbf{w}})=\sum_{i=1}^N\left[\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i)-\log(1+\exp(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i)))\right]\end{array}
$$
则需要求解最优化问题：$\vec{\mathbf{w}}^*=\arg\max_{\vec{\mathbf{w}}}L(\vec{\mathbf{w}})$。`logistic` 回归的最优化问题，通常用梯度下降法或者拟牛顿法来求解。

##### 线性判别分析

线性判别分析基本思想：

- 训练时给定训练样本集，设法将样例投影到某一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离。要学习的就是这样的一条直线。
- 预测时对新样本进行分类时，将其投影到学到的直线上，在根据投影点的位置来确定新样本的类别。

考虑二类分类问题。设数据集为：$\mathbb{D}=\left\{\left(\vec{\mathbf{x}}_{1}, \tilde{y}_{1}\right),\left(\vec{\mathbf{x}}_{2}, \tilde{y}_{2}\right), \cdots,\left(\vec{\mathbf{x}}_{N}, \tilde{y}_{N}\right)\right\},\vec{\mathbf{x}}_{i}\in \mathcal{X}, y_i\in \mathcal{Y}=\{0,1\})$

设$\mathbb{D}_0$表示类别为 `0` 的样例的集合，这些样例的均值向量为$\vec{\mu}_{0}=(\mu_{0,1},\cdots,\mu_{0,n})^T$，这些样例的特征之间协方差矩阵为$\Sigma_0$。设$\mathbb{D}_1$表示类别为 `1` 的样例的集合，这些样例的均值向量为$\vec{\mu}_{1}=(\mu_{1,1},\cdots,\mu_{1,n})^T$，这些样例的特征之间协方差矩阵为$\Sigma_1$。假定直线为：$y=\vec{\mathbf{w}}^{T} \vec{\mathbf{x}}$，其中$\vec{\mathbf{x}}=(\omega_1,\omega_2,\cdots,\omega_n)^T,\vec{\mathbf{x}}=(x_1,x_2,\cdots,x_n)^T$。这里省略了常量$b$，因为考察的是样本点在直线上的投影，总可以平行移动直线到原点而保持投影不变，此时$b=0$。

将数据投影到直线上，则：两类样本的中心在直线上的投影分别为$\vec{\mathbf{w}}^{T} \vec{\mu}_{0}$和$\vec{\mathbf{w}}^{T} \vec{\mu}_{1}$。两类样本投影的方差分别为$\vec{\mathbf{w}}^{T} \Sigma_{0}\mathbf{w}$和$\vec{\mathbf{w}}^{T} \Sigma_{1}\mathbf{w}$

根据线性判别分析的思想：要使得同类样例的投影点尽可能接近，则可以使同类样例投影点的方差尽可能小，即$\mathbf{w}^{T} \Sigma_{0} \vec{\mathbf{w}}+\vec{\mathbf{w}}^{T} \Sigma_{1}\mathbf{w}$尽可能小。要使异类样例的投影点尽可能远，则可以使异类样例的中心的投影点尽可能远，即$\|\vec{\mathbf{w}}^{T} \vec{\mu}_{0}-\vec{\mathbf{w}}^{T} \vec{\mu}_{1}\|_{2}^{2}$尽可能大。同时考虑两者，则得到最大化的目标：
$$
\begin{equation}J=\frac{\left\|\vec{\mathbf{w}}^{T} \vec{\mu}_{0}-\vec{\mathbf{w}}^{T} \vec{\mu}_{1}\right\|_{2}^{2}}{\vec{\mathbf{w}}^{T} \Sigma_{0} \vec{\mathbf{w}}+\vec{\mathbf{w}}^{T} \Sigma_{1} \vec{\mathbf{w}}}=\frac{\vec{\mathbf{w}}^{T}\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)^{T} \vec{\mathbf{w}}}{\vec{\mathbf{w}}^{T}\left(\Sigma_{0}+\Sigma_{1}\right) \vec{\mathbf{w}}}\end{equation}
$$


定义类内散度矩阵：
$$
\mathbf{S}_{w}=\Sigma_{0}+\Sigma_{1}=\sum_{\vec{\mathbf{x}} \in \mathbb{D}_{0}}\left(\vec{\mathbf{x}}-\vec{\mu}_{0}\right)\left(\vec{\mathbf{x}}-\vec{\mu}_{0}\right)^{T}+\sum_{\vec{\mathbf{x}} \in \mathbb{D}_{1}}\left(\vec{\mathbf{x}}-\vec{\mu}_{1}\right)\left(\vec{\mathbf{x}}-\vec{\mu}_{1}\right)^{T}
$$
类间散度矩阵：$\mathbf{S}_{b}=\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)^{T}$。

利用类内散度矩阵和类间散度矩阵，线性判别分析的最优化目标为：$J=\frac{\vec{w}^{T} \mathbf{S}_{b} \vec{\mathbf{w}}}{\vec{\mathbf{w}}^{T} \mathbf{S}_{w} \vec{\mathbf{w}}}$。现在求解最优化问题：$\vec{\mathbf{w}}^{*}=\arg \max _{\vec{\mathbf{w}}} \frac{\vec{\mathbf{w}}^{T} \mathbf{S}_{b} \vec{\mathbf{w}}}{\vec{\mathbf{w}}^{T} \mathbf{S}_{w} \vec{\mathbf{w}}}$。考虑到分子与分母都是关于$\vec{\mathbf{w}}$的二次项，因此上式的解与$\vec{\mathbf{w}}$的长度无关，只与$\vec{\mathbf{w}}$的方向有关。令$\vec{\mathbf{w}}^{T} \mathbf{S}_{w} \vec{\mathbf{w}}=1$，则最优化问题改写为：
$$
\begin{array}{c}{\vec{\mathbf{w}}^{*}=\arg \min _{\vec{\mathbf{w}}}-\vec{\mathbf{w}}^{T} \mathbf{S}_{b} \vec{\mathbf{w}}} \\ {s . t . \vec{\mathbf{w}}^{T} \mathbf{S}_{w} \vec{\mathbf{w}}=1}\end{array}
$$
应用拉格朗日乘子法，上式等价于$\mathbf{S}_{b} \vec{\mathbf{w}}=\lambda \mathbf{S}_{w} \vec{\mathbf{w}}$。令$\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)^{T} \vec{\mathbf{w}}=\lambda_{\vec{\mathbf{w}}}$，其中$\lambda_{\vec{\mathbf{w}}}$为实数。则$\mathbf{S}_{b} \vec{\mathbf{w}}=\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)^{T} \vec{\mathbf{w}}=\lambda_{\vec{w}}\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)$。代入上式有：$\mathbf{S}_{b} \vec{\mathbf{w}}=\lambda_{\vec{\mathbf{w}}}\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)=\lambda \mathbf{S}_{w} \vec{\mathbf{w}}$

###### 多分类模型

与二分类线性判别分析不同，在多分类线性判别分析中投影方向是多维的，因此使用投影矩阵$\mathbf{W}$。二分类线性判别分析的投影方向是一维的，所以使用投影向量$\vec{\mathbf{w}}$。

上述最优化问题可以通过广义特征值问题求解：$\mathbf{S}_b\mathbf{W} = \lambda\mathbf{S}_w\mathbf{W}$ 

-  $\mathbf{W}$的解析解为$\mathbf{S}_w^{-1}\mathbf{S}_b$的$M-1$个最大广义特征值所对应的特征向量组成的矩阵。
-  多分类线性判别分析将样本投影到$M-1$维空间。
-  通常$M-1$远小于数据原有的特征数，`LDA`因此也被视作一种经典的监督降维技术。



