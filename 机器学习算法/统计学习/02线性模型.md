给定样本$\vec{\mathbf{x}}$，其中$\vec{\mathbf{x}}=\{x_1,\cdots,x_n\}^T$，$x_i$为样本$\vec{\mathbf{x}}$的第$i$个特征。线性模型的形式为：
$$
f(\vec{\mathbf{x}})=\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}+b
$$
其中$\vec{\mathbf{w}}$为每个特征对应的权重生成的权重向量。线性模型的优点是：模型简单；可解释性强，权重向量$\vec{\mathbf{w}}$直观地表达了各个特征在预测中的重要性。

##### 线性回归

给定数据集$\mathbb{D}=\{(\vec{\mathbf{x}}_1,\tilde{y}_1),\cdots,(\vec{\mathbf{x}}_N,\tilde{y}_N)\}$，其中$\vec{\mathbf{x}}_i=(x_{i,1},\cdots,x_{i,n})^T\in\mathcal{X},y_i\in \mathcal{Y}$

线性回归问题试图学习模型：$f(\vec{\mathbf{x}})=\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}+b$

对于每个$\vec{\mathbf{x}}_i$，其预测值为$\hat{y}_i=f(\vec{\mathbf{x}}_i)=\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b$。采用平方损失函数，则在训练集 上，模型的损失函数为：
$$
L(f)=\sum_{i=1}^N(\hat{y}_i-\tilde{y}_i)^2=\sum_{i=1}^N(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b-\tilde{y}_i)^2
$$
优化目标是损失函数最小化，即：
$$
(\vec{\mathbf{w}}^*,b^*)=\arg\min_{\vec{\mathbf{w}},b}\sum_{i=1}^N(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i+b-\tilde{y}_i)^2
$$
可以用梯度下降法来求解上述最优化问题的数值解，但是实际上该最优化问题可以通过最小二乘法获得解析解

当$\mathbf{X}^T\mathbf{X}$为满秩矩阵时，可得：$\vec{\mathbf{w}}^*=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\vec{\mathbf{y}}$。当$\mathbf{X}^T\mathbf{X}$不是满秩矩阵。此时存在多个解析解，他们都能使得均方误差最小化。究竟选择哪个解作为输出，由算法的偏好决定。

##### 广义线性模型

考虑单调可微函数$g(\cdot)$，令$g(y)=\vec{\mathbf{w}}^T\vec{\mathbf{x}}+b$，这样得到的模型称作广义线性模型。其中函数$g(\cdot)$称作联系函数。

如果给定$\vec{\mathbf{x}}$和$\vec{\mathbf{w}}$之后，$y$的条件概率分布$p(y | \vec{\mathbf{x}} ; \vec{\mathbf{w}})$服从指数分布族，则该模型称作广义线性模型。指数分布族的形式为：$p(y ; \eta)=b(y) * \exp (\eta T(y)-a(\eta))$。$\eta$是$\vec{\mathbf{x}}$的线性函数：$\eta=\vec{\mathbf{w}}^{T} \vec{\mathbf{x}}$ 。$b(y),T(y)$为$y$的函数。$\alpha(\eta)$为$\eta$的函数.

###### 高斯分布

假设$y$服从高斯正态分布，则可以写成如下的指数分布族形式：
$$
\begin{equation}p(y)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(y-\mu)^{2}}{2 \sigma^{2}}\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{y^{2}}{2 \sigma^{2}}\right) \exp \left(\frac{\mu}{\sigma^{2}} \times y-\frac{\mu^{2}}{2 \sigma^{2}}\right) \\
\begin{array}{c}
b(y)=\frac{1}{\sqrt{2 \pi} \sigma} \times \exp \left(-\frac{y^{2}}{2 \sigma^{2}}\right) \\
T(y)=y \\
\eta=\frac{\mu}{\sigma^{2}} \\
a(\eta)=\frac{\mu^{2}}{2 \sigma^{2}}
\end{array}\end{equation}
$$

###### 多元伯努利分布

如果$y$服从多元伯努利分布，假设有$K$个分类，样本标记$\tilde{y} \in\{1,2, \cdots, K\}$。每种分类对应的概率为$\phi_{1}, \phi_{2}, \cdots, \phi_{K}$。则根据全概率公式，有
$$
\begin{array}{c}{\sum_{i=1}^{K} \phi_{i}=1} \\ {\phi_{K}=1-\sum_{i=1}^{K-1} \phi_{i}}\end{array}
$$
定义$T(y)$为一个$K-1$维的列向量：
$$
T(1)=\left[ \begin{array}{c}{1} \\ {0} \\ {0} \\ {\vdots} \\ {0}\end{array}\right], T(2)=\left[ \begin{array}{c}{0} \\ {1} \\ {0} \\ {\vdots} \\ {0}\end{array}\right], \cdots, T(K-1)=\left[ \begin{array}{c}{0} \\ {0} \\ {0} \\ {\vdots} \\ {1}\end{array}\right], T(K)=\left[ \begin{array}{c}{0} \\ {0} \\ {0} \\ {\vdots} \\ {0}\end{array}\right]
$$
定义示性函数 : $I(y=i)$表示属于$i$分类；$I(y \neq i)$表示不属于$i$分类。则有：$T(y)_{i}=I(y=i)$。构建概率密度函数为：

$$
\begin{array}{l}p(y ; \phi)=\phi_{1}^{I(y=1)} \times \phi_{2}^{I(y=2)} \times \cdots \times \phi_{K}^{I(y=K)}\\
=\phi_{1}^{T(y)_{1}} \times \phi_{2}^{T(y)_{2}} \times \cdots \times \phi_{K}^{1-\sum_{i-1}^{K-1} T(y)_{i}}\\
{=\exp \left(T(y)_{1} \times \ln \phi_{1}+T(y)_{2} \times \ln \phi_{2}+\cdots+\left(1-\sum_{i=1}^{K-1} T(y)_{i}\right) \times \ln \phi_{K}\right)} \\ {=\exp \left(T(y)_{1} \times \ln \frac{\phi_{1}}{\phi_{K}}+T(y)_{2} \times \ln \frac{\phi_{2}}{\phi_{K}}+\cdots+T(y)_{K-1} \times \ln \frac{\phi_{K-1}}{\phi_{K}}+\ln \phi_{K}\right)}\\
\text{令}：\eta=\left(\ln \frac{\phi_{1}}{\phi_{K}}, \ln \frac{\phi_{2}}{\phi_{K}}, \cdots, \ln \frac{\phi_{K-1}}{\phi_{K}}\right)^{T}\\
\text{则有}:p(y ; \phi)=\exp \left(\eta \cdot T(y)+\ln \phi_{K}\right)\\
\text{令}b(y)=1, a(\eta)=-\ln \phi_{K}，\text{则满足广义线性模型}\\
\phi_{i}=\left\{\begin{array}{ll}{\frac{e^{\eta_{i}}}{1+\sum_{j=1}^{K-1} e^{\eta_{j}}},} & {i=1,2, \cdots, K-1} \\ {\frac{1}{1+\sum_{j=1}^{K-1} e^{\eta_{j}}},} & {i=K}\end{array}\right.
\end{array}
$$

###### 伯努利分布

$y$服从伯努利分布（$y$为 0 或者 1，取 1的概率为$\phi$）`logistic` 回归属于伯努利分布的广义形式。
$$
\begin{equation}p(y ; \phi)=\phi^{y}(1-\phi)^{1-y}=\exp \left(y \ln \frac{\phi}{1-\phi}+\ln (1-\phi)\right)\\
\begin{array}{c}
b(y)=1 \\
\eta=\ln \frac{\phi}{1-\phi} \\
T(y)=y \\
a(\eta)=-\ln (1-\phi)
\end{array}\end{equation}
$$

##### 逻辑回归

考虑二分类问题。给定数据集
$$
\mathbb{D}=\{(\vec{\mathbf{x}}_1,\tilde{y}_1),\cdots,(\vec{\mathbf{x}}_N,\tilde{y}_N)\},\vec{\mathbf{x}}_i\in \mathcal{X},y_i\in \mathcal{Y}=\{0,1\}
$$
考虑到$\vec{\mathbf{w}}\cdot \vec{\mathbf{x}}+b$取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率$p(y=1|\vec{\mathbf{x}})$，因为概率的取值也是连续的。但是对于$\vec{\mathbf{w}}\ne \vec{\mathbf{0}}$， $\vec{\mathbf{w}}\cdot \vec{\mathbf{x}}+b$取值是$\mathbb{R}$，不符合概率取值为$[0,1]$，因此考虑采用广义线性模型。最理想的是单位阶跃函数：
$$
\begin{equation}p(y=1 | \overrightarrow{\mathbf{x}})=\left\{\begin{array}{ll}
0, & z<0 \\
0.5, & z=0 \\
1, & z>0
\end{array}, z=\overrightarrow{\mathbf{w}} \cdot \overrightarrow{\mathbf{x}}+b\right.\end{equation}
$$
但是阶跃函数不满足单调可微的性质，不能直接用作$g(\cdot)$。对数几率函数就是这样的一个替代函数：
$$
\begin{equation}p(y=1 | \overrightarrow{\mathbf{x}})=\frac{1}{1+e^{-z}} \quad, z=\overrightarrow{\mathbf{w}} \cdot \overrightarrow{\mathbf{x}}+b\end{equation}
$$
这样的模型称作对数几率回归模型。由于$p(y=0|\vec{\mathbf{x}})=1-p(y=1|\vec{\mathbf{x}})$，则有：
$$
\begin{equation}\ln \frac{P(y=1 | \overrightarrow{\mathbf{x}})}{P(y=0 | \overrightarrow{\mathbf{x}})}=z=\overrightarrow{\mathbf{w}} \cdot \overrightarrow{\mathbf{x}}+b\end{equation}
$$
其优点：直接对分类的可能性进行建模，无需事先假设数据分布，这就避免了因为假设分布不准确带来的问题；不仅预测出来类别，还得到了近似概率的预测，这对许多需要利用概率辅助决策的任务有用；对数函数是任意阶可导的凸函数，有很好的数学性质，很多数值优化算法都能直接用于求取最优解。

###### 参数估值

给定数据集
$$
\mathbb{D}=\{(\vec{\mathbf{x}}_1,\tilde{y}_1),\cdots,(\vec{\mathbf{x}}_N,\tilde{y}_N)\},\vec{\mathbf{x}}_i\in \mathbb{R}^n,y_i\in \{0,1\}
$$
可以用极大似然估计法估计模型参数，为了便于讨论，将参数$b$吸收进$\vec{\mathbf{x}}$中。从而得出模型。
$$
P(y=1 | \vec{\mathbf{x}})=\pi(\vec{\mathbf{x}})=\frac{\exp(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}})}{1+\exp(\mathbf{w}\cdot\vec{\mathbf{x}})}\\
P(y=0 | \vec{\mathbf{x}})=1-\pi(\vec{\mathbf{x}})
$$
则似然函数为：$\prod_{i=1}^N[\pi(\vec{\mathbf{x}}_i)]^{\tilde{y}_i}[1-\pi(\vec{\mathbf{x}})]^{1-\tilde{y}_i}$。

对数似然函数为：
$$
L(\vec{\mathbf{w}})=\sum_{i=1}^N\left[\tilde{y}_i\log\frac{\pi(\vec{\mathbf{x}})}{1-\pi(\vec{\mathbf{x}})}+\log(1-\pi(\vec{\mathbf{x}})\right]\\
L(\vec{\mathbf{w}})=\sum_{i=1}^N\left[\tilde{y}_i(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i)-\log(1+\exp(\vec{\mathbf{w}}\cdot\vec{\mathbf{x}}_i)))\right]
$$
则需要求解最优化问题：$\vec{\mathbf{w}}^*=\arg\max_{\vec{\mathbf{w}}}L(\vec{\mathbf{w}})$。`logistic` 回归的最优化问题，通常用梯度下降法或者拟牛顿法来求解。

##### 线性判别分析

线性判别分析基本思想：

- 训练时给定训练样本集，设法将样例投影到某一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离。要学习的就是这样的一条直线。
- 预测时对新样本进行分类时，将其投影到学到的直线上，在根据投影点的位置来确定新样本的类别。

考虑二类分类问题。设数据集为：$\mathbb{D}=\left\{\left(\vec{\mathbf{x}}_{1}, \tilde{y}_{1}\right),\left(\vec{\mathbf{x}}_{2}, \tilde{y}_{2}\right), \cdots,\left(\vec{\mathbf{x}}_{N}, \tilde{y}_{N}\right)\right\},\vec{\mathbf{x}}_{i}\in \mathcal{X}, y_i\in \mathcal{Y}=\{0,1\})$

设$\mathbb{D}_0$表示类别为 `0` 的样例的集合，这些样例的均值向量为$\vec{\mu}_{0}=(\mu_{0,1},\cdots,\mu_{0,n})^T$，这些样例的特征之间协方差矩阵为$\Sigma_0$。设$\mathbb{D}_1$表示类别为 `1` 的样例的集合，这些样例的均值向量为$\vec{\mu}_{1}=(\mu_{1,1},\cdots,\mu_{1,n})^T$，这些样例的特征之间协方差矩阵为$\Sigma_1$。假定直线为：$y=\overrightarrow{\mathbf{w}}^{T} \vec{\mathbf{x}}$，其中$\vec{\mathbf{x}}=(\omega_1,\omega_2,\cdots,\omega_n)^T,\vec{\mathbf{x}}=(x_1,x_2,\cdots,x_n)^T$。这里省略了常量$b$，因为考察的是样本点在直线上的投影，总可以平行移动直线到原点而保持投影不变，此时$b=0$。

将数据投影到直线上，则：两类样本的中心在直线上的投影分别为$\overrightarrow{\mathbf{w}}^{T} \vec{\mu}_{0}$和$\overrightarrow{\mathbf{w}}^{T} \vec{\mu}_{1}$。两类样本投影的方差分别为$\overrightarrow{\mathbf{w}}^{T} \Sigma_{0}\mathbf{w}$和$\overrightarrow{\mathbf{w}}^{T} \Sigma_{1}\mathbf{w}$

根据线性判别分析的思想：要使得同类样例的投影点尽可能接近，则可以使同类样例投影点的方差尽可能小，即$\mathbf{w}^{T} \Sigma_{0} \overrightarrow{\mathbf{w}}+\overrightarrow{\mathbf{w}}^{T} \Sigma_{1}\mathbf{w}$尽可能小。要使异类样例的投影点尽可能远，则可以使异类样例的中心的投影点尽可能远，即$\|\overrightarrow{\mathbf{w}}^{T} \vec{\mu}_{0}-\overrightarrow{\mathbf{w}}^{T} \vec{\mu}_{1}\|_{2}^{2}$尽可能大。同时考虑两者，则得到最大化的目标：
$$
\begin{equation}J=\frac{\left\|\overrightarrow{\mathbf{w}}^{T} \vec{\mu}_{0}-\overrightarrow{\mathbf{w}}^{T} \vec{\mu}_{1}\right\|_{2}^{2}}{\overrightarrow{\mathbf{w}}^{T} \Sigma_{0} \overrightarrow{\mathbf{w}}+\overrightarrow{\mathbf{w}}^{T} \Sigma_{1} \overrightarrow{\mathbf{w}}}=\frac{\overrightarrow{\mathbf{w}}^{T}\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)^{T} \overrightarrow{\mathbf{w}}}{\overrightarrow{\mathbf{w}}^{T}\left(\Sigma_{0}+\Sigma_{1}\right) \overrightarrow{\mathbf{w}}}\end{equation}
$$


定义类内散度矩阵：
$$
\mathbf{S}_{w}=\Sigma_{0}+\Sigma_{1}=\sum_{\overrightarrow{\mathbf{x}} \in \mathbb{D}_{0}}\left(\overrightarrow{\mathbf{x}}-\vec{\mu}_{0}\right)\left(\overrightarrow{\mathbf{x}}-\vec{\mu}_{0}\right)^{T}+\sum_{\overrightarrow{\mathbf{x}} \in \mathbb{D}_{1}}\left(\overrightarrow{\mathbf{x}}-\vec{\mu}_{1}\right)\left(\overrightarrow{\mathbf{x}}-\vec{\mu}_{1}\right)^{T}
$$
类间散度矩阵：$\mathbf{S}_{b}=\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)^{T}$。

利用类内散度矩阵和类间散度矩阵，线性判别分析的最优化目标为：$J=\frac{\vec{w}^{T} \mathbf{S}_{b} \overrightarrow{\mathbf{w}}}{\overrightarrow{\mathbf{w}}^{T} \mathbf{S}_{w} \overrightarrow{\mathbf{w}}}$。现在求解最优化问题：$\overrightarrow{\mathbf{w}}^{*}=\arg \max _{\overrightarrow{\mathbf{w}}} \frac{\overrightarrow{\mathbf{w}}^{T} \mathbf{S}_{b} \overrightarrow{\mathbf{w}}}{\overrightarrow{\mathbf{w}}^{T} \mathbf{S}_{w} \overrightarrow{\mathbf{w}}}$。考虑到分子与分母都是关于$\overrightarrow{\mathbf{w}}$的二次项，因此上式的解与$\overrightarrow{\mathbf{w}}$的长度无关，只与$\overrightarrow{\mathbf{w}}$的方向有关。令$\overrightarrow{\mathbf{w}}^{T} \mathbf{S}_{w} \overrightarrow{\mathbf{w}}=1$，则最优化问题改写为：
$$
\begin{array}{c}{\overrightarrow{\mathbf{w}}^{*}=\arg \min _{\overrightarrow{\mathbf{w}}}-\overrightarrow{\mathbf{w}}^{T} \mathbf{S}_{b} \overrightarrow{\mathbf{w}}} \\ {s . t . \overrightarrow{\mathbf{w}}^{T} \mathbf{S}_{w} \overrightarrow{\mathbf{w}}=1}\end{array}
$$
应用拉格朗日乘子法，上式等价于$\mathbf{S}_{b} \overrightarrow{\mathbf{w}}=\lambda \mathbf{S}_{w} \overrightarrow{\mathbf{w}}$。令$\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)^{T} \overrightarrow{\mathbf{w}}=\lambda_{\overrightarrow{\mathbf{w}}}$，其中$\lambda_{\overrightarrow{\mathbf{w}}}$为实数。则$\mathbf{S}_{b} \overrightarrow{\mathbf{w}}=\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)^{T} \overrightarrow{\mathbf{w}}=\lambda_{\vec{w}}\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)$。代入上式有：$\mathbf{S}_{b} \overrightarrow{\mathbf{w}}=\lambda_{\overrightarrow{\mathbf{w}}}\left(\vec{\mu}_{0}-\vec{\mu}_{1}\right)=\lambda \mathbf{S}_{w} \overrightarrow{\mathbf{w}}$

###### 多分类模型

与二分类线性判别分析不同，在多分类线性判别分析中投影方向是多维的，因此使用投影矩阵$\mathbf{W}$。二分类线性判别分析的投影方向是一维的，所以使用投影向量$\vec{\mathbf{w}}$。

上述最优化问题可以通过广义特征值问题求解：$\mathbf{S}_b\mathbf{W} = \lambda\mathbf{S}_w\mathbf{W}$ 

-  $\mathbf{W}$的解析解为$\mathbf{S}_w^{-1}\mathbf{S}_b$的$M-1$个最大广义特征值所对应的特征向量组成的矩阵。
-  多分类线性判别分析将样本投影到$M-1$维空间。
-  通常$M-1$远小于数据原有的特征数，`LDA`因此也被视作一种经典的监督降维技术。



##### 参数范数正则化

一些正则化方法通过对目标函数$J$添加一个参数范数正则化项$\Omega(\vec{\theta})$来限制模型的容量。正则化之后的目标函数为$\tilde{J} : \tilde{J}(\vec{\theta} ; \mathbf{X}, \overrightarrow{\mathbf{y}})=J(\vec{\theta} ; \mathbf{X}, \overrightarrow{\mathbf{y}})+\alpha \Omega(\vec{\theta})$。

$\alpha\in [0,\infin)$为正则化项的系数，它衡量正则化项$\Omega(\vec{\theta})$和标准目标函数$J$的比重。参数范数正则化可以缓解过拟合。如果$\alpha$设置的足够大，则参数$\vec{\theta}$就越接近零。这意味着模型变得更简单，简单的模型不容易过拟合。

###### $\text{L2}$正则化

$\text{L}_2$正则化通常被称作岭回归，正则化项为$\Omega(\vec{\theta})=\frac{1}{2}||\vec{\theta}||_2^2$。该正则化形式倾向于使得参数$\vec{\theta}$更接近零。假设$\vec{\theta}$参数就是权重$\vec{\mathbf{w}}$，没有偏置参数，则：
$$
\tilde{J}(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})=J(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})+\frac{\alpha}{2} \overrightarrow{\mathbf{w}}^{T} \overrightarrow{\mathbf{w}}
$$
对应的梯度为：$\nabla_{\vec{w}} \tilde{J}(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})=\nabla_{\vec{w}} J(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})+\alpha \overrightarrow{\mathbf{w}}$。

使用梯度下降法来更新权重，则权重的更新公式为：$\overrightarrow{\mathbf{w}}\leftarrow\overrightarrow{\mathbf{w}}-\epsilon\left(\nabla_{\overrightarrow{\mathbf{w}}} J(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})+\alpha \overrightarrow{\mathbf{w}}\right)$。

即：$\overrightarrow{\mathbf{w}} \leftarrow(1-\epsilon \alpha) \overrightarrow{\mathbf{w}}-\epsilon \nabla_{\overrightarrow{\mathbf{w}}} J(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})$。

$\text{L2}$正则化对于梯度更新的影响是：每一步执行梯度更新之前，会对权重向量乘以一个常数因子来收缩权重向量。因此`L2` 正则化也被称作权重衰减。

令$\overrightarrow{\mathbf{w}}^{*}=\arg \min _{\overrightarrow{\mathbf{w}}} J(\overrightarrow{\mathbf{w}})$，它就是无正则化项时使得目标函数最小的权重向量。根据极小值的条件，有$\nabla_{\overrightarrow{\mathbf{w}}} J\left(\overrightarrow{\mathbf{w}}^{*}\right)=\overrightarrow{\mathbf{0}}$。于是在$\overrightarrow{\mathbf{w}}^{*}$的邻域内泰勒展开$J(\overrightarrow{\mathbf{w}})$。
$$
\hat{J}(\overrightarrow{\mathbf{w}})=J\left(\overrightarrow{\mathbf{w}}^{*}\right)+\overrightarrow{\mathbf{0}}+\frac{1}{2}\left(\overrightarrow{\mathbf{w}}-\overrightarrow{\mathbf{w}}^{*}\right)^{T} \mathbf{H}\left(\overrightarrow{\mathbf{w}}-\overrightarrow{\mathbf{w}}^{*}\right), \quad \overrightarrow{\mathbf{w}} \in \mathbb{N}\left(\overrightarrow{\mathbf{w}}^{*}\right)
$$
则$J(\overrightarrow{\mathbf{w}})$的梯度为：$\nabla_{\overrightarrow{\mathbf{w}}} \hat{J}(\overrightarrow{\mathbf{w}})=\mathbf{H}\left(\overrightarrow{\mathbf{w}}-\overrightarrow{\mathbf{w}}^{*}\right), \quad \overrightarrow{\mathbf{w}} \in \mathbb{N}\left(\overrightarrow{\mathbf{w}}^{*}\right)$。

令$\overrightarrow{\overline{\mathbf{w}}}^{*}=\arg \min _{\overrightarrow{\mathbf{w}}} \tilde{J}(\overrightarrow{\mathbf{w}})$，它就是有正则化项时使得目标函数最小的权重向量。

假设$\tilde{\mathbf{w}}^{*} \in \mathbb{N}\left(\overrightarrow{\mathbf{w}}^{*}\right)$， 即$\tilde{\mathbf{w}}^{*} $在$\overrightarrow{\mathbf{w}}^{*}$的一个邻域内，则有：$\nabla_{\vec{w}} J\left(\tilde{\vec{\mathbf{w}}}^{*}\right)=\mathbf{H}\left(\hat{\vec{\mathbf{w}}}^{*}-\overrightarrow{\mathbf{w}}^{*}\right)$。

根据极小值条件，则有：$\mathbf{H}\left(\overrightarrow{\mathbf{w}}^{*}-\overrightarrow{\mathbf{w}}^{*}\right)+\alpha \overrightarrow{\mathbf{w}}^{*}=\overrightarrow{\mathbf{0}} \rightarrow(\mathbf{H}+\alpha \mathbf{I}) \overrightarrow{\mathbf{w}}^{*}=\mathbf{H} \overrightarrow{\mathbf{w}}^{*}$

$\text{L}_2$正则化对模型整体的影响：沿着$\mathbf{H}$的特征向量所定义的轴来缩放$\vec{\mathbf{w}}^{*}$。

-  的第  个特征向量对应的    分量根据  因子缩放。
- 沿着  特征值较大的方向受到正则化的影响较小。
- 当  的方向对应的权重分量将被缩小到几乎为零。



###### $\text{L1}$正则化

模型参数$\vec{\mathbf{w}}$的$\mathbf{L}_1$的正则化形式为：$\Omega(\vec{\theta})=\|\overrightarrow{\mathbf{w}}\|_{1}=\sum_{i}\left|w_{i}\right|$。即各个参数的绝对值之和。

$\mathbf{L}_1$正则化后的目标函数$\tilde{J}(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}}) : \tilde{J}(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})=J(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})+\alpha\|\overrightarrow{\mathbf{w}}\|_{1}$。

对应的梯度为$\nabla_{\overrightarrow{\mathbf{w}}} \tilde{J}(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})=\nabla_{\overrightarrow{\mathbf{w}}} J(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})+\alpha \operatorname{sign}(\overrightarrow{\mathbf{w}})$。如果自变量大于零，则取值为 1；如果自变量小于零，则取值为 -1；如果自变量为零，则取值为零。使用梯度下降法来更新权重，给出权重的更新公式为：
$$
\begin{array}{l}{\overrightarrow{\mathbf{w}} \leftarrow \overrightarrow{\mathbf{w}}-\epsilon\left(\nabla_{\overrightarrow{\mathbf{w}}} J(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})+\alpha \operatorname{sign}(\overrightarrow{\mathbf{w}})\right)} \\ {=(\overrightarrow{\mathbf{w}}-\epsilon \alpha \operatorname{sign}(\overrightarrow{\mathbf{w}}))-\epsilon \nabla_{\overrightarrow{\mathbf{w}}} J(\overrightarrow{\mathbf{w}} ; \mathbf{X}, \overrightarrow{\mathbf{y}})}\end{array}
$$

$\text{L}_1$正则化对于梯度更新的影响是：不再是线性地缩放每个$\omega_i$ （$\text{L}_2$正则化项的效果），而是减去与$\text{sign}(\omega_i)$同号的常数因子。

##### 显式约束正则化

可以通过添加一个显式约束来实现正则化：$\min _{\vec{\theta}} J(\vec{\theta} ; \mathbf{X}, \overrightarrow{\mathbf{y}}), \quad$ st. $\Omega(\vec{\theta})<k$。其中$k$为一个常数。可以通过构建广义拉格朗日函数来求解该约束最优化问题。定义广义拉格朗日函数：$\mathcal{L}(\vec{\theta}, \alpha)=J(\vec{\theta})+\alpha(\Omega(\vec{\theta})-k)$。则上述约束最优化问题的解由下式给出：$\vec{\theta}^{*}=\arg \min _{\vec{\theta}} \max _{\alpha, \alpha>0} \mathcal{L}(\vec{\theta}, \alpha)$。假设$\alpha$的解为$\alpha^*$，固定$\alpha^*$则：$\vec{\theta}^{*}=\arg \min _{\vec{\theta}} J(\vec{\theta})+\alpha^{*} \Omega(\vec{\theta})$.