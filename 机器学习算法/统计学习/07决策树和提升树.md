### 决策树

#### 决策树原理

决策树将特征空间划分为互不相交的单元，在每个单元定义一个类的概率分布，这就构成了一个条件概率分布。决策树的每一条路径对应于划分中的一个基本单元。设某个单元$\mathbb{S}$内部有$N_S$个样本点，则它定义了一个条件概率分布$p(y|\vec{\mathbf{x}}),\vec{\mathbf{x}}\in \mathbb{S}$。单元上的条件概率偏向哪个类，则该单元划归到该类的概率较大。即单元的类别为：
$$
\text{arg}\max_{y}p(y|\vec{\mathbf{x}}),\vec{\mathbf{x}}\in \mathbb{S}
$$
决策树所代表的条件概率分布由各个单元给定条件下类的条件概率分布组成。

决策树的学习目标是：根据给定的训练数据集构造一个决策树模型，使得它能够对样本进行正确的分类。决策树最优化的策略是：损失函数最小化。决策树的损失函数通常是正则化的极大似然函数。

选择最优决策树的问题是个 `NP` 完全问题。一般采用启发式方法近似求解这个最优化问题，这时的解为次最优的。决策树学习的算法通常递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类。

##### 决策树生成

决策树生成算法：构建根结点，将所有训练数据放在根结点。

选择一个最优特征，根据这个特征将训练数据分割成子集，使得各个子集有一个在当前条件下最好的分类。

- 若这些子集已能够被基本正确分类，则将该子集构成叶结点。
- 若某个子集不能够被基本正确分类，则对该子集选择新的最优的特征，继续对该子集进行分割，构建相应的结点。

如此递归下去，直至所有训练数据子集都被基本正确分类，或者没有合适的特征为止。

上述生成的决策树可能对训练数据有很好的分类能力，但是对于未知的测试数据却未必有很好要的分类能力，即可能发生过拟合的现象。

- 解决的方法是：对生成的决策树进行剪枝，从而使得决策树具有更好的泛化能力。
- 剪枝是去掉过于细分的叶结点，使得该叶结点中的子集回退到父结点或更高层次的结点并让其成为叶结点。

决策树的生成对应着模型的局部最优，决策树的剪枝则考虑全局最优。

如果学习任意大小的决策树，则可以将决策树算法视作非参数算法。但是实践中经常有大小限制（如限制树的高度、限制树的叶结点数量），从而使得决策树成为有参数模型。

##### 特征选择

选取对训练数据有较强分类能力的特征。若一个特征的分类结果与随机分类的结果没有什么差别，则称这个特征是没有分类能力的。通常特征选择的指标是：信息增益或者信息增益比。这两个指标刻画了特征的分类能力。

对于分布$p(y)$，熵为$H(y)=\sum_y-p(y)\log p(y)$。定义数据集$\mathbb{D}$的经验熵为：
$$
H(\mathbb{D})=-\sum_{k=1}^K\frac{N_k}{N}\log\frac{N_k}{N}
$$
其中：样本的类别分别为$c_1,c_2,\cdots,c_K$。类别$c_k$的样本的数量为$N_k$，所有样本的总数为$N$。

对于特征$A$，定义数据集$\mathbb{D}$在$A$上的经验熵为：
$$
H_A(\mathbb{D})=-\sum_{i=1}^{n_A}\frac{N_{a_i}}{N}\log\frac{N_{a_i}}{N}
$$
其中：特征$A$的取值范围为$\{a_1,a_2,\cdots,a_{n_A}\}$。属性$A=a_i$的样本的数量为$N_{a_{i}}$。

对于特征$A$，其条件熵为：$H(y|A)=\sum_Ap(A)H(y|A)$

定义数据集$\mathbb{D}$关于特征$A$的经验条件熵为：
$$
H(\mathbb{D}|A)=\sum_{i=1}^{n_A}p(A=a_i)H(y|A=a_i)=\sum_{i=1}^{n_A}\frac{N_{a_i}}{N}\left[-\sum_{k=1}^K\frac{N_{a_i,k}}{N_{a_i}}\log\frac{N_{a_i,k}}{N_{a_i}}\right]
$$


其中：属性$A$且类别为$c_k$的样本的数量为$N_{a_i,k}$，所有样本的总数为$N$。

特征$A$对训练数据集$\mathbb{D}$的信息增益$g(\mathbb{D},A)$定义为：集合$\mathbb{D}$的经验熵$H(\mathbb{D})$与关于特征$A$经验条件熵$H(\mathbb{D}|A)$之差。即：$g(\mathbb{D},A)=H(\mathbb{D})-H(\mathbb{D}|A)$。

决策树学习可以应用信息增益来选择特征。给定训练集$\mathbb{D}$和特征$A$：

- 信息增益$H(\mathbb{D})-H(\mathbb{D}|A)$刻画了由于特征$A$的确定，从而使得对数据集$\mathbb{D}$的分类的不确定性减少的程度。

不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力 。如果一个特征的信息增益为0，则表示该特征没有什么分类能力。

以信息增益作为划分训练集的特征选取方案，存在偏向于选取值较多的特征的问题。

公式$H(\mathbb{D}|A)=\sum_{i=1}^{n_A}\frac{N_{a_i}}{N}\left[-\sum_{k=1}^K\frac{N_{a_i,k}}{N_{a_i}}\log\frac{N_{a_i,k}}{N_{a_i}}\right]$中：

当极限情况下 ，特征$A$在每个样本上的取值都不同，此时特征$A$将每一个样本都划分到不同的子结点。条件熵的最小值为 0，这意味着该情况下的信息增益达到了最大值。然而很显然这个特征$A$显然不是最佳选择，因为它并不具有任何分类能力。

可以通过定义信息增益比来解决该问题。特征$A$对训练集$\mathbb{D}$的信息增益比$g_R(\mathbb{D},A)$定义为：信息增益$g(\mathbb{D},A)$与关于特征$A$的熵$H_A(\mathbb{D})$之比：

$$
g_R(\mathbb{D},A)=\frac{g(\mathbb{D},A)}{H_A(\mathbb{D})}
$$
$H_A(\mathbb{D})$表征了特征$A$对训练集$\mathbb{D}$的拆分能力。信息增益比本质上是对信息增益乘以一个加权系数：当特征$A$的取值集合较大时，加权系数较小，表示抑制该特征。当特征$A$的取值集合较小时，加权系数较大，表示鼓励该特

##### 决策树生成算法

决策树有两种常用的生成算法：`ID3` 生成算法。`C4.5` 生成算法。`ID3` 生成算法和 `C4.5` 生成算法只有树的生成算法，生成的树容易产生过拟合：对训练集拟合得很好，但是预测测试集效果较差

###### `ID3`生成算法

`ID3` 生成算法核心是在决策树的每个结点上应用信息增益准则选择特征，递归地构建决策树。

- 从根结点开始，计算结点所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征划分出子结点。
- 再对子结点递归地调用以上方法，构建决策树。
- 直到所有特征的信息增益均很小或者没有特征可以选择为止，最后得到一个决策树 

如果不设置特征信息增益的下限，则可能会使得每个叶子都只有一个样本点，从而划分得太细。`C4.5` 生成算法与 `ID3` 算法相似，但是 `C4.5` 算法在生成过程中用信息增益比来选择特征。

##### 剪枝算法

设树$T$的叶结点个数为$|T_f|$。设$T_t, t=1,\cdots,|T_f|$为树的叶结点，该叶结点有$N_t$个样本点，其中属于$c_k$类的样本点有$N_{t,k}$个，$k=1,2,\cdots,K$。令$H(t)$为叶结点$T_t$上的经验熵，令$\alpha\ge0$为参数，则决策树$T$的损失函数定义为：
$$
C_{\alpha}(T)=\sum_{t=1}^{|T_f|}N_tH(t)+\alpha|T_f|
$$
其中：$H(t)=-\sum_{k=1}^K\frac{N_{t,k}}{N_t}\log\frac{N_{t,k}}{N_t}$。叶结点个数越多，表示决策树越复杂，则损失函数越大。叶结点经验熵越大，表示叶结点的样本类别分布很分散，则损失函数越大。叶结点经验熵还需要加权，权重为叶结点大小。即：越大的叶结点，其分类错误的影响越大。

令$C(T)=\sum_{t=1}^{|T_f|}N_tH(t)$，则：$C_{\alpha}(T)=C(T)+\alpha|T_f|$。其中$\alpha|T_f|$为正则化项，$C(T)$表示预测误差。$C(T)=0$意味着 ，即每个结点$T_t$内的样本都是纯的，即单一的分类。决策树划分得越细致，则$T$的叶子结点越多。当叶结点的数量等于样本集的数量时，树$T$的每个叶子结点只有一个样本点。此时每个叶结点内的样本都是纯的，从而$C(T)=0$。

参数$\alpha$控制预测误差与模型复杂度之间的关系。较大的$\alpha$会选择较简单的模型 。较小的$\alpha$会选择较复杂的模型。 $\alpha=0$只考虑对训练集的拟合，不考虑模型复杂度。

#### `CART`树

`CART` 假设决策树是二叉树：内部结点特征的取值为 `是` 与 `否` 。其中：左侧分支取 `是`，右侧分支取  `否` 。它递归地二分每个特征，将输入空间划分为有限个单元。

`CART` 树与`ID3` 决策树和 `C4.5` 决策树的重要区别：

- `CART` 树是二叉树，而后两者是`N` 叉树。
- `CART` 树的特征可以是离散的，也可以是连续的。而后两者的特征是离散的。如果是连续的特征，则需要执行分桶来进行离散化。`CART` 树处理连续特征时，也可以理解为二分桶的离散化。

`CART`算法分两步：决策树生成：用训练数据生成尽可能大的决策树。决策树剪枝：用验证数据基于损失函数最小化的标准对生成的决策树剪枝。

`CART`生成算法有两个生成准则：

- `CART` 回归树：用平方误差最小化准则。
- `CART` 分类树：用基尼指数最小化准则。

##### `CART`回归树

设输出$y$为连续变量，训练数据集$\mathbb{D}=\{(\vec{\mathbf{x}_1},\tilde{y_1}),\cdots,(\vec{\mathbf{x}_N},\tilde{y_N})\}$。

设已经将输入空间划分为$M$个单元$R_1,\cdots,R_M$，且在每个单元$R_m$上有一个固定的输出值$c_m$。则`CART` 回归树模型可以表示为：
$$
f(\vec{\mathbf{x}})=\sum_{m=1}^Mc_mI(\vec{\mathbf{x}}\in R_m)
$$
其中$I(\cdot)$为示性函数。

如果已知输入空间的单元划分，基于平方误差最小的准则，则`CART` 回归树在训练数据集上的损失函数为：
$$
\sum_{m=1}^M\sum_{\vec{\mathbf{x}_i}\in R_m}(\tilde{y_i}-c_m)^2
$$
根据损失函数最小，则可以求解出每个单元上的最优输出值$\hat{c}_m$为：$R_m$上所有输入样本$\vec{\mathbf{x}_i}$对应的输出$\tilde{y}_i$的平均值。

定义$R_m$上样本的方差为$\text{Var}_m$，则有：$\text{Var}_m=\sum_{\vec{\mathbf{x}_i}\in R_m}(\tilde{y_i}-\hat{c}_m)^2$。则`CART` 回归树的训练集损失函数重写为：$N\times\sum_{m=1}^M(\frac{N_m}{N}\times\text{Var}_m)$。

定义样本被划分到$R_m$中的概率为$P_m$，则$P_m=\frac{N_m}{N}$。由于$N$是个常数，因此损失函数重写为：
$$
\sum_{m=1}^MP_m\times\text{Var}_m
$$
问题是输入空间的单元划分是未知的。如何对输入空间进行划分？

设输入为$n$维：$\vec{\mathbf{x}}=\{x_1,\cdots,x_n\}^T$。选择第$j$维$x_j$和它的取值$s$作为切分变量和切分点。定义两个区域：
$$
R_1(j,s)=\{\vec{\mathbf{x}}:x_j\le s\}\\
R_2(j,s)=\{\vec{\mathbf{x}}:x_j>s\}
$$
然后寻求最优切分变量$j$和最优切分点$s$。即求解：
$$
(j^*,s^*)=\min_{j,s}\left[\min_{c_1}\sum_{\vec{\mathbf{x}}_i\in R_1(j,s)}(\tilde{y}_i-c_1)^2+\min_{c_2}\sum_{\vec{\mathbf{x}}_i\in R_2(j,s)}(\tilde{y}_i-c_2)^2\right]
$$
首先假设已知切分变量$j$，则遍历最优切分点$s$，则到：
$$
\hat{c}_1=\frac{1}{N_1}\sum_{\vec{\mathbf{x}}_i\in R_1(j,s)}\tilde{y}_i,\hat{c}_2=\frac{1}{N_2}\sum_{\vec{\mathbf{x}}_i\in R_2(j,s)}\tilde{y}_i
$$
然后遍历所有的特征维度，对每个维度找到最优切分点。从这些`(切分维度,最优切分点)` 中找到使得损失函数最小的那个。依次将输入空间划分为两个区域，然后重复对子区域划分，直到满足停止条件为止。这样的回归树称为最小二乘回归树。

##### `CART`分类树

`CART` 分类树采用基尼指数选择最优特征。假设有$K$个分类，样本属于第$k$类的概率为$p_k=p(y=c_k)$。则概率分布的基尼指数为：
$$
\text{Gini}(p)=\sum_{k=1}^kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
$$
基尼指数表示：样本集合中，随机选中一个样本，该样本被分错的概率。基尼指数越小，表示越不容易分错。对于给定的样本集合$\mathbb{D}$，设属于类$c_k$的样本子集为$\mathbb{D}_k$，则样本集的基尼指数为：
$$
\text{Gini}(\mathbb{D})=1-\sum_{k=1}^K\left(\frac{N_k}{N}\right)^2
$$
若样本集$\mathbb{D}$根据特征$A$是否小于$a$而被分为两个子集: $\mathbb{D}_1$和$\mathbb{D}_2$，其中：
$$
\mathbb{D}_1=\{(\vec{\mathbf{x}},y)\in \mathbb{D}|x_A\le a\}\\
\mathbb{D}_2=\{(\vec{\mathbf{x}},y)\in \mathbb{D}|x_A> a\}=\mathbb{D}-\mathbb{D}_1
$$
则在特征$A:a$的条件下，集合$\mathbb{D}$的基尼指数为：
$$
\text{Gini}(\mathbb{D},A:a)=\frac{N_1}{N}\text{Gini}(\mathbb{D}_1)+\frac{N_2}{N}\text{Gini}(\mathbb{D}_2)
$$
设输出$y$为分类的类别，是离散变量。训练数据集$\mathbb{D}=\{(\vec{\mathbf{x}_1},\tilde{y_1}),\cdots,(\vec{\mathbf{x}_N},\tilde{y_N})\}$

设已经将输入空间划分为$M$个单元$R_1,\cdots,R_M$，且在每个单元$R_m$上有一个固定的输出值$c_m$。则`CART` 分类树模型可以表示为：
$$
f(\vec{\mathbf{x}})=\sum_{m=1}^Mc_mI(\vec{\mathbf{x}}\in R_m)
$$
如果已知输入空间的单元划分，基于分类误差最小的准则，则`CART` 分类树在训练数据集上的损失函数为：
$$
\sum_{m=1}^M\sum_{\vec{\mathbf{x}_i}\in R_m}I(\tilde{y_i}\ne c_m)
$$
根据损失函数最小，则可以求解出每个单元上的最优输出值$\hat{c}_m$为：$R_m$上所有输入样本$\vec{\mathbf{x}_i}$对应的输出$\tilde{y}_i$的众数。

类似`CART` 回归树，`CART` 分类树遍历所有可能的维度$j$和该维度所有可能的取值$s$，取使得基尼系数最小的那个维度$j$和切分点$s$。即求解：
$$
(j^*,s^*)=\min_{j,s}\text{Gini}(\mathbb{D},j:s)
$$
`CART` 分类树和`CART` 回归树通常的停止条件为：结点中样本个数小于预定值，这表示树已经太复杂；样本集的损失函数或者基尼指数小于预定值，表示结点已经非常纯净；没有更多的特征可供切分。

`CART` 树的特征可以为离散值，此时切分区域定义为：
$$
R_1(j,s)=\{\vec{\mathbf{x}}:x_j= s\}\\
R_2(j,s)=\{\vec{\mathbf{x}}:x_j\ne s\}
$$
连续的特征也可以通过分桶来进行离散化，然后当作离散特征来处理。

##### 连续值处理

常用的离散化技术为二分法`bi-partition` ：给定样本集$\mathbb{D}$和连续属性$A$，假设该属性在$\mathbb{S}$中出现了$M$个不同的取值。将这些值从小到大进行排列，记作$a_1,a_2,\cdots,a_M$。选取$M-1$个划分点，依次为：$\frac{a_1+a_2}{2},\cdots,\frac{a_{M-1}+a_{M}}{2}$。然后就可以像离散属性值一样来考察这些划分点，选取最优的划分点进行样本集和的划分。这也是 `C4.5` 算法采取的方案。

##### 缺失值处理

如何在属性值缺失的情况下选择划分属性？给定划分属性，如果样本在该属性上的值缺失，则如何划分样本？

###### 划分属性选择

给定训练集$\mathbb{D}$和属性$A$， 令$\tilde{\mathbb{D}}$表示$\mathbb{D}$中在属性$A$上没有缺失的样本子集。则可以仅根据$\tilde{\mathbb{D}}$来判断属性$A$的优劣。

假定属性$A$有$n$个可能的取值$a_1,a_2,\cdots,a_n$。$\tilde{\mathbb{D}}^i$表示$\tilde{\mathbb{D}}$中在属性$A$上取值为$a_i$的样本的子集；$\tilde{\mathbb{D}}_k$表示$\tilde{\mathbb{D}}$中属于第$k$类的样本子集

为每个样本$\vec{\mathbf{x}}$赋予一个权重$\omega_{\vec{\mathbf{x}}}$，定义：
$$
\rho=\frac{\sum_{\vec{\mathbf{x}}\in \tilde{\mathbb{D}}}\omega_{\vec{\mathbf{x}}}}{\sum_{\vec{\mathbf{x}}\in \mathbb{D}}\omega_{\vec{\mathbf{x}}}},\tilde{p_k}=\frac{\sum_{\vec{\mathbf{x}}\in \tilde{\mathbb{D}}_k}\omega_{\vec{\mathbf{x}}}}{\sum_{\vec{\mathbf{x}}\in \tilde{\mathbb{D}}}\omega_{\vec{\mathbf{x}}}},\tilde{r_i}=\frac{\sum_{\vec{\mathbf{x}}\in \tilde{\mathbb{D}}^i}\omega_{\vec{\mathbf{x}}}}{\sum_{\vec{\mathbf{x}}\in \tilde{\mathbb{D}}}\omega_{\vec{\mathbf{x}}}}
$$
将信息增益的计算公式推广为：
$$
g(\mathbb{D},A)=\rho\times G(\tilde{\mathbb{D}},A)=\rho\times (H(\tilde{\mathbb{D}})=\sum_{i=1}^n\tilde{r_i}H(\tilde{\mathbb{D}}^i|A))
$$

###### 样本划分

基于权重的样本划分：如果样本$\vec{\mathbf{x}}$在划分属性$A$上的取值已知，则：将$\vec{\mathbf{x}}$划入与其对应的子结点。$\vec{\mathbf{x}}$的权值在子结点中保持为$\omega_{\vec{\mathbf{x}}}$。

如果样本$\vec{\mathbf{x}}$在划分属性$A$上的取值未知，则：将$\vec{\mathbf{x}}$同时划入所有的子结点。

$\vec{\mathbf{x}}$的权值在所有子结点中进行调整：在属性值为$a_i$对应的子结点中，该样本的权值调整为$\tilde{r_i}\times \omega_{\vec{\mathbf{x}}}$。`C4.5`使用了该方案。

### 提升树

提升树模型可以表示为决策树为基本学习器的加法模型：
$$
f(\vec{\mathbf{x}})=f_M(\vec{\mathbf{x}})=\sum_{m=1}^Mh_m(\vec{\mathbf{x}};\Theta_m)
$$
其中 ：$h_m(\vec{\mathbf{x}};\Theta_m)$表示第$m$个决策树。$\Theta_m$为第$m$个决策树的参数。$M$为决策树的数量。

提升树算法采用前向分步算法。首先确定初始提升树$f_0(\vec{\mathbf{x}})=0$；第$m$步模型为$f_m(\vec{\mathbf{x}})=f_{m-1}(\vec{\mathbf{x}})+h_m(\vec{\mathbf{x}};\Theta_m)$。其中$h(\cdot)$为待求的第$m$个决策树；通过经验风险极小化确定第$m$个决策树的参数$\Theta_m$： 
$$
\hat{\Theta}_m=\arg\min_{\Theta_m}\sum_{i=1}^NL(\tilde{y}_i,f_m(\vec{\mathbf{x}}_i))
$$
不同问题的提升树学习算法主要区别在于使用的损失函数不同（设预测值为$\hat{y}$，真实值为$\tilde{y}$)：回归问题：通常使用平方误差损失函数：$L(\tilde{y},\hat{y})=(\tilde{y}-\hat{y})^2$。分类问题：通常使用指数损失函数：$L(\tilde{y},\hat{y})=e^{-\tilde{y}\hat{y}}$。

提升树中，当损失函数是平方损失函数和指数损失函数时，每一步优化都很简单。因为平方损失函数和指数损失函数的求导非常简单。

当损失函数是一般函数时，往往每一步优化不是很容易。针对这个问题，`Freidman`提出了梯度提升算法。梯度提升树`GBT` 是利用最速下降法的近似方法。其关键是利用损失函数的负梯度在当前模型的值作为残差的近似值，从而拟合一个回归树。根据：
$$
L(\tilde{y},f_m(\vec{\mathbf{x}}))=L(\tilde{y},f_{m-1}(\vec{\mathbf{x}})+h_m(\vec{\mathbf{x}};\Theta_m))=L(\tilde{y},f_{m-1}(\vec{\mathbf{x}}))+\frac{\part{L(\tilde{y},f_{m-1}(\vec{\mathbf{x}}))}}{\part{f_{m-1}(\vec{\mathbf{x}})}}h_m(\vec{\mathbf{x}};\Theta_m)\\
\Delta{L}=L(\tilde{y},f_m(\vec{\mathbf{x}}))-L(\tilde{y},f_{m-1}(\vec{\mathbf{x}}))=\frac{\part{L(\tilde{y},f_{m-1}(\vec{\mathbf{x}}))}}{\part{f_{m-1}(\vec{\mathbf{x}})}}h_m(\vec{\mathbf{x}};\Theta_m)
$$
要使得损失函数降低：
$$
h_m(\vec{\mathbf{x}};\Theta_m)=-\frac{\part{L(\tilde{y},f_{m-1}(\vec{\mathbf{x}}))}}{\part{f_{m-1}(\vec{\mathbf{x}})}}
$$
对于平方损失函数，它就是通常意义上的残差。对于一般损失函数，它就是残差的近似 

初始化：$f_0(\vec{\mathbf{x}})=\arg\min_{c}=\sum_{i=1}^NL(\tilde{y},c)$。它是一颗只有根结点的树，根结点的输出值为：使得损失函数最小的值。

计算$r_{m,i}$，对$r_{m,i}$拟合一棵回归树，得到第$m$棵树的叶结点区域$R_{m,j},j=1,\cdots,J$
$$
r_{m,i}=-\left[\frac{\part{L(\tilde{y},f(\vec{\mathbf{x}}_i))}}{\part{f(\vec{\mathbf{x}}_i)}}\right]_{f(\vec{\mathbf{x}})=f_{m-1}(\vec{\mathbf{x}})}
$$
对$j=1,2,\cdots,J$计算每个区域$\mathbf{R}_{m,j}$上的输出值：
$$
c_{m,j}=\arg\min_{c}=\sum_{\vec{\mathbf{x}}_i \in \mathbf{R}_{m,j}}L(\tilde{y},f_{m-1}(\vec{\mathbf{x}})+c)
$$
更新$f_{m}(\vec{\mathbf{x}})=f_{m-1}(\vec{\mathbf{x}})+\sum_{j=1}^Jc_{m,j}\mathbf{I}(\vec{\mathbf{x}}_i \in \mathbf{R}_{m,j})$ 

梯度提升决策树算法`GBDT`与`GBRT`类似，主要区别是`GBDT`的损失函数与`GBRT`的损失函数不同。

##### 正则化

在工程应用中，通常利用下列公式来更新模型：
$$
f_m(\vec{\mathbf{x}})=f_{m-1}(\vec{\mathbf{x}})+\rho h_m(\vec{\mathbf{x}};\Theta_m), 0<\rho\le 1
$$
其中$\rho$称作学习率。学习率是正则化的一部分，它可以降低模型更新的速度。经验表明：一个小的学习率$rho$可以显著提高模型的泛化能力。如果学习率较大会导致预测性能出现较大波动。

每一轮迭代中，新的决策树拟合的是原始训练集的一个子集的残差。这个子集是通过对原始训练集的无放回随机采样而来。子集的占比$f$是一个超参数，并且在每轮迭代中保持不变。如果$f=1$，则与原始的梯度提升树算法相同。较小的$f$会引入随机性，有助于改善过拟合，因此可以视作一定程度上的正则化。

梯度提升树会限制每棵树的叶子结点包含的样本数量至少包含$m$个样本，其中$m$为超参数。

#### `xgboost`

`xgboost` 也是使用与提升树相同的前向分步算法。其区别在于：`xgboost` 通过结构风险极小化来确定下一个决策树的参数$\Theta_m$
$$
\hat{\Theta}_m=\arg\min_{\Theta_m}\sum_{i=1}^NL(\tilde{y}_i,f_m(\vec{\mathbf{x}}_i))+\Omega(h_m(\vec{\mathbf{x}}))
$$
其中：$\Omega(h_m(\vec{\mathbf{x}}))$为第$m$个决策树的正则化项。这是`xgboost`  和`GBT`的一个重要区别。定义：
$$
\hat{y}^{m-1}=f_{m-1}(\vec{\mathbf{x}}_i),g_i=\frac{\part L(\tilde{y}_i,\hat{y}^{m-1})}{\part \hat{y}^{m-1}},h_i=\frac{\part^2 L(\tilde{y}_i,\hat{y}^{m-1})}{\part^2 \hat{y}^{m-1}}
$$
对目标函数$\mathcal{L}$执行二阶泰勒展开：
$$
\mathcal{L}\approx\sum_{i=1}^N\left[L(\tilde{y}_i,\hat{y}^{m-1})+g_ih_m(\vec{\mathbf{x}})+\frac{1}{2}h_ih^2_m(\vec{\mathbf{x}})\right]+\Omega(h_m(\vec{\mathbf{x}}))
$$
提升树模型只采用一阶泰勒展开。这也是`xgboost` 和`GBT`的另一个重要区别。

给定输入$\vec{\mathbf{x}}$，该决策树将该输入经过不断的划分，最终划分到某个叶结点上去。给定一个叶结点，该叶结点有一个输出值。因此将决策树拆分成结构部分$q(\cdot)$，和叶结点权重部分$\vec{\mathbf{w}}=(\omega_1,\cdots,\omega_T)$，其中$T$为叶结点的数量。结构部分$q(\vec{\mathbf{x}})$的输出是叶结点编号$d$。它的作用是将输入$\vec{\mathbf{x}}$映射到编号为$d$的叶结点。叶结点权重部分就是每个叶结点的值。它的作用是输出编号为$d$的叶结点的值$\omega_d$。因此决策树改写为：$h_m(\vec{\mathbf{x}})=\omega_{q(\vec{\mathbf{x}})}$ 

定义一个决策树的复杂度为：$\Omega(h_m(\vec{\mathbf{x}}))=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^T\omega_j^2$。叶结点越多，则决策树越复杂。每个叶结点输出值的绝对值越大，则决策树越复杂。

对于每个样本$\vec{\mathbf{x}}_i$，它必然被划分到树$h_m$的某个叶结点。定义划分到叶结点$j$的样本的集合为：$\mathbb{I}_j=\{i|q(\vec{\mathbf{x}}_i)=j\}$。则有：
$$
\mathcal{L}\approx\sum_{j=1}^T\left[\left(\sum_{i\in\mathbb{I}_j}g_i\right)\omega_j+\frac{1}{2}\left(\sum_{i\in\mathbb{I}_j}h_i+\lambda\right)\omega_j^2\right]+\gamma T +\text{constant}
$$
定义 ：$\mathbf{G}_j=\sum_{i\in\mathbb{I}_j}g_i,\mathbf{H}_j=\sum_{i\in\mathbb{I}_j}h_i$ 。$\mathbf{G}_j$刻画了隶属于叶结点$j$的那些样本的一阶偏导数之和。$\mathbf{H}_j$刻画了隶属于叶结点$j$的那些样本的二阶偏导数之和。
$$
\mathcal{L}\approx\sum_{j=1}^T\left[\mathbf{G}_j\omega_j+\frac{1}{2}\left(\mathbf{H}_j+\lambda\right)\omega_j^2\right]+\gamma T +\text{constant}
$$
假设$\omega_j$与$T,\mathbf{G}_j,\mathbf{H}_j$无关，对$\omega_j$求导等于0，则得到：$\omega_j^*=-\frac{\mathbf{G}_j}{\mathbf{H}_j+\lambda}$。忽略常数项，于是定义目标函数为：
$$
\mathcal{L}^*=-\frac{1}{2}\sum_{j=1}^T\frac{\mathbf{G}_j^2}{\mathbf{H}_j+\lambda}+\gamma T
$$
在推导过程中假设$\omega_j$与$T,\mathbf{G}_j,\mathbf{H}_j$无关，这其实假设已知树的结构。事实上$\mathcal{L}^*$是与$T$相关的，甚至与树的结构相关，因此定义$\mathcal{L}^*$为结构分。结构分刻画了：当已知树的结构时目标函数的最小值。

##### 分解节点

现在的问题是：如何得到最佳的树的结构，从而使得目标函数全局最小。

###### 贪心算法

第一种方法是对现有的叶结点加入一个分裂，然后考虑分裂之后目标函数降低多少。如果目标函数下降，则说明可以分裂。如果目标函数不下降，则说明该叶结点不宜分裂。

对于一个叶结点，假如给定其分裂点，定义划分到左子结点的样本的集合为：$\mathbb{I}_L=\{i|q(\vec{\mathbf{x}}_i)=L\}$；定义划分到右子结点的样本的集合为：$\mathbb{I}_R=\{i|q(\vec{\mathbf{x}}_i)=R\}$。则有：
$$
\mathbf{G}_L=\sum_{i\in\mathbb{I}_L}g_i,\mathbf{H}_L=\sum_{i\in\mathbb{I}_L}h_i\\
\mathbf{G}_R=\sum_{i\in\mathbb{I}_R}g_i,\mathbf{H}_R=\sum_{i\in\mathbb{I}_R}h_i\\
\mathbf{G}=\mathbf{G}_L+\mathbf{G}_R,\mathbf{H}=\mathbf{H}_R+\mathbf{H}_L
$$
定义叶结点的分裂增益为：
$$
\text{gain}=\frac{1}{2}\left[\frac{1}{2}\sum_{j=1}^T\frac{\mathbf{G}_L^2}{\mathbf{H}_L+\lambda}+\frac{1}{2}\sum_{j=1}^T\frac{\mathbf{G}_R^2}{\mathbf{H}_R+\lambda}-\frac{1}{2}\sum_{j=1}^T\frac{\mathbf{G}^2}{\mathbf{H}+\lambda}\right]-\lambda
$$
对于每个叶结点，存在很多个分裂点，且可能很多分裂点都能带来增益。解决的办法是：对于叶结点中的所有可能的分裂点进行一次扫描。然后计算每个分裂点的增益，选取增益最大的分裂点作为本叶结点的最优分裂点。

###### 近似算法

近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升。

训练数据集$\mathbb{D}=\{(\vec{\mathbf{x}}_1,\tilde{y}_1),\cdots,(\vec{\mathbf{x}}_N,\tilde{y}_N)\}$，样本$\vec{\mathbf{x}}_i=(x_{i,1},\cdots,x_{i,n})^T$。

对第$k$个特征进行分桶：如果第$k$个特征为连续特征，则执行百分位分桶，得到分桶的区间为：$\mathbb{S}_k=\{s_{k,1},\cdots,s_{k,l}\}$，其中$s_{k,1}<\cdots<s_{k,l}$。分桶的数量、分桶的区间都是超参数，需要仔细挑选。

如果第$k$个特征为离散特征，则执行按离散值分桶，得到的分桶为：$\mathbb{S}_k=\{s_{k,1},\cdots,s_{k,l}\}$，其中$s_{k,1},\cdots,s_{k,l}$为第$k$个特征的所有可能的离散值。分桶的数量$l$就是所有样本在第$k$个特征上的取值的数量。

分桶有两种模式：全局模式：在算法开始时，对每个维度分桶一次，后续的分裂都依赖于该分桶并不再更新。优点是：只需要计算一次，不需要重复计算。缺点是：在经过多次分裂之后，叶结点的样本有可能在很多全局桶中是空的。

局部模式：除了在算法开始时进行分桶，每次拆分之后再重新分桶。优点是：每次分桶都能保证各桶中的样本数量都是均匀的。缺点是：计算量较大。

##### 缺失值

理论上，数据缺失和数值0的含义是不同的，数值 0 是有效的。实际上，数值0的处理方式类似缺失值的处理方式，都视为稀疏特征。在`xgboost` 中，数值0的处理方式和缺失值的处理方式是统一的。这只是一个计算上的优化，用于加速对稀疏特征的处理速度。对于稀疏特征，只需要对有效值进行处理，无效值则采用默认的分裂方向。

遍历各拆分点：沿着第$k$维，将当前有效的叶结点的样本**从小到大**排序。

> 这相当于所有无效特征值的样本放在最右侧，因此可以保证无效的特征值都在右子树。

遍历各拆分点：沿着$k$维，将当前叶结点的样本**从大到小**排序。

> 这相当于所有无效特征值的样本放在最左侧，因此可以保证无效的特征值都在左子树。

#### `LightGBM`

`LightGBM` 的思想：若减少训练样本的数量，或者减少样本的训练特征数量，则可以大幅度提高训练速度。

`LightGBM` 提出了两个策略：`Gradient-based One-Side Sampling(GOSS)`： 基于梯度的采样。该方法用于减少训练样本的数量。`Exclusive Feature Bundling(EFB)`： 基于互斥特征的特征捆绑。该方法用于减少样本的特征。

##### `GOSS`

减少样本的数量的难点在于：不知道哪些样本应该被保留，哪些样本被丢弃。传统方法：采用随机丢弃的策略。`GOSS` 方法：保留梯度较大的样本，梯度较小的样本则随机丢弃。

`GOSS` 采用样本的梯度作为样本的权重：

- 如果权重较小，则说明样本的梯度较小，说明该样本已经得到了很好的训练。因此对于权重较小的样本，则可以随机丢弃。
- 如果权重较大，则说明样本的梯度较大，说明该样本未能充分训练。因此对于权重较大的样本，则需要保留。

`GOSS` 丢弃了部分样本，因此它改变了训练样本的分布。这会影响到模型的预测准确性。为了解决这个问题，`GOSS` 对小梯度的样本进行了修正：对每个保留下来的、小梯度的样本，其梯度乘以系数$\frac{1-a}{b}$

其中（假设样本总数为$N$）：$a$表示大梯度采样比。其意义为：保留梯度的绝对值在 `top`$a\times N$的样本作为重要的样本。$b$表示小梯度采样比。其意义为：从不重要的样本中随机保留$b\times N$的样本。$1-a$表示不重要的样本的比例。刻画了：$\frac{1-a}{b}$从不重要的样本中，随机保留的样本的比例的倒数。

根据梯度的绝对值大小，将样本按照从大到小排列。取其中取$\text{topN}$的样本作为重要性样本。在$\text{topN}$之外的样本中，随机选取$\text{randN}$的样本作为保留样本，剩下的样本被丢弃。构建新的训练集：`重要性样本+随机保留的样本`，其中：$\text{topN}$个重要性样本，每个样本的权重都为 1。$\text{randN}$个随机保留的样本，每个样本的权重都为$\frac{1-a}{b}$。根据新的训练集及其权重，训练决策树模型$h_m(\vec{\mathbf{x}})$来拟合残差。返回训练好的$h_m(\vec{\mathbf{x}})$。

`GOSS` 的采样增加了基学习器的多样性，有助于提升集成模型的泛化能力。

##### `EFB`

`LightBGM` 的思路是：很多特征都是互斥的，即：这些特征不会同时取得非零的值。如果能将这些互斥的特征捆绑打包成一个特征，那么可以将特征数量大幅度降低。

现在有两个问题：如何找到互斥的特征。如何将互斥的特征捆绑成一个特征

定义`打包特征集`为这样的特征的集合：集合中的特征两两互斥。给定数据集$\mathbb{D}=\{(\vec{\mathbf{x}}_1,\tilde{y_1}),\cdots,(\vec{\mathbf{x}}_N,\tilde{y_N})\}$，其中样本$\vec{\mathbf{x}}_i=(x_{i,1},\cdots,x_{i,n})^T$。如果对每个$i=1,2,\cdots,N$，都不会出现$x_{i,j}\ne0 \text{and}x_{i,k}\ne0$，则特征$j$和特征$k$互斥。

将每个特征视为图中的一个顶点。遍历每个样本$\vec{\mathbf{x}}_i\in\mathbb{D}$， 如果特征$j,k$之间不互斥，则：如果顶点$j,k$之间不存在边，则在顶点$j,k$之间连接一条边，权重为 1 。如果顶点$j,k$之间存在边，则顶点$j,k$之间的边的权重加1。最终，如果一组顶点之间都不存在边，则它们是相互互斥的，则可以放入到同一个`打包特征集` 中。事实上有些特征之间并不是完全互斥的，而是存在非常少量的冲突。即：存在少量的样本，在这些样本上，这些特征之间同时取得非零的值。如果允许这种少量的冲突，则可以将更多的特征放入`打包特征集`中，这样就可以减少更多的特征。

互斥特征打包的思想：可以从打包的特征中分离出原始的特征。假设特征 `a` 的取值范围为 `[0,10)`， 特征 `b` 的取值范围为 `[0,20)` 。如果`a,b` 是互斥特征，那么打包的时候：对于特征 `b`的值，给它一个偏移量，比如 20。最终打包特征的取值范围为：`[0,40)`。如果打包特征的取值在 `[0,10)`， 说明该值来自于特征 `a` 。如果打包特征的取值在`[20,40)`，说明该值来自于特征 `b` 。

基于`histogram` 的算法需要考虑分桶，但是原理也是类似：将 `[0,x]` 之间的桶分给特征 `a`， 将 `[x+offset,y]` 之间的桶分给特征 `b`。 其中 `offset > 0` 。

##### 优化

###### `histogram`算法

基本思想：先把连续的浮点特征值离散化成$k$个整数，同时构造一个宽度为$k$的直方图。

在遍历数据时：

- 根据离散化后的值作为索引在直方图中累积统计量。
- 当遍历一次数据后，直方图累积了需要的统计量。
- 然后根据直方图的离散值，遍历寻找最优的分割点。

###### `leaf-wise`策略

大部分梯度提升树算法采用`level-wise` 的叶子生长策略；而`lightgbm` 采用`leaf-wise` 的叶子生长策略。

`level-wise` ：优点：过一遍数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。缺点：实际上`level-wise`是一种低效算法 。它不加区分的对待同一层的叶子，带来了很多没必要的开销：实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。

`leaf-wise`：是一种更为高效的策略。每次从当前所有叶子中，找到分裂增益最大的一个叶子来分裂。优点：同`level-wise` 相比，在分裂次数相同的情况下，`leaf-wise` 可以降低更多的误差，得到更好的精度。缺点：可能会长出比较深的决策树，产生过拟合。

因此 `lightgbm` 在 `leaf-wise` 之上增加了一个最大深度限制，在保证高效率的同时防止过拟合。

训练数据集$\mathbb{D}=\{(\vec{\mathbf{x}}_1,\tilde{y_1}),\cdots,(\vec{\mathbf{x}}_N,\tilde{y_N})\}$。设已经将输入空间划分为$J$个单元$R_1,\cdots,R_J$，且在每个单元$R_j$上有一个固定的输出值$c_j$。 则决策树可以表示为：
$$
h(\vec{\mathbf{x}};\Theta_m)=\sum_{j=1}^Jc_jI(\vec{\mathbf{x}}\in R_j)
$$
$J$是决策树的复杂度，即叶结点个数。

对于回归问题，我们常用的损失函数是MSE；对于分类问题，我们常用的损失函数是对数损失函数：$L(\theta)=\sum_i\left[y_i\ln(1+e^{-\hat{y}_i})+(1-y_i)\ln(1+e^{\hat{y}_i})\right]$



