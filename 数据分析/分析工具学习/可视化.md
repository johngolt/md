##### 可视化

 探索性分析指理解数据并找出值得分析或分享给他人的精华。而解释性分析，我们迫切希望能够言之有物，讲好某个故事。一个完整的数据可视化过程，主要包括以下4个步骤：确定数据可视化的主题、提炼可视化主题的数据、根据数据关系确定图表、进行可视化布局及设计

![](../../picture/2/100.png)

###### 数据类型

分类数据：指针反映事物类别的数据。如：用户的设备可以分为Iphone用户和andorid用户两种；支付方式可以分为支付宝、微信、现金支付三种等。诸如此类的分类所得到的数据被称为分类数据。

时序数据：也称时间序列数据，是指同一统一指标按时间顺序记录的数据列。如:每个月的新增用户数量、某公司近十年每年的GMV等。诸如此类按时间顺序来记录的指标对应的数据成为时序数据。

空间数据：指用来表示空间实体的位置、形状、大小及其分布特征诸多方面信息的数据，它可以用来描述来自现实世界的目标，它具有定位、定性、时间和空间关系等特性。

多变量：数据通常以表格形式的出现，表格中有多个列，每一列代表一个变量，将这份数据就称为多变量数据，多变量常用来研究变量之间的相关性。即用来找出影响某一指标的因素有哪些。

###### 可视化形式

在做可视化的过程中，我们需要先明确我们有什么数据，然后再去研究这些数据适合做什么类型的可视化，再然后从这些适合的可视化类型中选择能够很好的满足我们需求的视图。

为了找到合适的可视化形式我们需要先介绍两个内容：有哪些可视化形式、如何让可视化更加清晰。

  1. 有哪些可视化形式：基于数据的可视化形式有：视觉暗示、坐标系、标尺、背景信息以及前面四种形式的任意组合。① 视觉暗示：指通过查看图表就可以与潜意识中的意识进行联系从而得出图表表达的意识。常用的视觉暗示主要有：位置（位置高低）、长度（长短）、角度（大小）、方向（方向上升还是下降）、形状（不同形状代表不同分类）、面积（面积大小）、体积（体积大小）、饱和度（色调的强度，就是颜色的深浅）、色调（不同颜色）。
      坐标系：这里的坐标系和我们之前数学中学到的坐标系是相同的，只不过坐标轴的意义可能稍有不同。常见的坐标系种类有：直角坐标系、极坐标系和地理坐标系。
     ③ 标尺：前面说到的三种坐标系只是定义了展示数据的维度和方向，而标尺的作用是用来衡量不同方向和维度上的大小，其实和我们熟悉的刻度挺像。④ 背景信息：此处的背景和我们在语文中学习到的背景是一个概念，是为了说明数据的相关信息（who、what、when、where、why），使数据更加清晰，便于读者更好的理解。⑤ 组合组件：组合组件就是根据目标用途将上面四种信息进行组合。

2. 如何让可视化更加清晰① 建立视觉层次：把图表在视觉上进行分层，把非重点信息弱化，重点信息强化突出。② 增强图标可读性：让数据点更容易比较 、留白，图表之间留有一定空间的空白。③ 高亮显示重点内容：高亮就是以特殊形式显示的内容，便于读者在一堆数据中很快抓住重点。注释可视化：一般指图标的标题部分。帮助读者更好地理解图表的意思。

 可视化元素由3部分组成：可视化空间+标记+视觉通道， 数据可视化的显示空间，通常是二维。三维物体的可视化，通过图形绘制技术，解决了在二维平面显示的问题。 标记，是数据属性到可视化几何图形元素的映射，用来代表数据属性的归类。根据空间自由度的差别，标记可以分为点、线、面、体，分别具有零自由度、一维、二维、三维自由度。如我们常见的散点图、折线图、矩形树图、三维柱状图，分别采用了点、线、面、体这四种不同类型的标记。数据属性的值到标记的视觉呈现参数的映射，叫做视觉通道，通常用于展示数据属性的定量信息。常用的视觉通道包括：标记的位置、大小、形状、方向、颜色等。

![](../picture/2/101.png)

 如果Python没有在其标准库中找到该模块，它将检查外部模块。这个操作也是以一种非常特殊的顺序进行的。它将首先在当前目录中进行搜索，然后移动到包安装的目录。 要让Python认识到一个目录是一个模块，也该目录必须包含一个__init_ .py文件。这样做是为了防止意外的名称冲突 

 求和编码通过比较某一特征取值下对应标签（或其他相关变量）的均值与标签的均值之间的差别来对特征进行编码。  Helmet编码是仅次于OHE和SumEncoder使用最广泛的编码方法，与SumEncoder不同的是，它比较的是某一特征取值下对应标签（或其他相关变量）的均值与他之前特征的均值之间的差异，而不是和所有特征的均值比较。

 对于无序的离散特征，实战中使用 OneHot, Hashing, LeaveOneOut, and Target encoding 方法效果较好，但是使用OneHot时要避免高基类别的特征以及基于决策树的模型， 使用Xgboost处理高维稀疏的问题效果并不会很差。  对于有序离散特征，尝试 Ordinal (Integer), Binary, OneHot, LeaveOneOut, and Target. Helmert, Sum, BackwardDifference and Polynomial 基本没啥用，但是当你有确切的原因或者对于业务的理解的话，可以进行尝试。 对于回归问题而言，Target 与 LeaveOneOut 方法可能不会有比较好的效果。LeaveOneOut、WeightOfEvidence、James-Stein、M-estimator 适合用来处理高基数特征。Helmert、Sum、Backward Difference、Polynomial 在机器学习问题里的效果往往不是很好(过拟合的原因)。











数仓经常会碰到的两类问题：

1、两个数据报表进行对比，结果差异很大，需要人工核对分析指标的维度信息，比如从头分析数据指标从哪里来，处理条件是什么，最后才能分析出问题原因       ——数据回溯问题

2、基础数据表因某种原因需要修改字段，需要评估其对数仓的影响，费时费力，然后在做方案        —— 影响分析问题

这两类问题都属于数据血缘分析问题，数据血缘分析还有其它的积极意义，比如：

问题定位分析

类似于影响分析，当程序运行出错时，可以方便找到问题的节点，并判断出问题的原因以及后续的影响

指标波动分析

当某个指标出现较大的波动时，可进行溯源分析，判断是由哪条数据发生变化所导致的

数据体检

判定系统和数据的健康情况，是否存在大量的冗余数据、无效数据、无来源数据、重复计算、系统资源浪费等问题

数据评估

通过血缘分析和元数据，可以从数据的集中度、分布、冗余度、数据热度、重要性等多角度进行评估分析，从而初步判断数据的价值

##### 结构思维

结构思维就是对应着数据分析的目的。需要通过数据分析来解释的问题，无外乎在三个方面：what，why，how即是什么，为什么以及怎么样三个方面的问题。针对问题的拆分和对比也是围绕着三个点展开。对what而言，就是需要了解现在的情况是什么样子，有哪些对象，在对象中涵盖了哪些数据，对象可以分成哪些部分，各个部门数据的大小如何，主要的数据贡献是来自于哪些地方等等。What就是状态进行展开，尤其是现状展开。比如分析现在各家产商的份额就是属于what的问题。

对why而言，比what更进一步，从现状进行回溯，需要通过分析来理清问题发生的因果关系以及对象之间的逻辑关系。和what聚焦静态数据相比，why更专注于动态数据，重点在考察哪些数据在不同的情况下发生的变化。比如需要分析市场份额为什么发生的变化，价格为什么上涨或者下降等等。

对how而言，从关注原因发展到了关注结果，更注重未来的情况，各个对象的数据未来会如何变化，对于分析的对象而言未来是什么样子。比如在制定了一项市场决策后，就会预测未来的投资收益情况。



用户画像的标签一般通过两种形式获得，基于已有数据或者一定规则加工，流失标签和距今天数皆是。另外一种是基于已有的数据计算概率模型，会用到机器学习和数据挖掘。

用户画像一般按业务属性划分多个类别模块。除了常见的人口统计，社会属性外。还有用户消费画像，用户行为画像，用户兴趣画像等。具体的画像得看产品形态，像金融领域，还会有风险画像，包括征信、违约、洗钱、还款能力、保险黑名单等。电商领域会有商品的类目偏好、品类偏好、品牌偏好

![](./picture/2/132.png)















 

 

 

 



















#### 后端基础设施

SaaS平台供应商将应用软件统一部署在自己的服务器上，客户可以根据工作实际需求，通过互联网向厂商定购所需的应用软件服务，按定购的服务多少和时间长短向厂商支付费用，并通过互联网获得Saas平台供应商提供的服务。

后端服务器的主要功能概括起来就是：接收外界的API请求，解析后去执行数据库操作，最后将数据包装好返回给调用者（当然，中间还包含其他业务逻辑）。所以，这句话中包含了三类框架的信息，分别是：和数据操作有关的，和API请求响应有关的，和业务逻辑有关的，我们下面就这三类来讲述。

和数据操作相关的：这类框架一般负责和数据库进行连接，负责SQL的处理，以及将查询到的数据映射成指定的java对象。这样子的框架有：mybatis: 、hibernate 

 和API处理相关的：这类框架一般负责根据不同的API请求来调用程序中不同的处理方法，负责将调用者传入的数据映射成java对象，也负责处理程序返回给客户端的响应数据格式等。这样子的框架一般有：struts、spring mvc:

和业务逻辑相关的：这类框架主要用到了两个特性（其实是一个），以此来简化程序的复杂性，这里先稍微提及一下，以后再做说明，他们是：控制反转和依赖注入。总之呢，我们在程序中结合这类框架，写出来的程序具有更好的维护性扩展性，也更加清晰简洁。这类程序有：spring、guice

 我们选用的框架有： 数据层：mybatis、API层：spring mvc 、业务层：spring、外加重要的：spring boot 。这里说一下 spring boot ，其他三个你们都知道了。那 spring boot 是什么呢？就是进一步简化基于 spring 框架的项目的构建工具。boot本就是启动的意思，用上 spring boot 进行java后台开发，在spring的基础上又省力了许多。

 前端至少要懂的三个部分：HTML，CSS，JavaScript（简称JS），那首先先明确这三个概念：HTML负责结构，网页想要表达的内容由html书写。CSS负责样式，网页的美与丑由它来控制JS负责交互，用户和网页产生的互动由它来控制。

![](./picture/2/135.png)

后端基础设施主要指的是应用在线上稳定运行需要依赖的关键组件或者服务。开发或者搭建好以上的后端基础设施，一般情况下是能够支撑很长一段时间内的业务的。此外，对于一个完整的架构来说，还有很多应用感知不到的系统基础服务，如负载均衡、自动化部署、系统安全等

##### API网口

在移动APP的开发过程中，通常后端提供的接口需要以下功能的支持：负载均衡、API访问权限控制、用户鉴权。一般的做法，使用Nginx做负载均衡，然后在每个业务应用里做API接口的访问权限控制和用户鉴权，更优化一点的方式则是把后两者做成公共类库供所有业务调用。但从总体上来看，这三种特性都属于业务的公共需求，更可取的方式则是集成到一起作为一个服务，既可以动态地修改权限控制和鉴权机制，也可以减少每个业务集成这些机制的成本。这种服务就是API网关，可以选择自己实现。

![](./picture/2/136.png)

但是以上方案的一个问题是由于所有API请求都要经过网关，它很容易成为系统的性能瓶颈。因此，可以采取的方案是：去掉API网关，让业务应用直接对接统一认证中心，在基础框架层面保证每个API调用都需要先通过统一认证中心的认证，这里可以采取缓存认证结果的方式避免对统一认证中心产生过大的请求压力。

##### 业务应用和后端基础框架

业务应用分为：在线业务应用和内部业务应用。在线业务应用：直接面向互联网用户的应用、接口等，典型的特点就是：请求量大、高并发、对故障的容忍度低。内部业务应用：主要面向公司内部用户的应用。比如，内部数据管理平台、广告投放平台等。相比起在线业务应用，其特点: 数据保密性高、压力小、并发量小、允许故障的发生。

业务应用基于后端的基础框架开发，针对Java后端来说，应该有以下几个框架：MVC框架：统一开发流程、提高开发效率、屏蔽一些关键细节的Web/后端框架。典型的如SpringMVC、Jersey以及国人开发的JFinal以及阿里的WebX。

IOC框架：实现依赖注入/控制反转的框架。Java中最为流行的Spring框架的核心就是IOC功能。

ORM框架：能够屏蔽底层数据库细节，提供统一的数据访问接口的数据库操作框架，额外地能够支持客户端主从、分库、分表等分布式特性。MyBatis是目前最为流行的ORM框架。此外，Spring ORM中提供的JdbcTemplate也很不错。当然，对于分库分表、主从分离这些需求，一般就需要自己实现，开源的则有阿里的TDDL、当当的sharding-jdbc（从datasource层面解决了分库分表、读写分离的问题，对应用透明、零侵入）。缓存框架：对Redis、Memcached这些缓存软件操作的统一封装，能够支持客户端分布式方案、主从等。一般使用Spring的RedisTemplate即可，也可以使用Jedis做自己的封装，支持客户端分布式方案、主从等。

JavaEE应用性能检测框架：对于线上的JavaEE应用，需要有一个统一的框架集成到每一个业务中检测每一个请求、方法调用、JDBC连接、Redis连接等的耗时、状态等。

##### 缓存、数据库、搜索引擎、消息队列

缓存、数据库、搜索引擎、消息队列这四者都是应用依赖的后端基础服务，他们的性能直接影响到了应用的整体性能。

缓存: 缓存通常被用来解决热点数据的访问问题，是提高数据查询性能的强大武器。在高并发的后端应用中，将数据持久层的数据加载到缓存中，能够隔离高并发请求与后端数据库，避免数据库被大量请求击垮。目前常用的除了在内存中的本地缓存，比较普遍的集中缓存软件有Memcached和Redis。

数据库：数据库可以说是后端应用最基本的基础设施。基本上绝大多数业务数据都是持久化存储在数据库中的。主流的数据库包括传统的关系型数据库（MySQL、PostgreSQL）以及最近几年开始流行的NoSQL（MongoDB、HBase）。其中HBase是用于大数据领域的列数据库，受限于其查询性能，一般并不用来做业务数据库。

搜索引擎：搜索引擎是针对全文检索以及数据各种维度查询设计的软件。目前用的比较多的开源软件是Solr和Elasticsearch，不同之处主要在于termIndex的存储、分布式架构的支持等。Elasticsearch由于对集群的良好支持以及高性能的实现，已经逐渐成为搜索引擎的主流开源方案。

消息队列：数据传输的一种方式就是通过消息队列。目前用的比较普遍的消息队列包括为日志设计的Kafka以及重事务的RabbitMQ等。在对消息丢失不是特别敏感且并不要求消息事务的场景下，选择Kafka能够获得更高的性能；否则，RabbitMQ则是更好的选择。

##### 文件存储

不管是业务应用、依赖的后端服务还是其他的各种服务，最终还是要依赖于底层文件存储的。通常来说，文件存储需要满足的特性有：可靠性、容灾性、稳定性，即要保证存储的数据不会轻易丢失，即使发生故障也能够有回滚方案，也要保证高可用。在底层可以采用传统的RAID作为解决方案，再上一层，目前Hadoop的HDFS则是最为普遍的分布式文件存储方案，当然还有NFS、Samba这种共享文件系统也提供了简单的分布式存储的特性。此外，如果文件存储确实成为了应用的瓶颈或者必须提高文件存储的性能从而提升整个系统的性能时，那么最为直接和简单的做法就是抛弃传统机械硬盘，用SSD硬盘替代。

##### 统一认证中心

统一认证中心，主要是对APP用户、内部用户、APP等的认证服务，包括用户的注册、登录验证、Token鉴权内部信息系统用户的管理和登录鉴权APP的管理，包括APP的secret生成，APP信息的验证（如验证接口签名）等。之所以需要统一认证中心，就是为了能够集中对这些所有APP都会用到的信息进行管理，也给所有应用提供统一的认证服务。尤其是在有很多业务需要共享用户数据的时候，构建一个统一认证中心是非常必要的。此外，通过统一认证中心构建移动APP的单点登录也是水到渠成的事情：模仿Web的机制，将认证后的信息加密存储到本地存储中供多个APP使用。

##### 单点登录系统

目前很多大的在线Web网站都是有单点登录系统的，通俗的来说就是只需要一次用户登录，就能够进入多个业务应用（权限可以不相同），非常方便用户的操作。而在移动互联网公司中，内部的各种管理、信息系统甚至外部应用同样也需要单点登录系统。

![](./picture/2/137.png)

##### 统一配置中心

在Java后端应用中，一种读写配置比较通用的方式就是将配置文件写在Propeties、YAML、HCON等文件中，修改的时候只需要更新文件重新部署即可，可以做到不牵扯代码层面改动的目的。统一配置中心，则是基于这种方式之上的统一对所有业务或者基础后端服务的相关配置文件进行管理的统一服务, 具有以下特性：能够在线动态修改配置文件并生效配置文件可以区分环境（开发、测试、生产等）在Java中可以通过注解、XML配置的方式引入相关配置

##### 服务治理框架

对于外部API调用或者客户端对后端API的访问，可以使用HTTP协议或者RESTful。但对于内部服务间的调用，一般都是通过RPC机制来调用的。目前主流的RPC协议有：RMI、Hessian、Thrift、Dubbo这些RPC协议各有优劣点，需要针对业务需求做出最好的选择。这样，当你的系统服务在逐渐增多，RPC调用链越来越复杂，很多情况下，需要不停的更新文档来维护这些调用关系。一个对这些服务进行管理的框架可以大大减少因此带来的繁琐的人力工作。传统的ESB（企业服务总线）本质就是一个服务治理方案，但ESB作为一种proxy的角色存在于Client和Server之间，所有请求都需要经过ESB，使得ESB很容易成为性能瓶颈。因此，基于传统的ESB，更好的一种设计如下图所示：

![](./picture/2/138.png)

如图，以配置中心为枢纽，调用关系只存在于Client和提供服务的Server之间，就避免了传统ESB的性能瓶颈问题。对于这种设计，ESB应该支持的特性如下：服务提供方的注册、管理服务消费者的注册、管理服务的版本管理、负载均衡、流量控制、服务降级、资源隔离服务的容错、熔断

##### 统一调度中心

在很多业务中，定时调度是一个非常普遍的场景，比如定时去抓取数据、定时刷新订单的状态等。通常的做法就是针对各自的业务依赖Linux的Cron机制或者Java中的Quartz。统一调度中心则是对所有的调度任务进行管理，这样能够统一对调度集群进行调优、扩展、任务管理等。Azkaban和Yahoo的Oozie是Hadoop的流式工作管理引擎，也可以作为统一调度中心来使用。当然，你也可以使用Cron或者Quartz来实现自己的统一调度中心。根据Cron表达式调度任务动态修改、停止、删除任务支持任务分片执行支持任务工作流：比如一个任务完成之后再执行下一个任务任务支持脚本、代码、url等多种形式任务执行的日志记录、故障报警对于Java的Quartz这里需要说明一下：这个Quartz需要和Spring Quartz区分，后者是Spring对Quartz框架的简单实现也是目前使用的最多的一种调度方式。但其并没有做高可用集群的支持。而Quartz虽然有集群的支持，但是配置起来非常复杂。现在很多方案都是使用Zookeeper来实现Spring Quartz的分布式集群。

##### 统一日志服务

日志是开发过程必不可少的东西。打印日志的时机、技巧是很能体现出工程师编码水平的。毕竟，日志是线上服务能够定位、排查异常最为直接的信息。通常的，将日志分散在各个业务中非常不方便对问题的管理和排查。统一日志服务则使用单独的日志服务器记录日志，各个业务通过统一的日志框架将日志输出到日志服务器上。

##### 数据基础设施

###### 数据高速公路

接着上面讲的统一日志服务，其输出的日志最终是变成数据到数据高速公路上供后续的数据处理程序消费的。这中间的过程包括日志的收集和传输。

收集：统一日志服务将日志打印在日志服务上之后，需要日志收集机制将其集中起来。目前，常见的日志收集方案有：Scribe、Chukwa、Kakfa和Flume。

传输：通过消息队列将数据传输到数据处理服务中。对于日志来说，通常选择Kafka这个消息队列即可。

此外，这里还有一个关键的技术就是数据库和数据仓库间的数据同步问题，即将需要分析的数据从数据库中同步到诸如Hive这种数据仓库时使用的方案。

###### 离线数据分析

离线数据分析是可以有延迟的，一般针对的是非实时需求的数据分析工作，产生的也是延迟一天的报表。目前最常用的离线数据分析技术除了Hadoop还有Spark。相比Hadoop，Spark性能上有很大优势，当然对硬件资源要求也高。其中，Hadoop中的Yarn作为资源管理调度组件除了服务于MR还可以用于Spark（Spark on Yarn），Mesos则是另一种资源管理调度系统。

对于Hadoop，传统的MR编写很复杂，也不利于维护，可以选择使用Hive来用SQL替代编写MR。而对于Spark，也有类似Hive的Spark SQL。

此外，对于离线数据分析，还有一个很关键的就是数据倾斜问题。所谓数据倾斜指的是region数据分布不均，造成有的结点负载很低，而有些却负载很高，从而影响整体的性能。处理好数据倾斜问题对于数据处理是很关键的。

###### 实时数据分析

实时数据处理一般情况下都是基于增量处理的，相对于离线来说并非可靠的，一旦出现故障（如集群崩溃）或者数据处理失败，是很难对数据恢复或者修复异常数据的。因此结合离线+实时是目前最普遍采用的数据处理方案。Lambda架构就是一个结合离线和实时数据处理的架构方案。此外，实时数据分析中还有一个很常见的场景：多维数据实时分析，即能够组合任意维度进行数据展示和分析。目前有两种解决此问题的方案：ROLAP和MOLAP。ROLAP：使用关系型数据库或者扩展的关系型数据库来管理数据仓库数据，以Hive、Spark SQL、Presto为代表。MOLAP：基于数据立方体的多位存储引擎，用空间换时间，把所有的分析情况都物化为物理表或者视图。以Druid、Pinot和Kylin为代表，不同于ROLAP（Hive、Spark SQL）, 其原生的支持多维的数据查询。

###### 数据即席分析

离线和实时数据分析产生的一些报表是给数据分析师、产品经理参考使用的，但是很多情况下，线上的程序并不能满足这些需求方的需求。这时候就需要需求方自己对数据仓库进行查询统计。针对这些需求方，SQL上手容易、易描述等特点决定了其可能是一个最为合适的方式。因此提供一个SQL的即席查询工具能够大大提高数据分析师、产品经理的工作效率。

##### 故障监控

对于面向用户的线上服务，发生故障是一件很严重的事情。因此，做好线上服务的故障检测告警是一件非常重要的事情。可以将故障监控分为以下两个层面的监控：系统监控：主要指对主机的带宽、CPU、内存、硬盘、IO等硬件资源的监控。可以使用Nagios、Cacti等开源软件进行监控。目前，市面上也有很多第三方服务能够提供对于主机资源的监控，如监控宝等。对于分布式服务集群（如Hadoop、Storm、Kafka、Flume等集群）的监控则可以使用Ganglia。此外，小米开源的OpenFalcon也很不错，涵盖了系统监控、JVM监控、应用监控等，也支持自定义的监控机制。业务监控：是在主机资源层面以上的监控，比如APP的PV、UV数据异常、交易失败等。需要业务中加入相关的监控代码，比如在异常抛出的地方，加一段日志记录。监控还有一个关键的步骤就是告警。告警的方式有很多种：邮件、IM、短信等。考虑到故障的重要性不同、告警的合理性、便于定位问题等因素，有以下建议：告警日志要记录发生故障的机器ID，尤其是在集群服务中，如果没有记录机器ID，那么对于后续的问题定位会很困难。要对告警做聚合，不要每一个故障都单独进行告警，这样会对工程师造成极大的困扰。要对告警做等级划分，不能对所有告警都做同样的优先级处理。使用微信做为告警软件，能够在节省短信成本的情况下，保证告警的到达率。



##### 用户画像

###### 流程、方法

用户画像的核心工作就是给用户打标签，标签通常是人为规定的高度精炼的特征标识，如年龄、性别、地域、兴趣等。这些标签集合就能抽象出一个用户的信息全貌，每个标签分别描述了该用户的一个维度，各个维度之间相互联系，共同构成对用户的一个整体描述。

目前主流的标签体系都是层次化的。首先标签分为几个大类，每个大类下进行逐层细分。在构建标签时，我们只需要构建最下层的标签，就能够映射到上面两级标签。

上层标签都是抽象的标签集合，一般没有实用意义，只有统计意义。例如我们可以统计有人口属性标签的用户比例，但用户有人口属性标签本身对广告投放没有任何意义。

![](./picture/2/167.png)

用于广告投放和精准营销的一般是底层标签，对于底层标签有两个要求：一个是每个标签只能表示一种含义，避免标签之间的重复和冲突，便于计算机处理；另一个是标签必须有一定的语义，方便相关人员理解每个标签的含义。

此外，标签的粒度也是需要注意的，标签粒度太粗会没有区分度，粒度过细会导致标签体系太过复杂而不具有通用性。

最后介绍一下各类标签构建的优先级。构建的优先级需要综合考虑业务需求、构建难易程度等，业务需求各有不同，这里介绍的优先级排序方法主要依据构建的难易程度和各类标签的依存关系

![](./picture/2/168.png)

基于原始数据首先构建的是事实标签，事实标签可以从数据库直接获取（如注册信息），或通过简单的统计得到。这类标签构建难度低、实际含义明确，且部分标签可用作后续标签挖掘的基础特征（如产品购买次数可用来作为用户购物偏好的输入特征数据）。

事实标签的构造过程，也是对数据加深理解的过程。对数据进行统计的同时，不仅完成了数据的处理与加工，也对数据的分布有了一定的了解，为高级标签的构造做好了准备。

模型标签是标签体系的核心，也是用户画像工作量最大的部分，大多数用户标签的核心都是模型标签。模型标签的构造大多需要用到机器学习和自然语言处理技术，我们下文中介绍的标签构造方法主要指的是模型标签，具体的构造算法会在本文第03章详细介绍。

最后构造的是高级标签，高级标签是基于事实标签和模型标签进行统计建模得出的，它的构造多与实际的业务指标紧密联系。只有完成基础标签的构建，才能够构造高级标签。构建高级标签使用的模型，可以是简单的数据统计，也可以是复杂的机器学习模型。

第一类是人口属性，这一类标签比较稳定，一旦建立很长一段时间基本不用更新，标签体系也比较固定；第二类是兴趣属性，这类标签随时间变化很快，标签有很强的时效性，标签体系也不固定；第三类是地理属性，这一类标签的时效性跨度很大，如GPS轨迹标签需要做到实时更新，而常住地标签一般可以几个月不用更新，挖掘的方法和前面两类也大有不同
人口属性包括年龄、性别、学历、人生阶段、收入水平、消费水平、所属行业等。这些标签基本是稳定的，构建一次可以很长一段时间不用更新，标签的有效期都在一个月以上。同时标签体系的划分也比较固定
一般会用填写了信息的这部分用户作为样本，把用户的行为数据作为特征训练模型，对无标签的用户进行人口属性的预测。这种模型把用户的标签传给和他行为相似的用户，可以认为是对人群进行了标签扩散，因此常被称为标签扩散模型。
地理位置画像一般分为两部分：一部分是常驻地画像；一部分是GPS画像。两类画像的差别很大，常驻地画像比较容易构造，且标签比较稳定，GPS画像需要实时更新。
兴趣画像的人为评估比较困难，我们对于兴趣画像的常用评估方法是设计小流量的A/B-test进行验证。

我们可以筛选一部分标签用户，给这部分用户进行和标签相关的推送，看标签用户对相关内容是否有更好的反馈。
标签的准确率指的是被打上正确标签的用户比例，准确率是用户画像最核心的指标，一个准确率非常低的标签是没有应用价值的。
标签的覆盖率指的是被打上标签的用户占全量用户的比例，我们希望标签的覆盖率尽可能的高。但覆盖率和准确率是一对矛盾的指标，需要对二者进行权衡，一般的做法是在准确率符合一定标准的情况下，尽可能的提升覆盖率。

###### 模型设计与存储

以Hive为例，我们最常用的就是**横表**，也就是一个 key，跟上它的所有标签。

![](./picture/2/169.png)

1. 由于用户的标签会非常多，而且随着用户画像的深入，会有很多细分领域的标签，这就意味着标签的数量会随时增加，而且可能会很频繁。
2. 不同的标签计算频率不同，比如说学历一周计算一次都是可以接收的，但是APP登录活跃情况却可能需要每天都要计算。
3. 计算完成时间不同，如果是以横表的形式存储，那么最终需要把各个小表的计算结果合并，此时如果出现了一部分结果早上3点计算完成，一部分要早上10点才能计算完成，那么横表最终的生成时间就要很晚。
4. 大量空缺的标签会导致存储稀疏，有一些标签会有很多的缺失，这在用户画像中很常见。

竖表长下面这个样子：

![](./picture/2/170.png)

竖表其实就是将标签都拆开，一个用户有多少标签，那么在这里面就会有几条数据。

![](./picture/2/171.png)

统计类标签

这类标签是最为基础也最为常见的标签类型，例如，对于某个用户来说，其性别、年龄、城市、星座、近7日活跃时长、近7日活跃天数、近7日活跃次数等字段可以从用户注册数据、用户访问、消费数据中统计得出。该类标签构成了用户画像的基础。

2. 规则类标签

该类标签基于用户行为及确定的规则产生。例如，对平台上“消费活跃”用户这一口径的定义为“近30天交易次数≥2”。在实际开发画像的过程中，由于运营人员对业务更为熟悉，而数据人员对数据的结构、分布、特征更为熟悉，因此规则类标签的规则由运营人员和数据人员共同协商确定；

3. 机器学习挖掘类标签

该类标签通过机器学习挖掘产生，用于对用户的某些属性或某些行为进行预测判断。例如，根据一个用户的行为习惯判断该用户是男性还是女性、根据一个用户的消费习惯判断其对某商品的偏好程度。该类标签需要通过算法挖掘产生。

![](./picture/2/172.png)

###### 标签命名方式

![](./picture/2/216.png)

用户画像基础：需要了解、明确用户画像是什么，包含哪些模块，数据仓库架构是什么样子，开发流程，表结构设计，ETL设计等。这些都是框架，大方向的规划，只有明确了方向后续才能做好项目的排期和人员投入预算。这对于评估每个开发阶段重要指标和关键产出非常重要。
数据指标体系：根据业务线梳理，包括用户属性、用户行为、用户消费、风险控制等维度的指标体系。
标签数据存储：标签相关数据可存储在Hive、MySQL、HBase、Elasticsearch等数据库中，不同存储方式适用于不同的应用场景。
标签数据开发：用户画像工程化的重点模块，包含统计类、规则类、挖掘类、流式计算类标签的开发，以及人群计算功能的开发，打通画像数据和各业务系统之间的通路，提供接口服务等开发内容。
开发性能调优：标签加工、人群计算等脚本上线调度后，为了缩短调度时间、保障数据的稳定性等，需要对开发的脚本进行迭代重构、调优。
作业流程调度：标签加工、人群计算、同步数据到业务系统、数据监控预警等脚本开发完成后，需要调度工具把整套流程调度起来。
用户画像产品化：为了能让用户数据更好地服务于业务方，需要以产品化的形态应用在业务上。产品化的模块主要包括标签视图、用户标签查询、用户分群、透视分析等。
用户画像应用：画像的应用场景包括用户特征分析、短信、邮件、站内信、Push消息的精准推送、客服针对用户的不同话术、针对高价值用户的极速退货退款等VIP服务应用。

画像通常从八个维度组织标签，分别为：基本属性、平台属性、行为属性、产品偏好、兴趣偏好、敏感度、消费属性、用户生命周期及用户价值。

基本属性是指一个用户的基本社会属性和变更频率低的平台特征，例如真实社会年龄、性别、婚姻状况、昵称、号码、账号、lbs等标签。

平台属性是用户在平台上表现出的基本属性特征，是利用用户行为进行算法挖掘，标识用户真实属性的标签。

行为属性记录的是用户的全部单点行为，用户的单点行为包括启动、登录、浏览、点击、加车、下单等非常多，而且跟不同的产品，不同的模块交互，不同的时间窗选取，行为就更加复杂了，如何能够全面的梳理，可以按照“产品＊功能模块＊用户单点行为*时间”四大要素来组织。

产品偏好是对用户使用某些产品、产品核心功能或者其他渠道的偏好程度的刻画，属于挖掘型标签，其中产品的选取可以包括自家产品、竞品；功能和渠道包括站内产品功能，也包括push、短信、开屏、弹窗等几大运营和产品法宝。

兴趣偏好是用户画像内非常重要的维度，以电商产品为例，用户对商品的喜爱程度是用户最终的信息之一，兴趣偏好就是对用户和物品之间的关系进行深度刻画的重要标签，其中最典型的要属品牌偏好、类目偏好和标签偏好。

在营销活动时，我们留意到有些用户不需要优惠也会下单，而有些用户一定要通过优惠券刺激才会转化，优惠券的额度也影响了用户下单的金额，这种情况下，如何识别对优惠敏感的用户发放合理的券额的优惠券，保证优惠券不浪费，去报促销活动的ROI最大，其中一个很重要的标签就是用户的敏感度标签，敏感度代表用户对平台活动或者优惠的敏感程度，也是典型的挖掘类标签。

消费属性包括统计型标签——消费频次、消费金额、最近一次消费时间等，也包括挖掘型标签——消费能力和消费意愿，同时包含敏感度标签——优惠促销敏感度、活动敏感度、新品敏感度、爆款敏感度等。

用户生命周期是用户运营的重要法典，一个用户从进入产品到离开，通常会经历“新手”“成长”“成熟”“衰退”“流失”5个典型阶段，每个阶段对用户运营存在策略差异，画像在其中的作用是明确标记用户所处生命周期的阶段，便于后续业务同学落地。





在数据竞赛中，使用EDA完成数据分析的过程如下：

- 读取并分析数据质量；

- 探索性分析每个变量；

- - 变量是什么类型；
  - 变量是否有缺失值；
  - 变量是否有异常值；
  - 变量是否有重复值；
  - 变量是否均匀；
  - 变量是否需要转换；

- 探索性分析变量与target标签的关系；

- - 变量与标签是否存在相关性；
  - 变量与标签是否存在业务逻辑；

- 探索性分析变量之间的关系；

- - 连续型变量与连续型变量;

  - - 可视化：散点图、相关性热力图；
    - 皮尔逊系数；
    - 互信息；

  - 离散变量与离散变量；

  - - 可视化：柱状图、饼图、分组表；
    - 卡方检验；

  - 检查变量之间的正态性；

  - - 直方图；
    - 箱线图；
    - Quantile-Quantile (QQ图)；

根据EDA我们可以得出以下结论：

- 变量是否需要筛选、替换和清洗；
- 变量是否需要转换；
- 变量之间是否需要交叉；
- 变量是否需要采样；

![](./picture/2/272.png)