#### 时间序列回归模型

$$
y_t = \beta_0 + \beta_1x_{1,t}+\cdots+\beta_kx_{k,t}+\epsilon_t
$$

首先，我们假设预测变量和被预测变量之间的关系基本满足这个线性方程。其次，我们对误差项$(\epsilon_1,\cdots,\epsilon_T)$做出如下假设：

- 期望为零$E[\epsilon_t]=0$，$\sum_{t=1}^T\epsilon_t=0$；否则预测结果会产生系统性偏差。
- 随机误差项彼此不相关，$\sum_{t}\epsilon_t\epsilon_{t+1}=0$；否则模型存在自相关性，自相关性会使测得的标准差会偏小，会导致置信区间变窄。
- 与预测变量不相关，$\sum_{t=1}^T{x_{k,t}\epsilon_t}=0$；否则表明模型的系统部分中应该包含更多信息。
- 误差项$\epsilon_t$的方差为常数，$Var(\epsilon_t)=\sigma^2$；否则模型的残差存在异方差性，异常值重要性会被高估，异方差时，标准差和置信区间不一定会变大还是变小。
- 自变量$x_t, x_{t+1}$，相互独立，$\sum_{k}x_{k,t}x_{k,t+1}=0$；否则模型具有多重共线性，变量之间的相互关系会导致标准差偏大，置信区间变宽，岭回归或`ElasticNet`可以一定程度上解决多重共线性。

为了方便得到预测区间，我们还需要假设随机误差项服从方差为$\sigma^2$的正态分布，如果误差项不呈正态分布，意味着置信区间会变得很不稳定。

##### 最小二乘估计

可决系数$R^2$评价线性回归模型对数据的拟合程度
$$
R^2 = \frac{\sum{(\hat{y}_t}-y)^2}{\sum(y_t-\overline{y})^2}
$$
模型在测试集上的预测结果来衡量模型好坏比直接根据$R^2$大小来衡量模型更加有效。另外一个衡量模型拟合效果的指标是残差的标准偏差，通常称之为残差标准误差。

$$
\hat{\sigma}_e = \sqrt{\frac{1}{T-k-1}\sum_{t=1}^Te_t^2}
$$
其中， $k$是模型中预测变量的个数。

##### 回归模型的评估

###### 残差的自相关函数图

另一个用于检验残差自相关的效果较好的检验方法是`Breusch-Godfrey`检验。假如p值小于一个特定值，则表明残差中存在显著的自相关性。

###### 残差直方图

检查残差是否服从正态分布也是很有必要的。它对预测值并不重要，但它可以让我们更加容易的确定预测区间。

残差直方图、残差时序图、ACF图

时序图显示了不同时间下残差的变化，显然存在异方差性。这种异方差性会导致预测区间的不准确。

预测变量与残差关系图：期望残差是随机分布的并且不显示任何规律，还需要对没有加入到模型中的预测变量绘制其与残差的散点图。如果某个残差图显示出明显的规律，则需要将对应的预测变量加入到模型之中

残差与拟合值之间也应没有明显规律。如果观察到明显规律，则残差中可能存在“异方差性”，这意味着残差的方差不是固定的。如果出现异方差性，可能需要对预测变量做对数或者平方根变换



时间序列数据一般都是不平稳，不平稳的时间序列会导致伪回归。伪回归的特点是高$R^2$值和高残差自相关共存。

##### 预测变量的筛选

一个常见的但是不推荐的方法是画出被预测变量和特定的预测变量之间的关系图，如果不能看出明显的相关关系，则删除该预测变量。但这个方法常常会失效，尤其在未考虑其他预测变量时，散点图并不总能正确的反映两个变量之间的关系。

另一种常见的无效方法是对所有预测变量进行多元线性回归，并删除所有$p$值大于0.05的所有变量。统计显著性并不总能表示预测变量的预测价值。因为当两个或者多个预测变量相互关联时，$p$值可能会是错误的结果

对于$CV$，$AIC$，$AICc$和$BIC$准则，它们的值越小越好；而对于调整的可决系数$\overline{R}^2$，我们希望它尽可能的大。采用调整的可决系数可以解决以上问题：
$$
\overline{R}^2 = 1-(1-R^2)\frac{T-1}{T-k-1}
$$
最大化$\overline{R}^2$在筛选变量时一般会比较有效，但当变量数目太多时，效果往往会比较差。

对选择的预测变量使用经典的留一法交叉验证，该过程步骤如下：

1. 将$t$时刻的观测值从数据集中移出，用剩下的数据拟合出模型。然后计算$t$时刻观测值和预测值之间的误差$e_t^* = y_t-\hat{y}_t$。
2. 分别令$t=1,\cdots,T$，重复步骤一。
3. 计算$e_1^*,\cdots,e_T^*$的MSE 。我们称之为 **CV**。

赤池信息准则也是一个重要的方法，我们通常称之为 AIC 准则，其定义如下：
$$
AIC = T\times \log(\frac{SSE}{T}) + 2(k+2)
$$
其中，$T$是观测点的个数；$k$是预测变量的个数。

当$T$值较小时，$AIC$准则总是倾向于选择更多的预测变量，因此出现了修正的$AIC$准则，其定义如下：
$$
AIC_c = AIC + \frac{2(k+2)(k+3)}{T-k-3}
$$
施瓦茨的贝叶斯信息准则，通常被称为 BIC 、 SBIC 或 SC ，其定义如下：
$$
BIC = T\times \log(\frac{SSE}{T}) + (k+2)\log(T)
$$
与 AIC 一样，最小化 BIC 可以选择得到最佳模型。BIC 准则选择出的模型比 AIC 准则选择出的模型具有更少的预测变量。这是因为 BIC 准则对参数个数的惩罚力度更强。

被广泛用于模型筛选，但是由于它倾向于选择具有很多预测变量的模型，因此它并不适用于预测。测。在给定足够多数据情况下， BIC 准则能找出真正完美拟合这些数据的模型，因此统计学家更喜欢用 BIC 准则筛选模型。但是，很少有数据存在完美的模型，即使存在，该模型的预测结果也不一定准确。因此，我们建议使用$AIC_c$、 AIC 或 CV 准则的其中一个，因为他们都是以预测数据为目标。当$T$足够大时，它们会选择出相同的模型。




1.1.1  基础版
使用有标签的数据集训练得到自己最好的模型（可以是单个也可以是多个），然后对测试集进行预测；
筛选出测试集合中的高概率的预测样本（例如二分类中，我们选出预测概率大于0.99的样本，并标注为1）；
将伪标签样本加入模型一起训练，然后预测提交。
1.1.2  升级版
使用有标签的数据集训练得到自己最好的模型（可以是单个也可以是多个），然后对测试集进行预测；
筛选出测试集合中的高概率的预测样本（例如二分类中，我们选出预测概率大于0.99的样本，并标注为1）；
将伪标签样本加入模型一起训练再得到自己最好的模型，然后对测试集进行预测，再回到步骤2.
这个操作，如果有效的话，第一轮和第二轮还是较为明显的，但是一般第三轮之后效果就变化不大了
1.1.3  微调版本¶
每次筛选样本的时候，概率阈值较难定，而且影响很大，那么怎么办呢？
使用有标签的数据集训练得到自己最好的模型（可以是单个也可以是多个），然后对测试集进行预测；
不再做筛选，我们把所有预测的样本拿过来，重新训练，训练的时候，我们将loss修改为：loss(训练集合) + $\alpha$* loss(伪标签集合);微调$\alpha$，提交测试。














在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：

0. 离散特征的增加和减少都很容易，易于模型的快速迭代；
1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。



![](../../picture/线性回归.png)

| 名称                 | 说明                                                         |
| -------------------- | ------------------------------------------------------------ |
| `Dep.Variable`       | 就是因变量                                                   |
| `Model`              | 就是最小二乘模型，`OLS`                                      |
| `Method`             | 系统给出的结果是Least Squares。                              |
| `No. Observations`   | 样本量，就是输入的数据量。                                   |
| `Df Residuals`       | 残差自由度，其值= No.Observations - Df Model - 1，           |
| `Df Model`           | 模型自由度，其值=自变量的维度。                              |
| `Covariance Type`    | 协方差阵的稳健性                                             |
| `R-squared`          | 决定系数，其值=SSR/SST，其值越接近1，说明回归效果越好。      |
| `Adj. R-squared`     | 对R-squared进行修正。                                        |
| `F-statistic`        | F检验，值越大越能推翻原假设，原假设是“模型不是线性模型”      |
| `Prob (F-statistic)` | F-statistic的概率，值越小越能拒绝原假设，证明模型是线性显著。 |
| `AIC`                | 其用来衡量拟合的好坏程度，一般选择AIC较小的模型              |
| `BIC`                | 贝叶斯信息准则,BIC更倾向于选择参数少的简单模型               |
| `coef`               | 指自变量和常数项的系数                                       |
| `std err`            | 系数估计的标准误差                                           |
| `t`                  | t统计量，值越大越能拒绝原假设，原假设为系数为0。             |
| `P>|t|`              | 统计检验中的P值，这个值越小越能拒绝原假设。                  |
| `Omnibus`            | 基于峰度和偏度进行数据正态性的检验                           |
| `Durbin-Watson`      | 检验残差中是否存在自相关                                     |
| `Jarque-Bera(JB)`    | 基于峰度和偏度进行数据正态性的检验。                         |
| `Cond. No.`          | 多重共线性的检验，即检验变量之间是否存在精确相关关系或高度相关关系。 |

条件指数或条件数`condition number` 是$\mathbf{X}^{\prime}\mathbf{X}$矩阵的最大和最小特征根之比的平方根，条件指数高，表明存在多重共线性。临界值是30。

##### 模型后检验

###### 多重共线性

通过计算变量的`VIF`，可以检验共线性问题是否存在，通过综合考虑单变量或多变量的AR值判断应该保留哪些变量、剔除哪些变量。常用的检验方法主要有简单相关系数检验法、容限度法、方差扩大因子法、特征值和条件指数法、`Theil`多重共线性效应系数法等。

容限度是由每个自变量$X_j$作为因变量对其他自变量回归时得到的余差比例，即：$\text{Tolerance}_j=1-R_j^2$。其中，$R_j^2$表示第$j$个自变量对其他自变量进行回归得到的判定系数$R^2$。容限度很大时，$R_j^2$很小，说明所$X_j$包含的独立信息很多，可能成为重要解释变量；反之，容限度很小，$R_j^2$很大，说明$X_j$与其他自变量的信息重复性越大，其对因变量$Y$的解释能力越小。容限度的大小是根据研究者的具体需要制定的，通常当容限度小于0.1时，便认为变量$X_j$与其他变量之间的多重共线性超过了容许界限。

方差扩大因子是容限度的倒数。即：$VIF_j=1/Tolerance_j=1/(1-R_j^2 )$。它表示所对应的偏回归系数的方差由于多重共线性而扩大的倍数。一般认为：若$VIF>10$，说明模型中有很强的共线性关系；若条件指数值在10与30间为弱相关，在30与100间为中等相关，大于100为强相关。

 The condition `indices` are computed as the square roots of the ratios of the largest eigenvalue to each successive eigenvalue. Values greater than 15 indicate a possible problem with collinearity; greater than 30, a serious problem. 最大的特征值除以其他特征值后的平方根。

**VIF**的取值大于1，VIF值越接近于1，多重共线性越轻，反之越重。通常以10作为判断边界。当VIF<10,不存在多重共线性；当10<=VIF<100,存在较强的多重共线性；当VIF>=100, 存在严重多重共线性。

**容忍度**的值界于0至1之间，当容忍度值较小时，表示此自变量与其他自变量之间存在共线性。

容忍度~VIF的倒数

检验多重共线性，可查看分析结果中的VIF值。VIF>5说明存在共线性问题，VIF>10说明存在严重的多重共线性问题，模型构建较差，需要进行处理。

（2）对自变量进行相关分析，找出相关系数高的变量，手工移出后再做线性回归分析。

（3）采用逐步回归法，让系统自动筛选出最优分析项，剔除引起多重共线性的变量。

（4）如果不想涉及核心自变量，不希望剔除，使用岭回归

###### 自相关

残差独立性是线性回归方程的基本前提之一。D-W值可用于判断自相关性，判断标准是2附近即可(1.8~2.2之间)，如果达标说明没有自相关性，即样本之间并没有干扰关系。如有自相关问题时建议查看因变量Y的数据。

###### 残差正态性

残差正态性也是线性回归方程的基本前提之一。在分析时可保存残差项，然后使用“正态图”直观检测残差正态性情况。如果残差直观上满足正态性，说明模型构建较好，反之说明模型构建较差。如果残差正态性非常糟糕，建议重新构建模型，比如对Y取对数后再次构建模型等。

###### 异方差

方差齐性可以通过散点图来考察，在分析时可保存残差项，以模型自变量X或因变量Y为横坐标，残差值为纵坐标，作散点图。如果随着预测值的增加，残差值保持相同的离散程度，则说明方差齐。如果残差值随着预测值的增加而变宽或变窄，则说明有异方差问题。

关于异方差的检验上，是怀特(White)异方差检验和BP检验，通常情况下我们使用怀特(White)异方差检验即可。另外，处理异方差问题有三种办法，分别是数据处理、稳健标准误回归、FGLS回归（可行广义最小二乘法回归）。分别如下：

处理异方差问题有三种办法，分别是数据处理、稳健标准误回归、FGLS回归（可行广义最小二乘法回归）。

- 数据处理

- 针对连续且大于0的原始自变量X和因变量Y，进行取自然对数（或10为底对数）操作，如果是定类数据则不处理。取对数可以将原始数据的大小进行‘压缩’，这样会减少异方差问题。事实上多数研究时默认就进行此步骤处理，而不需要先异方差检验发现有异方差再进行处理。负数不能直接取对数，如果数据中有负数，研究人员可考虑先对小于0的负数，先取其绝对值再求对数，然后加上负数符号。
- Robust稳健标准误回归

- 如果检验显示有异方差问题，可使用Robust稳健标准误回归法进行研究。此种研究方法是当前最为流行也最为有效的处理办法。
- FGLS回归

- 如果发现有异方差问题，还可使用FGLS法进行分析，以处理异方差问题。FGLS是这样的一类思路，即对于残差值越大的点，给予越小的权重，从而解决异方差问题，FGLS回归事实上一系列数据处理的过程，并且它是一种思路。从分析上看，它依然还是使用OLS回归方法进行，具体在案例里面会详细讲解。



