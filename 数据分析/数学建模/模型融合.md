### 模型融合

#### 简单加权融合

回归：算术平均、几何平均

分类：投票

综合：排序融合、log融合

#### Boosting/Bagging

平均精度(Average-Precision，AP):  P-R曲线围起来的面积，通常来说一个越好的分类器，AP值越高。

平均精度均值(Mean Average Precision，mAP): 即是把每个类别的AP都单独拿出来，然后计算所有类别AP的平均值，代表着对检测到的目标平均精度的一个综合度量。

函数：评价函数、目标函数

方法：交叉验证方法、留一验证方法、针对时间序列的验证方法

秩相关系数，顾名思义，秩的相关系数。秩是指样本值的大小在全体样本从小到大排序后所占的次序。我们还要引入一个定义，对于一对数$(X_1, Y_1)$和$(X_2, Y_2)$，如果$X_1>X_2$且$Y_1> Y_2$或者$X_1<X_2$且$Y_1<Y_2$，则称$(X_1,Y_1)$和$(X_2, Y_2)$是一致的；如果$X_1>X_2$且$Y_1<Y_2$或者$X_1<X_2$且$Y_1>Y_2$，则称$(X_1,Y_1)$和$(X_2, Y_2)$是不一致的；如果$X_1=X_2$或$Y_1=Y_2$，则称$(X_1, Y_1)$和$(X_2,Y_2)$是一个`tie`。在评估逻辑回归模型的过程中，通常让逻辑回归中的目标变量作为X，逻辑回归的结果作为Y。记$n_c$为一致对的个数，$n_d$为不一致对的个数，$n_t$为$X$值不等而$Y$值相等的tie的个 数，$N$为观测值的个数（样本量），可以有以下几个秩相关系数： 

一致性指标：$c=\frac{n_{c}+0.5 n_{t}}{n_{c}+n_{d}+n_{t}}$

`Gini coefficient`：$D_{Y X}=\frac{n_{c}-n_{d}}{n_{c}+n_{d}+n_{t}}$

`Goodman-Krustal Gamma`：$\Gamma=\frac{n_{c}-n_{d}}{n_{c}+n_{d}}$

`Kendall`：$\tau=\frac{n_{c}-n_{d}}{N(N-1) / 2}$

同时，`Gini`也等于2倍的`AUC`减一，`AUC`为`ROC`曲线与坐标轴围成的面积。对于二分类问题，这几种计算方法是等价的。 

`ROC`：那么一个模型的特异度可以定义为`TNR=TN/(FP+TN)`，灵敏度可以定义为`TPR=TP/(TP+FN)`。而`ROC`曲线的横坐标是1-特异度=`1-TNR=FP/(FP+TN)=FPR`，纵坐标是灵敏度即`TPR`。`KS`曲线中的所谓“累积比率”其实就是`ROC`曲线中的`TPR`和`FPR`。`ROC`曲线以`FPR`为横轴，`TPR`为纵轴，而`KS`曲线以阈值为横轴，`TPR`、`FPR`为纵轴。所以说，从某种角度看，`ROC`曲线和`KS`曲线其实是一回事。 

$\text{KS(Kolmogorov-Smirnov)}$：KS用于模型风险区分能力进行评估， 指标衡量的是好坏样本累计分部之间的差值。 好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。KS的计算步骤如下： 计算每个评分区间的好坏账户数。 计算每个评分区间的累计好账户数占总好账户数比率和累计坏账户数占总坏账户数比率。 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值，然后对这些绝对值取最大值即得此评分卡的K-S值。

提升度曲线：可以衡量使用这个模型比随机选择对坏样本的预测能力提升了多少倍。通常计算`LIFT`的时候会把模型的最终得分按照从低到高，排序并等频分为10组，计算分数最低的一组对应的`累计坏样本占比/累计总样本占比`就等于`LIFT`值了。从直观上理解，累计坏样本占比相当于是使用模型的情况下最差的这一组能够从所有的坏样本中挑出多少比例的坏样本，而累计总样本占比等于随机抽样的情况下从所有坏样本抽取了多少比例的坏样本。

而对模型的分析部分，则可以帮助我们了解模型哪些数据做的好，哪些数据做的不好，通过此类反馈，我们就可以对错误的数据展开研究，挖掘我们所遗漏的部分，进一步提升我们模型的预测性能。

模型特征重要性分析：`LGB/XGB`等的`importance`、`LR、SVM`的`coeff`等；特征重要性可以结合业务理解，有些奇怪的特征在模型中起着关键的作用，这些可以帮助我们更好地理解我们的业务，同时如果有些特征反常规，我们也可以看出来；可能这些就是过拟合的特征等等；     

模型分割方式分析：可视化模型的预测，包括`LGB`的每一颗数等；这些可以帮助我们很好的理解我们的模型，模型的分割方式是否符合常理也可以结合业务知识一起分析，帮助我们更好的设计模型；       

模型结果分析：这个在回归问题就是看预测的结果的分布；分类一般看混淆矩阵等。这么做可以帮助我们找到模型做的不好的地方，从而更好的修正我们的模型。

根据loss对样本加权的工作就已经有很多。神奇的是，其实在一条线上有着截然相反的想法的研究：第一类工作的想法是如果一个样本训练得不够好，也就是loss高的话，那么说明现在的模型没有很好fit到这样的数据，所以应该对这样的样本给予更高的权重。这一类工作就对应到经典的Hard Negative (Example) Mining，近期的工作如Focal Loss也是这个思想。另一类工作的想法是学习需要循序渐进，应该先学习简单的样本，逐渐加大难度，最终如果仍然后Loss很大的样本，那么认为这些样本可能是Outlier，强行fit这些样本反而可能会使泛化性能下降。这一类中对应的是Curriculum Learning或者Self-Paced Learning类型的工作。本质上，这两个极端对应的是对训练数据本身分布的不同假设。第一类方法认为那些fit不好的样本恰恰是模型应当着重去学习的，第二类方法认为那些fit不上的样本则很可能是训练的label有误。

###### K-S检验

KS检验，是统计学中的一种非参数假设检验，用来检测单样本是否服从某一分布，或者两样本是否服从相同分布。在单样本的情况下，我们想检验这个样本是否服从某一分布函数$F_0(x)$，记$F_1(x)$是该样本的经验分布函数。我们构造KS统计量：$D_n=\max_x|F_1(x)-F_0(x)|$

经验分布函数与目标分布的累积分布函数的最大差值就是我们要求的KS统计量：95%置信度的KS统计量的临界值由$D_n=\frac{1.36}{\sqrt{n}}$。两样本的KS检验，95%置信度的临界值为$D_n=1.36\sqrt{\frac{1}{n_x}+\frac{1}{n_y}}$，如果我们根据样本得到的KS统计量的值小于$D_n$，那么我们就接收原假设。否则，拒绝原假设。

#### 类别不平衡学习

不平衡数据集上分类困难的原因：①过多的少数类样本出现在多数类样本密集的区域；②类别之间的分布严重重叠；③数据中本身存在的噪声，尤其是少数类的噪声；④少数类分布的稀疏性以及稀疏性导致的拆分多个子概念并且每个子概念仅含有较少的样本数量。①②③都归因为一个因素：噪声，④又被称为small disjuncts问题，在同样的特征空间中，相比于只有一个cluster的简单少数类分布，具有多个子概念的少数类分布需要模型给出更复杂的决策边界来获得良好的预测。在模型复杂度不变的情况下，分类性能会因子概念个数的增多而变差。因此该问题的解决办法也较为简单：上更大容量的模型

#####  处理方法

###### 异常检测

分类问题的一个隐含假设是各个类别的数据都有自己的分布，当某类数据少到难以观察结构的时候，我们可以考虑抛弃该类数据，转而学习更为明显的多数类模式，而后将不符合多数类模式的样本判断为少数类，某些时候会有更好的效果。此时该问题退化为异常检测问题。

###### 数据级方法

数据级方法是不平衡学习领域发展最早、影响力最大、使用最广泛的一类方法，也可称为重采样方法。该类方法关注于通过修改训练数据集以使得标准学习算法也能在其上有效训练。根据实现方式的不同，数据级方法可被进一步分类为：

1. 从多数类别中删除样本的方法（欠采样，如RUS、NearMiss、ENN、Tomeklink等）

2. 为少数类别生成新样本的方法（过采样，如SMOTE，ADASYN，Borderline-SMOTE等）

3. 结合上述两种方案的混合类方法（过采样+欠采样去噪，如SMOTE+ENN等）

标准的随机重采样方法使用随机方法来选择用于预处理的目标样本。然而随机方法可能会导致丢弃含有重要信息的样本（随机欠采样）或者引入无意义的甚至有害的新样本（随机过采样），因此有一系列更高级的方法，试图根据根据数据的分布信息来在进行重采样的同时保持原有的数据结构。

 Edited Nearest Neighbours(ENN)：对于属于多数类的一个样本，如果其K个近邻点有超过一半都不属于多数类，则这个样本会被剔除。这个方法的另一个变种是所有的K个近邻点都不属于多数类，则这个样本会被剔除。

- NearMiss-1：选择到最近的K个少数类样本平均距离最近的多数类样本
- NearMiss-2：选择到最远的K个少数类样本平均距离最近的多数类样本
- NearMiss-3：对于每个少数类样本选择K个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围

优点：该类方法能够去除噪声/平衡类别分布：在重采样后的数据集上训练可以提高某些分类器的分类性能。欠采样方法减小数据集规模：欠采样方法会去除一些多数类样本，从而可能降低模型训练时的计算开销。

缺点：采样过程计算效率低下；易被噪声影响；过采样方法生成过多数据；不适用于无法计算距离的复杂数据集：

###### 算法级方法

算法级方法专注于修改现有的标准机器学习算法以修正他们对多数类的偏好。在这类方法中最流行的分支是代价敏感学习，我们在此也只讨论该类算法。代价敏感学习给少数类样本分配较高的误分类代价，而给多数类样本分配较小的误分类代价。通过这种方式代价敏感学习在学习器的训练过程中人为提高了少数类别样本的重要性，以此减轻分类器对多数类的偏好。

优点：不增加训练复杂度；可直接用于多分类问题

缺点：需要领域先验知识：必须注意的是，代价敏感学习中的代价矩阵（cost matrix）需要由领域专家根据任务的先验知识提供，这在许多现实问题中显然是不可用的；不能泛化到不同任务：对于特定问题设计的代价矩阵只能用于该特定任务，在其他任务上使用时并不能保证良好的性能表现；依赖于特定分类器

###### 集成学习方法

集成学习类方法专注于将一种数据级或算法级方法与集成学习相结合，以获得强大的集成分类器。它们中的大多数基于某种特定的集成学习算法并在集成的过程中嵌入一种其他的不平衡学习方法。另有一些集成学习方法的基学习器也是集成学习器。因此最终的分类器是一个“集成的集成”。 “集成的集成”并不代表一定会有更好的表现，作为基学习器的集成学习方法也会影响分类性能。

优点：效果通常较好；可使用迭代过程中的反馈进行动态调整

缺点：包含所使用的不平衡学习方法的缺点；过采样+集成进一步增大计算开销；对噪声不鲁棒

if you want to undersample or oversample your data you should not do it before cross validating. Why because you will be directly influencing the  validation set before implementing cross-validation causing a "data leakage" problem.

#### Stacking/Blending





##### 描述统计

描述统计是通过图表或数学方法，对数据资料进行整理、分析，并对数据的分布状态、数字特征和随机变量之间关系进行估计和描述的方法。描述统计分为集中趋势分析、离中趋势分析和相关分析三大部分。

1. 集中趋势分析：集中趋势分析主要靠平均数、中数、众数等统计指标来表示数据的集中趋势。例如被试的平均成绩多少？是正偏分布还是负偏分布？

2. 离中趋势分析：离中趋势分析主要靠全距、四分差、平均差、方差（协方差：用来度量两个随机变量关系的统计量）、标准差等统计指标来研究数据的离中趋势。例如，我们想知道两个教学班的语文成绩中，哪个班级内的成绩分布更分散，就可以用两个班级的四分差或百分点来比较。

3. 相关分析：相关分析探讨数据之间是否具有统计学上的关联性。这种关系既包括两个数据之间的单一相关关系，也包括多个数据之间的多重相关关系；既包括A大B就大(小)，A小B就小(大)的直线相关关系，也可以是复杂相关关系（A=Y-B*X）；既可以是A、B变量同时增大这种正相关关系，也可以是A变量增大时B变量减小这种负相关，还包括两变量共同变化的紧密程度——即相关系数。

4. 推论统计：它以统计结果为依据，来证明或推翻某个命题。具体来说,就是通过分析样本与样本分布的差异，来估算样本与总体、同一样本的前后测成绩差异，样本与样本的成绩差距、总体与总体的成绩差距是否具有显著性差异。

###### 信度分析

信度即可靠性，它是指采用同样的方法对同一对象重复测量时所得结果的一致性程度。 信度指标多以相关系数表示，大致可分为三类：稳定系数（跨时间的一致性），等值系数（跨形式的一致性）和内在一致性系数（跨项目的一致性）。信度分析的方法主要有以下四种：**重测信度法、复本信度法、折半信度法、α信度系数法。**

- 重测信度法编辑：这一方法是用同样的问卷对同一组被调查者间隔一定时间重复施测，计算两次施测结果的相关系数。显然，重测信度属于稳定系数。重测信度法特别适用于事实式问卷，如性别、出生年月等在两次施测中不应有任何差异，大多数被调查者的兴趣、爱好、习惯等在短时间内也不会有十分明显的变化。如果没有突发事件导致被调查者的态度、意见突变，这种方法也适用于态度、意见式问卷。
- 复本信度法编辑：让同一组被调查者一次填答两份问卷复本，计算两个复本的相关系数。复本信度属于等值系数。复本信度法要求两个复本除表述方式不同外，在内容、格式、难度和对应题项的提问方向等方面要完全一致，而在实际调查中，很难使调查问卷达到这种要求，因此采用这种方法者较少。
- α信度系数法编辑：α信度系数是目前最常用的信度系数，其公式为：$α=\frac{k}{k-1}\times (1-\sum{\frac{S_i^2}{S_T^2}})$。其中，K为量表中题项的总数， $S_i^2$为第i题得分的题内方差， $S_T^2$为全部题项总得分的方差。从公式中可以看出，属于内在一致性系数。这种方法适用于态度、意见式问卷的信度分析。总量表的信度系数最好在0.8以上，0.7-0.8之间可以接受；分量表的信度系数最好在0.7以上，0.6-0.7还可以接受。Cronbach 's alpha系数如果在0.6以下就要考虑重新编问卷。

分类：外在信度：不同时间测量时量表的一致性程度，常用方法重测信度。内在信度：每个量表是否测量到单一的概念，同时组成两表的内在体项一致性如何，常用方法分半信度。



###### 相关分析

研究现象之间是否存在某种依存关系，对具体有依存关系的现象探讨相关方向及相关程度。

1. 单相关：两个因素之间的相关关系叫单相关，即研究时只涉及一个自变量和一个因变量；

2. 复相关 ：三个或三个以上因素的相关关系叫复相关，即研究时涉及两个或两个以上的自变量和因变量相关；

3. 偏相关：在某一现象与多种现象相关的场合，当假定其他变量不变时，其中两个变量之间的相关关系称为偏相关。

###### 方差分析

使用条件：各样本须是相互独立的随机样本；各样本来自正态分布总体；各总体方差相等。

- 单因素方差分析：一项试验只有一个影响因素，或者存在多个影响因素时，只分析一个因素与响应变量的关系
- 多因素有交互方差分析：一项实验有多个影响因素，分析多个影响因素与响应变量的关系，同时考虑多个影响因素之间的关系
- 多因素无交互方差分析：分析多个影响因素与响应变量的关系，但是影响因素之间没有影响关系或忽略影响关系
- 协方差分析：传统的方差分析存在明显的弊端，无法控制分析中存在的某些随机因素，使之影响了分析结果的准确度。协方差分析主要是在排除了协变量的影响后再对修正后的主效应进行方差分析，是将线性回归与方差分析结合起来的一种分析方法。

> **横型诊断方法**
>
> 残差检验：观测值与估计值的差值要跟从正态分布
>
> 强影响点判断：寻找方式一般分为标准误差法、Mahalanobis距离法
>
> 共线性诊断：诊断方式：容忍度、方差扩大因子法(又称膨胀系数VIF)、特征根判定法、条件指针CI、方差比例，处理方法：增加样本容量或选取另外的回归如主成分回归等

数据转换的方式有：

- 数据归一化(MinMaxScaler)；
- 标准化(StandardScaler)；
- 对数变换(log1p)；
- 转换数据类型(astype)；
- 独热编码(OneHotEncoder)；
- 标签编码(LabelEncoder)；
- 修复偏斜特征(boxcox1p)等。



 

惯例： `scikit-learn estimator`遵守以下惯例：

 

\- 除非显式指定数据类型，否则所有的输入数据都被转换成 `float64`

\- 回归问题的输出被转换成 `float64`；分类问题的输出不被转换

\- `estimator`的参数可以更新：

 \- `estimator.set_params(...)`方法可以显式更新一个`estimator`的参数值

 \- 多次调用`estimator.fit(...)`方法可以隐式更新一个`estimator`的参数值。最近的一次训练学到的参数会覆盖之前那次训练学到的参数值。

 Dataset transformations

Transformers, which may clean, reduce, expand or generate feature representations. Like other estimators, these are represented by classes with a fit method, which learns model parameters from a training set, and a transform method which applies this transformation model to unseen data. `fit_transform` may be more convenient and efficient for `modelling` and transforming the training data simultaneously.

