



在拿到数据之后，我们必须要明确以下几件事情： 

1. 数据是如何产生的，数据又是如何存储的； 
2. 数据是原始数据，还是经过人工处理（二次加工的）； 
3. 数据由那些业务背景组成的，数据字段又有什么含义； 
4. 数据字段是什么类型的，每个字段的分布是怎样的； 
5. 训练集和测试集的数据分布是否有差异；

在分析数据的过程中，还必须要清除的以下数据相关的问题： 

1. 数据量是否充分，是否有外部数据可以进行补充； 
2. 数据本身是否有噪音，是否需要进行数据清洗和降维操作； 
3. 评价函数是什么，和数据字段有什么关系； 
4. 数据字段与的标签的关系；



- 分析数据字段两两之间，或者三者之间的关系； 
- 从其他维度分析数据

 

#### 单变量分析

单变量分析：包括每个变量的分布，缺失情况等；通过单变量分析，我们可以进一步的了解每个变量的分布情况，是否有无用的变量，是否出现了某些分布奇怪的变量等.   

一般可以通过 `data.describe()` 获取基本的描述性统计，根据均值、标准差、极大极小值等指标，结合变量含义来判断。再接着看不同类别之间的特征值分布情况，可通过画直方图（数值型变量）和计算变量值占比分布（类别变量）来观察。观察不同变量之间的相关性情况，可以通过绘制 相关矩阵的热力图 来观察大体情况。

![](../../picture/work/34.png)

| 名称   | 定义                                                         | 原因                                                         | 影响                                                         |
| ------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 缺失值 |                                                              | 有些信息暂时无法获取，或者获取信息的代价太大、有些信息被遗漏、属性值不存在 | 数据挖掘建模将丢失大量的有用信息；数据挖掘模型所表现出的不确定性更加显著，模型中蕴含的规律更难把握；包含空值的数据会使建模过程陷入混乱，导致不可靠的输出。 |
| 异常值 | 异常值是指样本中的个别值，其数值明显偏离其他的观测值。异常值也称为离群点 | 检验数据是否有录入错误，是否含有不合常理的数据               |                                                              |

##### 异常值分析

3σ原则：假设一组检测数据只含有随机误差，对原始数据进行计算处理得到标准差，然后按一定的概率确定一个区间，认为误差超过这个区间的就属于异常值。要求数据服从正态分布或近似服从正态分布

箱线图：判断异常值的标准以四分位数和四分位距为基础。异常值通常被定义为小于$Q_L-1.5\times \text{IQR}$或大于$Q_U-1.5\times \text{IQR}$值。对数据分布不作任何限制性要求。

对于分类问题可以分组查看，从而判断异常值。

数据不一致性是指数据的矛盾性、不相容性。直接对不一致的数据进行挖掘，可能会产生与实际相违背的挖掘结果。在数据挖掘过程中，不一致数据的产生主要发生在数据集成的过程中，可能是由于被挖掘数据来自于不同的数据源、对于重复存放的数据未能进行一致性更新造成的。

##### 变量分布

![](../../picture/2/272.png)

![](../../picture/1/293.png)

当样本数据表明质量特征的分布为非正态时，应用基于正态分布的方法会作出不正确的判决。约翰逊分布族即为经约翰变换后服从正态分布的随机变量的概率分布，约翰逊分布体系建立了三族分布，分别为有界$S_B$ 、对数正态$S_L$和无界$S_U$。

```python
y = Train_data['price']
plt.figure(1); plt.title('Johnson SU')
sns.distplot(y, kde=False, fit=st.johnsonsu)
plt.figure(2); plt.title('Normal')
sns.distplot(y, kde=False, fit=st.norm)
plt.figure(3); plt.title('Log Normal')
sns.distplot(y, kde=False, fit=st.lognorm)
```

很多模型假设数据服从正态分布，数据整体服从正态分布，样本均值和方差则相互独立。当样本不服从正态分布时，可以做如下转换：

- 线性变化z-scores：基于原始数据的均值和标准差进行数据的标准化。将A的原始值x使用z-score标准化到x’

- Boxcox变换：一种广义幂变换方法，是统计建模中常用的一种数据变换，用于连续的响应变量不满足正态分布的情况。

  ![](../../picture/2/86.png)

- yeo-johnson变换：是幂变换的方法之一，通过构建一组单调函数对随机变量进行数据变换。

![](../../picture/2/85.png)

- skeness：衡量随机变量概率分布的不对称性，是相对于平均值不对称程度的度量，通过对偏度系数的测量，我们能够判定数据分布的不对称程度以及方向，目前有两种计算方式。

$$
b_1 = \frac{\frac{1}{n}\sum_{i=1}^n(x_i-\overline{x})^3}{[\frac{1}{n-1}\sum_{i=1}^n(x_i-\overline{x})^2]^{3/2}}\\
g_1=\frac{\sqrt{n(n-1)}}{n-2}\frac{\frac{1}{n}\sum_{i=1}^n(x_i-\overline{x})^3}{[\frac{1}{n}\sum_{i=1}^n(x_i-\overline{x})^2]^{3/2}}
$$

- kurtosis：研究数据分布陡峭或平滑的统计量，通过对峰度系数的测量，我们能够判定数据相对于正态分布而言是更陡峭/平缓。有两种计算方式。

$$
g_2 =\frac{\frac{1}{n}\sum_{i=1}^n(x_i-\overline{x})^4}{[\frac{1}{n}\sum_{i=1}^n(x_i-\overline{x})^2]^2}-3\\
G_2 =\frac{n(n+1)}{(n-1)(n-2)(n-3)}\frac{\sum_{i=1}^n(x_i-\overline{x})^4}{[\sum_{i=1}^n(x_i-\overline{x})^2]^2}-3\frac{(n-1)^2}{(n-2)(n-3)}
$$

![](../../picture/1/294.png)

`Omnibus`检验：$K=Z_1(g_1)^2+Z_2(g_2)^2$，式中$Z_1$和$Z_2$是两个正态化函数，$g_1$和$g_2$则分别是偏度和峰度.

`Jarque-Bera`检验：$JB=\frac{n}{6}(g_1^2+\frac{1}{4}g_2^2)$

两个检验的原假设$H_0$:数据是正态分布的。备择假设$H_1$：数据不是正态分布。

##### 训练和测试集分析

训练集与测试集的分布分析：寻找差异大的变量，这些差异大的变量往往是导致线下和线上差异的核心因素，这有利于我们更好的设计线下的验证方法。在时间序列上核心变量(数值型)的`boxplot`走势；在时间序列上核心变量(字符型)的frequency走势；核心变量的数据重复程度；核心变量的数据缺失程度；核心变量的数据离散程度；最后总结数据质量表现由好到坏的指标

#### 多变量分析

 

##### 离散值与连续值  

##### 连续型变量与连续型变量

- 可视化：散点图、相关性热力图；
- 皮尔逊系数；
- 互信息；
- sns.pairplot()：展示变量两两之间的关系（线性或非线性，有无较为明显的相关关系）。

##### 离散变量与离散变量

- 可视化：柱状图、饼图、分组表；
- 卡方检验；



##### 变量正态性检验

- 直方图；
- 箱线图；
- Quantile-Quantile (QQ图)；

#### 数据类型

特征变量为连续值：如果为长尾分布并且考虑使用线性模型，可以对变量进行幂变换或者对数变换。

特征变量为离散值：观察每个离散值的频率分布，对于频次较低的特征，可以考虑统一编码为“其他”类别。

##### 数值特征

- 相关性分析：对两个或多个具备相关性的变量元素进行分析，从而衡量两个变量因素之间的相关密切程度。

- 特征的偏度和峰度

- 数字特征分布可视化

- - pd.melt()：处理数据，透视表格，可将宽数据转化为长数据，以便于后续分析。形成的数据即为，键：各特征名称，值：特征对应的值
  - sns.FacetGrid() ：先sns.FacetGrid()画出轮廓,再map()填充内容

##### 类别特征

- unique分布：对于一维数组或者列表，unique函数去除其中重复的元素，并按元素由大到小返回一个新的无元素重复的元组或者列表。

- 可视化：

- - 箱型图可视化：直观识别数据中的离群点，判断数据离散分布情况，了解数据分布状态。
  - 小提琴图可视化：用于显示数据分布及概率密度，这种图表结合了箱形图和密度图的特征，主要用来显示数据的分布形状
  - 柱形图可视化类别
  - 每个类别频数可视化

##### Pipelines and composite estimators

###### Pipeline: chaining estimators

All estimators in a pipeline, except the last one, must be transformers, 等价于must have a transform method. The last estimator may be any type. The Pipeline is built using a list of `(key, value)` pairs, where the `key` is a string containing the name you want to give this step and `value` is an estimator object

```python
estimators = [('reduce_dim', PCA()), ('clf', SVC())]
pipe = Pipeline(estimators)
# The utility function make_pipeline is a shorthand for constructing pipelines
make_pipeline(Binarizer(), MultinomialNB())
pipe.steps[0]; pipe[0]; pipe['reduce_dim']
pipe.named_steps.reduce_dim is pipe['reduce_dim']

pipe.set_params(clf__C=10) 
param_grid = dict(reduce_dim__n_components=[2, 5, 10],
                  clf__C=[0.1, 10, 100])
grid_search = GridSearchCV(pipe, param_grid=param_grid)
```

 The estimators of a pipeline are stored as a list in the `steps` attribute, but can be accessed by index or name by indexing the Pipeline. Pipeline’s `named_steps` attribute allows accessing steps by name with tab completion in interactive environments. A sub-pipeline can also be extracted using the slicing notation commonly used for Python Sequences, although only a step of 1 is permitted. This is convenient for performing only some of the transformations. Parameters of the estimators in the pipeline can be accessed using the `<estimator>__<parameter>` syntax. Calling `fit` on the pipeline is the same as calling fit on each estimator in turn, transform the input and pass it on to the next step. The pipeline has all the methods that the last estimator in the pipeline has. if the last estimator is a classifier, the Pipeline can be used as a classifier. If the last estimator is a transformer, again, so is the pipeline.

##### `FeatureUnion`: composite feature spaces

`FeatureUnion` combines several transformer objects into a new transformer that combines their output. A `FeatureUnion` takes a list of transformer objects. During fitting, each of these is fit to the data independently. The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix.

```python
estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]
combined = FeatureUnion(estimators)
```

`FeatureUnion` serves the same purposes as Pipeline - convenience and joint parameter estimation and validation. `FeatureUnion` and Pipeline can be combined to create complex models.
A `FeatureUnion` has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are the caller’s responsibility. A `FeatureUnion` is built using a list of `(key, value)` pairs, where the key is the name you want to give to a given transformation and `value` is an estimator object.

##### Column Transformer for heterogeneous data

The `ColumnTransformer` helps performing different transformations for different columns of the data, within a Pipeline that is safe from data leakage and that can be parametrized. `ColumnTransformer` works on arrays, sparse matrices, and pandas `DataFrames`. To each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method.

```python
X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],
     'title': ["His Last Bow", "How Watson Learned the Trick",
               "A Moveable Feast", "The Grapes of Wrath"],'expert_rating': [5, 3, 4, 5],
     'user_rating': [4, 5, 4, 3]})
'''In the above example, the CountVectorizer expects a 1D array as input and therefore the columns were specified as a string ('title'). However, preprocessing.OneHotEncoder as most of other transformers expects 2D data, therefore in that case you need to specify the column as a list of strings (['city']).'''
column_trans = ColumnTransformer([('city_category', OneHotEncoder(dtype='int'),['city']),
     ('title_bow', CountVectorizer(), 'title')],remainder='drop')

column_trans.fit(X) 
column_trans.get_feature_names()
column_trans.transform(X).toarray()
```

Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, or a boolean mask. Strings can reference columns if the input is a `DataFrame`, integers are always interpreted as the positional columns.

We can keep the remaining rating columns by setting `remainder='passthrough'`. The values are appended to the end of the transformation:

```
column_trans = ColumnTransformer([('city_category', OneHotEncoder(dtype='int'),['city']),
     ('title_bow', CountVectorizer(), 'title')], remainder='passthrough')
```

 The `remainder` parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation.

```python
column_trans = ColumnTransformer([('city_category', OneHotEncoder(), ['city']),
     ('title_bow', CountVectorizer(), 'title')], remainder=MinMaxScaler())
```

##### Feature extraction

The `sklearn.feature_extraction` module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.
Feature extraction is very different from Feature selection: the former consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning. The latter is a machine learning technique applied on these features.

###### loading features from `dicts`

`DictVectorizer` implements what is called one-of-K or “one-hot” coding for categorical features. Categorical features are “attribute-value” pairs where the value is restricted to a list of discrete of possibilities without ordering

