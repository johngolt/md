

##### 调参方法

贪心调参 （坐标下降）

坐标下降法是一类优化算法，其最大的优势在于不用计算待优化的目标函数的梯度。最容易想到一种特别朴实的类似于坐标下降法的方法，与坐标下降法不同的是，不是循环使用各个参数进行调整，而是贪心地选取了对整体模型性能影响最大的参数。参数对整体模型性能的影响力是动态变化的，故每一轮坐标选取的过程中，这种方法在对每个坐标的下降方向进行一次直线搜索

网格调参

作用是在指定的范围内可以自动调参，只需将参数输入即可得到最优化的结果和参数。

贝叶斯调参

贝叶斯优化通过基于目标函数的过去评估结果建立替代函数（概率模型），来找到最小化目标函数的值。贝叶斯方法与随机或网格搜索的不同之处在于，它在尝试下一组超参数时，会参考之前的评估结果，因此可以省去很多无用功。



#### stats

数据变换：`box-cox`, `yeo`

数据截断：箱型：`sigmaclip`；分位数:`trimboth`, `trim1`

##### 正态检

| 正态检验                             | `null hypothesis`                      |
| ------------------------------------ | -------------------------------------- |
| `probplot`                           |                                        |
| `normaltest, kurtosistest, skewtest` |                                        |
| `anderson`                           | 样本服从某个特定的分布                 |
| `shapiro`                            | 样本取自正态分布                       |
| `jarque_bera`                        | 样本的偏度和峰度是正态分布             |
| `ks_1samp, kstest`                   | 样本的分布$F(x)$与给定的分布$G(x)$一致 |



| 方差检验          | null hypothesis                    |
| ----------------- | ---------------------------------- |
| `kruskal`         | 不同群组的中位数相等               |
| `obrientransform` | 将不同方差的数据集变换为相同方差的 |
| `bartlett`        | 所有输入样本均来自方差相等的总体   |
| `levene`          | 所有输入样本均来自方差相等的总体   |
| `flingner`        | 所有输入样本均来自方差相等的总体   |

Levene检验 与Bartlette检验(巴特莱多)的区别：

1）对于正态分布的样本，Bartlette检验极其灵敏，但是对于非正态分布的样本，检验非常不准确；

2）Levene检验是一种更为稳健的检验方法，既可用于正态分布的样本，也可用于非正态分布的样本，同时对比较的各组样本量可以相等或不等；

3）两者的检验原理不同，Bartlette检验是对原始数据检验其方差是否齐性，而

Levene检验是检验组间残差是否齐性，而且一般认为要求残差的方差齐性

方差齐性是t检验和方差分析的前提条件



因为第三范式模型和维度模型都能够用E-R图来描述，我们才尽量避免将符合第三范式的模型称作E-R模型，而是将其称作规范化模型来防止混淆。
事实表在设计时，每个外键都必须和对应维度表的唯一主键相匹配，也就是说，事实表的外键不应当为空，因为如果为空，就违背了参照完整性。






有些模型的损失函数对数值大小是敏感的，即变量间的数值大小本身是有比较意义的，如逻辑回归，SVM等，我们暂将其称为A类模型；有些模型本身对数值变化不敏感，数值存在的意义更多的是为了排序，树模型

如果使用的是A类模型，所有的类别变量必须做one hot encoding，因为label encoding的赋值是没有数值含义的。
如果使用的是B类模型，并且是有序变量，则优先使用label encoding，且赋值要与顺序一致。



3.3 数据探索

主要包括四个方面：数据整体认知、数据质量分析、数据统计量分析、数据分布分析
3.3.1 数据整体认知

所谓的整体认知就是，研究训练集、测试集、表数、记录数、用户数、变量数、变量类型、变量属性值、标签等内容，然后绘制实体-关系图。实体-关系图（E-R图）包括三个组成部分：实体、属性、关系。学过数据库的同学应该熟悉E-R图这个概念。
3.3.2 数据质量分析

所谓数据质量分析，就是分析数据的缺失值、重复值、异常值、歧义值、正负样本比例（样本不平衡）等特性。由于这些东西都会影响到模型的学习效果，所以在后面做数据预处理时需要做相应的工作。
3.3.3 数据统计量分析

数据统计量分析主要分为三块：

第一，对单个变量的统计分析。比如考察单个变量的均值、中位数、众数、分位数、方差、变异系数等。常用的工具有：直方图、箱线图、小提琴图等。

第二，对两个变量的统计分析。这里主要考察的是两个变量统计分布之间的关系。常用的工具包括散点图、相关性分析图、热力图等。

第三，对多个变量的统计分析。可以使用彩色的散点图

3.3.4 数据分布分析

数据分布分析指的是考察某个字段或某些字段的统计分布。包括频数、时间、空间三个方面。

频数统计。用概率论的语言讲叫累积分布函数CDF。比如在IJCAI2018阿里妈妈国际广告算法大赛中，我们就统计了不同点击次数下各有多少用户。基于类似的累积分布函数图我们就可以知道用户行为的分布情况，进而可以帮助我们充分理解数据。

时间维度上的统计分布。我们可以观察事件发生的趋势和周期性，这里会涉及不少时间序列的知识。比如下图所示的“每天的点击数趋势”，就是在时间维度上考察点击数的变化情况。

空间维度上的统计分布，我们可以寻找某个变量在地理位置上的相关关系。比如2020年以来我们十分熟悉的疫情地图，就是一种空间上的分布分析。

另外，以上三种分析常常结合分组or聚类方法，对细分的业务场景进行考察，为后面的数据建模做铺垫。

3.3.5 数据探索小结

对于数据探索，我总结了几个需要牢记在心的关键点，分别是：对比，分组，频数，抓大放小和可视化。

所谓对比，指的是在做数据探索时，考虑对比训练集不同样本之间的特征分布，还要考虑对比训练集和测试集中每一个特征的分布。

所谓分组，就是在做数据探索时，常常用到按类别标签、某个离散变量的不同取值groupby后的sum、unique。

所谓频数，就是要注意考察并自行计算某些变量的概率累积分布。诸如“事件发生次数”这样的的统计量需要自己计算；有时还要关注“同id下某个事件多次发生”的统计。

所谓抓大放小，就是对于那些特征重要性较高的变量，要做重点分析。因为这些变量对你模型预测能力的影响是较大的。

所谓可视化，就是建议大家在做数据探索的时候多画图（尤其是各种趋势图、分布图），图形给人的冲击力往往是要大于数字本身的。

传统意义上的数据预处理一般包括数据清洗、数据集成、数据重采样、数据变换（特征编码）和数据规范化（特征缩放），这一块的内容也是网络上流传最广、介绍最多的数据分析技巧，因此我不做具体的解释，仅仅把各自常用的处理技巧罗列一下。

数据清洗

缺失值插补
均值、中位数、众数插补
固定值插补
最近邻插补
离群值
直接删除
替换法
异常、冗余值
直接删除
小技巧：用训练集数据学习一个模型，然后用它预测训练集的标签，删除预测结果偏差较大的样本
数据集成

多表数据整合
一对一
一对多
多对一
多对多
数据重采样

滑窗法：
对于时间序列数据，选取不同的时间窗间隔，可以得到多份训练数据集
该方法可以增加训练样本，也方便做交叉验证实验
非平衡重采样：调整正负样本量
欠采样
过采样
组合采样
数据变换

连续变量离散化（分箱）
等频
等宽
聚类
离散变量编码
One-hot Encoding
Label Encoding
长尾分布
Ln、Log
数据规范化

Min-Max
Z-score
MaxAbs



人行征信报告
一类是贷款信息；另一类是逾期信息。贷款信息能帮助我们了解当前客户的大致信贷情况，我们可以整理出以下特征：
总贷款笔数、未结清贷款笔数、结清贷款笔数、首笔贷款发放距今月份、最近一笔贷款发放距今月份、同期最大还款金额、是否有房贷、是否有车贷、是否有经营性贷款、是否有消费性贷款等。

对于逾期信息这块，因为历史严重逾期与多次逾期的客户均无法走到授信的环节，所以在这里能用到的特征相对要偏过程一些，比如历史最长逾期月数、单月最高逾期总额、总逾期次数等
贷记卡携带的信息中，包含了客户信用卡的额度，所以对于客户的授信提供了很强的依据，所以这个模块比较重要，可以梳理出以下特征：单家行最高授信额度、单家行最低授信额度、平均授信额度、现行贷记卡最高额度、近6个月平均使用额度、最大使用额度等。


地址的种类

· 家庭地址：通常为用户的住宅地址，可能是自有住房，也可能是租用住房；

· 公司地址：通常为用户的工作地址，例如xxx大厦，xxx园区；

· 户籍地址：户口本/身份证上的地址。

· 收货地址/订单地址：点外卖，网购时，要求送货员/快递员送货的地址。

· 定位地址：通过GPS或其他技术手段定位发生动作时的地址。

· 卡寄地址：办理信用卡时邮寄卡片的接收地址；

· 出险地址：车祸发生的地址。

模糊地址:为了骗过系统，绕过风控规则，不法分子采用模糊地址的方式进行交易。
特殊地址字符：为了对抗各个平台的风控规则，不法分子采用繁体字、错别字、地址中间夹杂特殊字符以分割关键词等方式进行攻击。

更改地址和地址排序：针对小贷申请场景，通常会让用户填写几个常用地址。用户可以通过调整常用地址次序来获得更高的评分；



确定预测目的

申请信用评分卡的预测目的自然是申请客户信用面的逾期概率，通常一个客户逾期缘由可以从大方向分为两种，一种是还款意愿，一种是还款能力。申请信用评分卡主要评估客户的还款能力，但有时意愿和能力的界限并没有想象的那么清晰，通常还款能力不足的样本还款意愿也较低。

恶意欺诈一般认为是贷款人的还款意愿不足导致的，尤其是恶意逾期，即在前几期就开始大量逾期的样本，所以信用评分的样本需要考虑是否要将恶意逾期的样本放进来。

确定样本范围

上一步确定预测目的后，我们就需要框定我们的样本范围，通常在一个公司中，你会遇到不同产品，不同渠道进来的申请。所以你需要确定，这些不同的样本之间在风险上和数据上有没有差异。如果差异较大，建议做多个模型，适用不同的场景。

目标样本的选择

上一步框定样本的范围后，就可以开始进行样本的选择，通俗点来说，就是获取好坏样本，所以这里要基于具体情况对好坏样本下一个合理的定义。这里的定义通常与好、坏样本的逾期天数，好样本的还款比例有关。

Vintage是在信贷行业风控中主要观测资产质量变化趋势的图标。通过Vintage可以：
观测资产的最终损失比例，帮助划分资产进行差异化定价，提前控制风险；
了解不同时段放款资产的变化趋势，观测资产逾期上升的趋势，便于提前对相应的风险进行控制；
观测已经放款资产的月度变化情况。关注整体进件资产的客群资质变化，便于对贷前或贷中策略进行调整

贷前风控架构策略设计的主要流程分成5个步骤：
熟悉信贷产品类型（明确核查审批的对象；了解对标产品和审批对象在信贷行业审批中的通用流程；评估信贷产品对应潜在客群是否存在明显风险点；明确产品目标客群的范围）、了解进件流程（收集进件流程中可获得的进件要素，如姓名、身份证、手机号等；寻找进件流程中的风险点，制定对应的风险排查方法）
根据明确的审批对象，制定主体策略模块
对应主体策略模块寻找风险解决方法
根据确认的策略模块，设计审批流程
确认审批流程落地的方案

对于具体策略规则的制定，主要分为5个核心步骤：
确定有效目标规则变量
对目标变量做风险分布分析，判断是否有风险区分度，初步确定可行的策略或规则
策略上线前预估对生产的影响
策略上线后监控是否有偏差，以及是否完全执行
上线后对有表现的策略规则进行数据分析，分析策略调整后的进件量、批核率及贷后表现



强相关数据是指能直接推算用户收入情况的数据，主要包括信用卡额度、房贷、公积金等。弱相关数据是指能间接反映用户的收入水平，可用于模型中预测用户的收入和还款能力，主要包含反映用户消费能力的信用卡账单、电商购物、运营商账单，反映用户个人信息的手机信息、学历、自填收入、是否有车、地址等。
信用卡数据主要包含两部分，一部分是信用卡额度，另一部分是信用卡账单。信用卡额度反映了银行对用户的授信情况，是推算收入的强相关数据。信用卡账单中的额度使用率、消费金额等则体现了用户的消费水平，可用于预测用户的收入和还款能力。
用户如果有房贷，则说明该用户是一个有房用户，资质不会太差。其次通过月房贷还款额则可推算出用户的收入情况。
公积金数据和收入是直接相关的，是银行对用户进行授信时常用的数据。并且公积金数据除了能推算收入的高低，还能反映用户收入的稳定情况。

电商购物数据能很好的反映用户日常消费水平。通过电商消费数据我们可以挖掘用户最大单笔支出金额、月平均消费水平、购物类别偏好，消费稳定性等特征，帮助我们预测用户的收入和还款能力。
运营商数据经常用于反欺诈环节，在网时长大于6个月通常是准入的必备条件，运营商每月的账单数据和充值数据从某种角度上体现用户的消费能力，可用于收入的预测。
用户的手机品牌可以很好的反映用户的消费能力，为此我们可以对不同的手机品牌和手机价格做一个映射关系，用户后续手机品牌特征的开发。另外如果用户的手机上安装了大量的借贷赌博类app，则该用户是一个急缺钱的偏坏用户，往往还款能力也较低。这些特征都可以用于收入和还款能力的预测。

学历和收入从整体上来来看还是正相关。
自填收入，自填收入的绝对值的确是不准确的，但是相对数值还是有可参考价值的，即对自填收入进行分箱后，还是会保留Ranking价值。
有车用户相比于无车用户还是会更优质一些，从某种程度上体现了用户的消费收入水平。
地址可以分为身份证地址、工作所在地地址、家庭地址等，这些都和收入有着较弱的相关性。
公积金月缴纳额=工资base✖️不同城市的缴纳比例✖️2（个人+公司）如果有用户的公积金月缴纳额，那么我们是不是能反推出相应的工资base，为了收入预测更加精确，我们需要对不同的城市构建一张不同的缴纳比例映射表。

对于小行来说，完全可以利用大行的授信额度对新用户进行授信，并此基础上给与相应的提额，即减少了风控成本也提高了吸引力
对于信贷机构来说，完全可以通过用户的信用卡额度来推算用户的收入，例如通过去掉大额信用卡后取平均或者取平均后除以相应的系数作为用户的收入。

在按揭的时候，每月还款额一般不能超过月收入的50%，因此利用未结清房贷的月还款额✖️相应的系数便能推算出相应的收入水平。

当我们只有反映用户消费水平和个人信息的弱相关数据的时候，我们可以通过建模的方式来预测用户的收入。
针对收入预测，最理想的方法是通过回归的方法来预测收入的具体值，这就要求必须有大量无偏样本的收入数据进行训练支持，但是大量的收入数据通常是比较难获取的，这个时候我们可以通过二分类的方法进行解决。
根据收入数据确定一个cutoff，从而将收入分为高和低

根据作用的时间段不同，M0（DPD1-30）模型可以细分为缓催响应模型和贷后N天流转模型：

缓催响应模型：部分入催用户可能是遗忘造成的逾期，因此不做提醒或者只要稍作提醒（短信、邮件等）就会很快进行还款，这部分人就可以进行缓催，因为不进行催收动作，他们也会还款。缓催响应模型主要用于预测哪些人适用于缓催，这可以降低人力成本，将更多的人力用于后续阶段的催收工作中。对于缓催模型的响应时间，应该根据相应时间段内的出催率变化来定义。
贷后N天流转模型：去除掉缓催人群后，我们需要预测入催人群中哪些人的出催概率较高，那么在人力有限的情况下，我们可以先对出催概率高的人群进行催收，提高出催率。例如我们定义缓催期为m天，则可以把入催样本中，m天后还没有出催的样本作为训练样本，构建贷后m天流转模型，去除缓催样本的影响，使模型更多的学习m天后才出催的样本信息。因为不同时间段出催的样本信息存在差异性，为了精细化操作，在M0阶段可以构建多个贷后流转模型，例如5天缓催响应模型+贷后5天流转模型+贷后15天流转模型等（各自模型的应用时间应视各个公司样本数据而决定）。
在M0阶段的入催用户，大部分还是会出催。但是当用户从M0变成M1+，那么意味着该用户出催的概率将大大降低。因此若无特殊原因影响，发生过M1+的用户需要我们重点关注。在之前的催收策略里，M1+阶段用户常常会进行委外处理，通常能保持不低的出催率。



1. DAU：Daily Active User 日活跃用户量。统计一日（统计日）之内，登陆或使用了某个产品的用户数（去重）
2. WAU：Weekly Active Users 周活跃用户量。统计一周（统计日）之内，登陆或使用了某个产品的用户数（去重）
3. MAU：Monthly Active User 月活跃用户量。统计一月（统计日）之内，登陆或使用了某个产品的用户数（去重）
4. DNU：Day New User 日新增用户，表示当天的新增用户
5. DOU：Day Old User 日老用户。当天登陆的老用户，非新增用户
6. ACU：Average Concurrent Users 平均同时在线人数
7. PCU：Peak Concurrent Users 最高同时在线人数
8. UV：Unique Visitor 唯一访问量，即页面被多少人访问过
9. PV：Page View 页面浏览量，即页面被多少人看过
10. ARPU：Average Revenue Per User 平均每个活跃用户收益。
11. ARPPU：Average Revenue Per Paying User 平均每个付费用户平均收益。统计周期内，付费用户对产品产生的平均收入。
12. LTV：Life Time Value 生命周期价值。产品从用户所有互动中获取的全部经济收益的总和
13. CAC：Customer Acquisition Cost 用户获取成本
14. ROI：Return On Investment 投资回报率。
ROI=利润总额/投入成本总额*100%
15. GMV：Gross Merchandise Volume 成交总额。是指下单产生的总金额
CMV=销售额+取消订单金额+退款金额
16. 支付UV：下单并成功支付的用户数
次日留存率：（第一天的激活用户中，在第2天还登录的用户数）/第一天激活用户数
7日留存率：（第一天的激活用户中，在第8天还登录的用户数）/第一天激活用户数
30日留存率：（第一天的激活用户中，在第31天还登录的用户数）/第一天激活用户数
一般用户留存会经历三个阶段，即高速流失期、缓慢流失期、平缓稳定期。
高速流失期：这个阶段一般是新增的激活用户，流失比较严重的阶段。这个阶段用户第一次接触到产品，对产品还没有认知，最主要的留存方法就是要引导用户，让用户感受到产品带给他的核心价值和帮助她解决痛点的能力。
缓慢流失期：这个阶段用户对产品有了一定的了解，虽然流失还在下降，但开始减缓。这个阶段需要给到用户新的留存的理由和新的刺激，常常有如下以下方法
（1）构建用户激励机制，激励机制就是给用户继续使用的好处。
（2）增加用户沉没成本，所谓沉没成本就是已经发生的不可回收的成本，比如金钱、时间、精力等，他会影响人们的当前决策，人做决策时候不仅仅看当前的利益，也关注以前的付出成本。增加用户沉没成本的目的就是通过提前的成本支出，让用户持续地使用我们的产品，提升用户留存。
金钱成本，比如提前充值，理发店经常会让用户办会员卡，就是为了提前绑定用户。
（3）个性化推荐，所谓个性化推荐也就是千人千面，根据不同的用户进行精准化的推荐，越精准用户越容易产生购买，越容易喜欢，让用户觉得产品是真正懂他的。
平缓稳定期：用户已经对产品比较熟悉，养成了习惯，用户基本较少的流失。这个阶段要防止用户流失，提高留存最重要的方法就是进行产品的更新迭代，开发创新的功能和玩法。





数据指标分为三大类：综合性指标、流程性指标、业务性指标。
综合性指标是能体现产品目前综合情况的指标。
流程性指标是指与用户操作行为相关的指标。
点击率：有PV点击率和UV点击率，一般使用PV点击率。
转换率：下一步操作用户数/上一步操作用户数
流失率：（上一步用户数-下一步用户数）/上一步用户数
完成率：完成率相对于转化率而言，是最终的结果数值。转化率是过程值，完成率是结果值。
业务性指标是跟产品业务相关的指标。例如视频网站，则可能需要的业务指标有：视频播放数、人均观看时长、人均播放数、播放率等。





贷前（申请与审批），贷中（监控，调整额度），贷后（从放款到全部收回），催收管理(出现逾期的处理）。
贷后和贷中有一点业务重叠，就是“监控”这个任务。但贷中更侧重发生逾期前的额度调整，贷后更侧重逾期发生后的催收，如果实在催收不成功，则坏账准备
除此之外，还可以构建异常行为特征。总结下在如果需要手动构建特征，一般可以从几个方面入手：
近期类特征。例如最近一次使用信用卡的时间，上一次联系时间，上次逾期时间等。
频率类特征：普通用户为了完成某一任务，通常目的达到动作就会跟着终止，所以正常用户的动作通常是离散和稀疏的。而对于黑产用户而言，为了实现收益的最大化，高频率的动作是降低成本的核心，所以往往黑产用户的动作是连续而紧密的。基于这种理念，频率类的策略在风控中就有着非常重要的作用。考虑频率特征时，通常考虑几个因素，分别是：时间窗口、资源、运算与阈值。
统计类（速度类或者方差类）特征：如果客户某个行为呈现加速状态或者减速状态，那么可能其风险等级也发生改变。例如过去六个月的平均消费金额为1000，但本月消费金额为15000，则显示出了十几倍的加速，这种情况可能说明客户在恶意透支信用额度，更可能是被盗刷。这个速度类特征也可以用方差类特征来代替，例如本次消费金额在过去六个月所有消费里，是否处于 图片 之外。
地理位置的异常偏移：比如ip的归属地和设备的定位差距过大，地理位置变化的速度太快。这说明用户可能通过作弊手段修改定位，使用代理ip等等。这个特征在反盗刷和反欺诈很重要。

分解到贷前中后三个阶段，分别有（行为类特征）：

贷前可构建这些特征特征：
如果客户在本行已有记录，则可以观测用户近期还款情况，上一次逾期时间。如果有信用卡，则观测近三个月内平均逾期天数比过去十二个月，是否变得更少。例如
如果客户在本行没有记录，则可以从征信信息观测其行为序列。例如过去几个月其贷款笔数是多了还是少了，其贷款总额的增速是多少。
如果是通过互联网银行/手机银行申请的，可以检查数据埋点的行为序列。我由于是给传统零售银行做业务的，所以我并没有使用过埋点数据。
贷中特征（凭着个人回忆写的，未能穷尽，欢迎补充）：
本笔信用卡消费在过去六个月所有消费里，是否处于 图片 之外。如果是的话，有被盗刷风险
分期还款笔数的增加或者减少（和用户经济负担有关，如果用户的卡里有着多个商品的分期付款，例如iphone 12的24期付款，那么再用分期付款买一个MacBook，那么经济负担就更重了，很可能还款能力就变低了）
额度使用度的变化。这个特征并不是单一方向的。例如信用卡有10000块额度，比如你所说每个月额度都刷满并且按时还款，那么银行可能会认为客户信用额度不够就给你提额，但也可能认为你在套现，给你降额。至于这个特征怎么决定客户的信用度，则需要结合其他特征来观测。(如果你工资卡也是这个银行的，那么它可以看你工资卡收入是否有变多）。又如果客户有一万块额度，之前六个月平均每个月花1000，并且能按时还上，这个月花了9999，并且逾期了，则说明这个客户可能在授信时额度给多了，需要调整额度。
消费习惯，负债变化。履约历史等。这里推荐


贷后特征：
客户本次逾期距离上次逾期的时间。（如果上个月逾期了这个月还是逾期，说明客户的情况并没有好转，考虑优先催收，如果距离上次逾期都一年了，说明这个客户并不是经常逾期，即使不催收也可能自己会还）
最近联系记录。由于客户逾期会触发金融机构的催收部门联系客户，所以联系记录也可以作为特征。例如最近一个月被电联的次数vs过去六个月平均每个月被电联次数（说明催收难度大了，客户的经济情况也许在恶化），过去一个月电话接通率vs过去六个月电话接通率（越来越难接通有可能说明客户在恶意欠款）

在应用RFM模型时，要有用户最基础的交易数据，至少包含用户ID，交易金额，交易时间三个字段。



信贷业务通过用户授权获取征信报告，基于征信报告了解用户的信用历史，通过分析用户的行为时序来理解用户的真实现金流需求。

常见的征信查询包括信用卡审批、贷款审批等，此类查询表征了用户在该时刻的资金需求，通过征信报告中贷款发放情况匹配贷款查询申请时间，可以分析用户资金的用信行为。传统金融行业常利用诸如基于不同时间滑动窗口的加工逻辑方式去进行统计，包括过去一个月三个月、六个月、十二个月，二十四个月征信报告查询次数等、过去一个月、三个月贷款发放笔数等指标。

征信报告还包括用户的公司地址变更、公积金变更等信息，将上述信息基于时间轴进行划分，就可以对用户在一段时间内的信贷需求和用信情况进行刻画分析。我们利用深度神经网络去进行分析，通过记录时间点、该时间点的动作、该动作的类型以及该动作的多个特征值，将每一项信息转为Item作为输入提供给LSTM单元，获取该单元的输出同时又作为下一个时间序列的输入，不断学习从而获得更好的信息表达。
信贷贷中行为数据主要用于B卡的建模，在整个客户生命周期线上，用户存在不断借钱、还钱的循环往复行为。基于此，在每个动作发生的时间切片上，可以生成诸如总额度、剩余本金、本次动作类型、本次金额、距离下个还款日多少天、下个还款日应还金额、剩余日均应还金额、提前还款次数、逾期次数等特征，将这些特征组织成item，通过各类RNN网络能够显著提升B卡的能力。
为了实现模型的高可解释性，我们常用的方案是将底层的高维变量按其来源进行分类，先利用复杂模型进行加工形成子模型特征，主要有以下几个方面：

利用用户的各类APP行为数据输出兴趣分、文化分等；

利用用户的大量支付消费数据输出收入分、消费分等；

利用用户的大量位置移动数据输出工作分、稳定分等；

利用用户的外部共债信息数据输出多头分、信用分等；

利用用户的详细征信报告数据输出信用分、负债分等；

利用用户的大量运营商数据输出欺诈分、信用分等；

基于上述多个子模型评分，再利用逻辑回归或简单决策树构建最终的模型。由于每个低维变量评分和人的认知接近，且与风险有很好的线性相关性，整个模型就有了很好的解释性。同时，对模型质量的监控也变得更加容易了。当模型出现问题时，可以很容易定位到可能出现问题的子模型分，然后再依据子模型评分去寻找对应的底层数据的异常。
不同概率水平的样本进行训练，得到的模型预测均值差异较大，这对策略应用的同学带来很大挑战。基于此，我们需要对不同的模型修正对齐真实的不良率，具体步骤如下：

按预测值排序分段，如100段；

对每段的真实逾期率取logit；

对每段内的所有预测值取logit后求平均；

尝试一次或二次曲线拟合该散点图；

参考诸如FICO的评分方式进行转化信用评分。

经过上述标准化后，模型与样本数据集或样本不良率无关，模型的升级就对策略、业务同事更加透明。
评分稳定性包括分布稳定性、性能稳定性以及微观层面的稳定性三方面：
分布稳定性：指的是将用户评分分段排序后按月进行统计，符合正常的正态分布情况，各个分数段的人数占比各月较为稳定，即整个客群的质量未发生较为明显的变化；

性能稳定性：指的是将用户不良率分段排序后按月进行统计，能够满足各个分数段对应的人数不良率稳定；
单一用户评分的稳定性：若用户存在新的借款、放款行为，则未来贷款风险会提高；若用户还清所有贷款，余额为0，则未来一段时间内风险会降低。模型对于用户短期行为反应较为灵敏，虽然整体客群排序稳定，但对于单一具体用户而言，各月之间的评分波动较大，策略就会难以管理。从策略层面上而言，模型不论是整体还是局部方面，都希望其稳定性较高，以便于对用户的评价相对稳定，这样才能更好地进行客户管理。

风险成本：

风险成本的计算需要考虑还款方式、客群、期限，对于还款方式，同样的年化和资金投入，在不同还款方式上的风险成本是不一样的
基础成本：

【资金成本】不同机构的资金成本差距较大，机构内部的资金有限，由于业务规模扩张，各机构都有不同方式的增资，持牌消金资金成本在4%~7%左右，助贷或者联合放款在7%~10%左右；

【运营成本】比如营销推广成本、三方支付成本等属于产品业务运营的成本

【人力成本】一般是涵盖催收人员或者线下产品的信审人员成本
 边际成本：

【获客成本】不同渠道不同合作模式的获客成本差距较大，大致可以分为固收和分润两种，当机构自主风控能力较强，有自营类的产品时，才会有按照点击等付费方式。

【数据成本】客户申请或者贷中管理查询三方数据源的成本，有时也归为信审成本，有包年和单笔两种付费方式，在策略制定中如何测试评估数据源，以及调用优化也是一项非常重要的工作，已达到有效利用数据源并控制数据成本。
违约率（PD）是指债务人无偿付债务的能力，发生了呆账，本息超过90天未支付，实际在工作中，违约率计算口径较为灵活，不一定必须设定为90天才算违约。
违约风险暴露敞口（EAD）是指客户违约时的暴露的风险敞口，不仅包含本金的剩余贷款余额，也包括利息、罚息，及违约后产品定义的收取的其他费用。
违约损失率（LGD）是指客户违约后发生损失的比率，由于LGD与贷款回收密切相关，通常用1-回收率计算估计，在不考虑折现以及催收费用情况下，回收率就是回收金额除以违约后的风险暴露




风险建模是一项侧重技术应用的标准流程工作，大致分为“需求立项”、“模型设计”、“样本准备”、“特征工程”、“变量筛选“、
”模型开发“、“模型确定与文档撰写、”部署上线、‘’模型管理”这9个主要环节。
第一罪：需求不全
模型确定立项之前，需要启动立项背景调查阶段，背调主要目的有两个：一方面确定模型开发意义，
另一方面帮助模型开发小组充分了解数据业务背景。

通常背调内容框架包括：
工作目标
模型适用范围
基本面分析
数据的可获取性分析

仅以工作目标和基本面分析为例说明。

工作目标可以借鉴以下三点，但不同评分模型开发目的不一致，仅做参考：

通过开发评分模型研究XX银行客户特征，预测和评判申请人风险。
协助XX银行在申请处理环节对风险进行分类，决定哪些申请可以批准，哪些需要拒绝以及哪些需要人工审批。
根据申请人的评分分布，判定其不同的风险等级，从而决策对于可以批准的账户应给予何种贷款条件。

基本面分析大致分为“业务基本面分析“、”监控报表分析“、”业务逾期分析“。其中，业务基本面分析可以分析各风险表现指标
随时间的分布变化、业务下各产品随时间变化的申请量、通过率以及各逾期指标分布；业务逾期分析可以从以下4点入手：

各逾期指标随时间的分布
各逾期指标流转（fpd30-60；fpd60-90）
vintage：ever 30+／M1+；ever 60+ / M2+；ever 90+/ M3+
All产品、各产品30天逾期指标分布
第二罪：目标不准
模型设计环节最重要的一环是目标变量（好坏）的定义。如果评分模型预测对象定位不准，整个工程相当于提前宣判失败。

好/坏定义用于将客户行为特征同随后的客户表现联系起来。好/坏定义会根据产品稍有不同。常用于推导好/坏定义的主要信息
项如下：

逾期状态（如逾期期数）
核销标志等

推导好/坏定义也有一套“非标推导方法”，主要结合Roll Rate和Vintage进行好坏定义推导，流程说明如下：

对于评分卡目标变量Y的界定，我们主要从Roll Rate和Vintage来观察分析
第三罪：大而不全
数据准备阶段，没有经验的建模工程师常会犯的一个错误是数据样本“不足够多”。不论是用于筛选的变量特征，还是建模样本，在我们开始进行特征工程前，一定希望建模宽表大而全，在之后环节再做减法。

举个例子，某银行的征信数据的逾期记录时间点与实际逾期时间点发生严重滞后现象，此时建模人员考虑排除征信特征变量，以免当评分模型的组成变量中包含征信变量后出现模型不准现象。

这种分析逻辑本身没错，但对于数据准备环节，大而全是我们的出发点，无征信模型与有征信模型本身效果未定情况下，为何不在一开始保留可用的征信特征呢？

大而不全，一旦因开发模型指标效果不佳时，极易造成重新进入建模工程前期再准备阶段，费时费力。
第四罪：信息错配
错配数据信息常常发生在数据处理阶段，一旦发生错配数据信息情况，一个变量的数据就没办法代表真实业务中应该存在的意义，由此特征变量构建的模型就会出现与业务逻辑不一致的问题。

在现实中，建模人员常常会遇到特征集存在不同程度的缺失，缺失原因多种多样。处理特征值缺失的方式，对数据分析与模型训练有直接影响，因此需要格外重视这一过程。

特征值缺失常见原因有如下4种：

原始数据缺失导致无法计算相关特征
无意义计算
特征值内涵超出定义
数据可采集但存在异常，导致无法计算相关特征

对于以上常见四种情况，特征值缺失本身包含了不同的信息。如果我们忽视了特征值缺失或作出不当处理（如简单填补0），建模工作将会受到极大负面影响，主要体现在：

用于建模的特征值丢失大量有用信息
包含空值的特征对模型拟合过程产生影响，导致不可靠输出
最终模型表现的不确定性与不稳定性更加明显，更难分析模型结果

为了保证特征值保留足够的信息，区分不同缺失情况，我在工作中主要采用以下两种方法：

1.基于数据逻辑的固定值填充
2.基于特征构造的虚拟变量引入

第五罪：时空穿越
特征时发生数据时间穿越，我们会得到“用户搜索浏览苹果手机壳后”大概率“会购买苹果手机”的错误结论。结果导致模型效果在训练阶段被高估很多，实际上线后效果一落千丈。
第六罪：算法为王
数据和特征的质量是决定模型最终效果的核心因子，算法只是帮助我们将模型效力无限逼近于最大值，前者是雪中送炭，后者是景上添花。

我想，并不需要改变对前沿技术的向往本心，但回归商业本身，业务导向才是建模人应该踏地而行的第一步。
第七罪：虎头蛇尾
模型开发之后，交付及验收环节是常被人忽视的一环，很多建模人认为模型开发出来后，没必要花大量时间与精力写几十页的模型交付验收报告。其实，模型交付标准化报告与交付验收流程，对于模型管理尤为重要。

模型管理主要包括”监控“、”微调“、”更新“、”重新培训“、”更换和启动“。当同时管理成百上千个风控模型时，不可能每一个模型出现短期震荡时就进行重新开发工程。如何最优化发挥每一个模型的效能，甚至避免一些周期性因素导致的短期模型效能失效，则成为模型管理的核心。

做好模型管理的第一步，就是要有模型交付标准化报告和严谨的交付验收流程。

模型交付标准化报告可以包括如下7个模块，仅供参考：

报告框架
概述
评分模型设计内容与关键定义
评分模型的开发
评分模型的验证
评分模型使用介绍和具体规则
附录

理论上来说Linear Regression模型的输出概率可以认为是真实概率，而其他分类器的输出概率并不反映真实概率。
接下来我们需要将模型预测结果先排序，再分箱计算各个桶的逾期率，找到满足盈亏平衡点要求的预测结果阈值，低于阈值通过贷款申请，反之则拒绝贷款申请。

模型换了，cutoff随之变化，分数X也跟着变了。同时每次更换模型，后台、前端都需要进行相应的逻辑、页面修改，每次模型发布需要多个环节协作完成，是一种高耦合的工作方式。
分段尺度变换，将cutoff 固定为 680分（本文假定的）然后分成两段作尺度变换。
模型切换用户无感
无论模型的cutoff如何调整，用户感知不到差别，只要分数超过680都可以成功申请到贷款；
解耦了模型团队与开发团队
也就是说当模型人员校准好评分后，后台开发只需要设定680分通过，从此以后无论模型人员怎么更换模型，后台开发都不用再重新修改代码。
随着用户的复贷次数增多，原有盈亏平衡点的计算方式会使得拉新成本被计算了多次（用户复贷几次就多计算几次）。

这也就引出了用户分层的一种场景：区分新老客户，分别建模，分别计算盈亏平衡点。

传统的评分卡中A卡B卡也有异曲同工之处：

A卡用于贷前审批阶段对借款申请人的量化评估，是使用最广泛的；
B卡用于贷中阶段，增加了借款人的还款及交易行为，预测借款人未来的还款能力和意愿；

综合分析新老用户分别建模的特点：

模型更准确
老客户建模引入了历史还款行为、交易行为，使得模型更准确。
老用户的成本更低
拉新成本归属于边际成本，当新用户转化为老用户后，每多一次复贷都会将拉新成本摊平。那么就可以将拉新成本算在新用户上，老用户的成本得以降低，对逾期率有更大的容忍度；
提高复贷率，提高盈利
修正老客户的盈亏平衡点，可以提高老客户的通过率，从而提高复贷率，提高盈利。
除了按照新老客户性质分层，还可以按照用户质量来划分用户。

将老用户分为优质用户和次优用户，这样做的目的是为优质用户匹配优质产品，更低利率或更高额度。

随之而来的问题也就出现了：

如果产品越来越丰富，变换额度、利率，所需要计算的盈亏平衡点越来越多，那怎么办呢？
再使用分段尺度变换这种简单粗暴的方法就显得越来越笨拙了，也增加了出错的概率。
当多个模型共同工作或切换备用模型时，如何保证分数尺度一致？
这里尺度一致的意思是，两个模型分数相同时对应的逾期率是否一致。
多产品、多客群、多模型下数映射的产生——分数校准

为了能够更好的进行风险定价，业务部门希望模型给出的分数能够准确的反映出真实的信用风险等级。

这一过程称为分数校准(Score Calibration)。

也就是说，我们最终的目标是：建立信用评估分数与预测逾期率的函数关系。



假如客户群体发生了变化，那么变化的类型可以分为四类
1）坏客户的评分朝着高分段偏移而好客户的评分朝着低分段偏移（见上图a）。反映到实际的情况是全部客户的评分均值变小，且好客户和坏客户的评分均值之差也变小。

分析：这种变化是导致模型预测能力下降最常见和最主要的原因。因为坏客户和好客户分布的交叉区域变大，意味着模型的排序能力降低，从而导致模型的区分度下降。发生这类变化的原因有可能是宏观经济恶化导致客户整体的还款能力下降，或者公司业务转型导致目标客户发生变化，或者公司业务团队在某段时间内集中某一类的客户过度营销，或者数据质量不稳定的原因。

2）坏客户的评分朝着低分段偏移而好客户的评分朝着高分段偏移（见上图b）。反映到实际的情况是全部客户的评分均值变大，且好客户和坏客户的评分均值之差也变大。

分析：第二种变化的结果是改善型的，模型的区分度不仅没有下降，反而比以前更高了，实践中几乎不可能发生。

3）坏客户和好客户的评分一起朝着高分段偏移；反映到实际的情况就是全部客户的评分均值变大，但好客户和坏客户的平分均值之差不变。

分析：这种变化相当于评分阈值的被动下调，从而导致提高了违约率，提升了通过率，但是模型的排序能力变化不大。

4）坏客户和好客户的评分同时朝着低分段偏移；反映到实际的情况就是全部客户的评分均值变小，但好客户和坏客户的平均均值之差不变。

分析：这种变化相当于评分阈值的上调，从而降低了通过率和违约率，但是模型的排序能力变化不大。
对于第二种变化，我们无需做任何调整。对于第三和第四种变化，我们只需要相应调整评分阈值。但是对于第一种变化，调整评分阈值无能为力，因为这是模型排序能力变化导致的。

是不是遇到第一类情况就一定要更换模型了呢？有没有其他的应对措施呢？

首先我们得分析导致客户分布发生变化的原因！

第一类是从时间切片的角度统计分类客户的PSI。

例如我们通过计算每个月末的分类客户PSI值，发现某一类客户的PSI值连续发生较大变化，我们可以单独分析该类客户PSI变化的原因，如果是该类客户所在的行业变动，地区性灾难（地震、洪水、疫情）等短期内不可逆的因素，建议将类似客户拒绝进件。若是公司营销部门针对某一类客户过度营销，建议与业务团队沟通优化业务方向。

第二种是从特征角度考虑，即整体客群整体好坏比不变的情况下客群结构发生变化，此时可以考虑重新调整个别特征的分箱。

如果在客群变化的情况下，特征的每个分箱的好坏比与模型开发时候的好坏比变化了，那么每个分箱WOE也就发生变化，从而影响模型的排序能力，因此如果能够将特征的分箱重新调整，使得新的分箱内的坏好比恢复到和模型开发时候一样，那就恢复了模型的排序能力。



T+1的概念来源于股票交易，是一种股票交易制度，即当日买进的股票，要到下一个交易日才能卖出。
在一些银行、中大型消金公司会由数据中心、数据部门，将大量的数据清洗并做结构化处理，后续给到策略部、模型开发部门进行分析、建模。

当然也有一些公司，策略部门、模型部门直接接触第三方数据、同业数据，所以我们需要对数据进行预处理、清洗，对数据的平衡度进行采样。

其中count encoder，one-hot encoder，label encoder主要针对低基数无序特征，比如性别。针对高基数无序特征，比如地区，邮编等，可以采用target encoder或者mean encoder的方法。
常规的target encoder目标编码代码容易造成过拟合，因此我们引入5折交叉验证的形式改进编码，即将样本分为5块（fold），每一fold中的该高基数无序特征类别由其它4个fold中的类别对应标签平均值替换表示


在深度学习中，经常会存在需要特征融合的地方[1]，而最基本的融合方法无非是：(1) 按点逐位相加(point-wise addition) 和 (2) 进行向量拼接(concatenate)。


使用LIMIT <M> OFFSET <N>可以对结果集进行分页，每次查询返回结果集的一部分；
分页查询需要先确定每页的数量和当前页数，然后确定LIMIT和OFFSET的值。

特征间的单位（尺度）可能不同，为了消除特征间单位和尺度差异的影响，以对每维特征同等看待，需要对特征进行归一化。
通过对特征进行zero-mean and unit-variance变换后，其损失函数的等高线图更接近圆形，梯度下降的方向震荡更小，收敛更快
feature scaling的方法可以分成2类，逐行进行和逐列进行。逐行是对每一维特征操作，逐列是对每个样本操作

SKU全称为Stock Keeping Unit(库存量单位)，即库存进出计量的基本单元，可以是以件，盒，托盘等为单位。
因此我们可以将所有其他不在样本里的类别分组为一个全部包罗的“其他”类别，称为 OOV（out-of-vocabulary）分桶。



帕累托分析法又称ABC 分类法，平常也称之为「80 对 20」规则，常用于商品的库存管理分析中。
把产品或业务分为A、B、 C三类，用于分清业务的重点和非重点，反映出每类产品的价值对库存、销售、成本等总价值的影响，从而实现差异化策略和管理。
分析思路：
1）计算不同品类商品累计销售额及其占比，
2）按照累计销售占比将品类分成几类，将品类按照累计销售额占比：
0-70% 1类；
70%-90% 2类；
90%-100% 3类


 针对数据晚到，还有一种方法，就是拆分，ETL作业拆分的越细，可以让先到的先跑，后到的后跑，这样晚到的作业影响的越小，只是这样必须对仓库的整个依赖要有一个清晰的了解，漏依赖或者多依赖对仓库跑批都有很大的影响。
 数据不到这种情况，也是有的，只有催数据了，就需要做一个模板，各个系统常到的时间节点，比如节点加上最早达到时间，最迟到达时间，到达时间中位数信息等等。超过最迟时间要报警通知处理了。
 ETL常见的问题如下：
数据晚到，但是不报错
数据不到
数据有乱码，换行等文件
一天需要多个批次
频繁需要手工补数
表结构经常变更
调度管理
源系统，目标系统对应关系
阿里的onedate采取类似从上到下的方式，先划分数据域，再拆分数据域中的业务过程，明确可分析维度，最后根据业务过程和可分析维度构建总线矩阵，

Embedding 对于推荐系统中深度学习方法的主要应用可以简单总结如下：
作为 Embedding 层嵌入到深度模型中，实现将高维稀疏特征到低维稠密特征的转换（如 Wide&Deep、DeepFM 等模型）；
作为预训练的 Embedding 特征向量，与其他特征向量拼接后，一同作为深度学习模型输入进行训练（如 FNN）；
在召回层中，通过计算用户和物品的 Embedding 向量相似度，作为召回策略（比 Youtube 推荐模型等）；
实时计算用户和物品的 Embedding 向量，并将其作为实时特征输入到深度学习模型中（比 Airbnb 的 embedding 应用）。
采用 embedding 技术来处理的数据（本文只简单列一下主要的点，后续会详细文章来具体讲如何处理以及其带来的效果）。
User 数据（用户的基础属性数据，如性别、年龄、关系链、兴趣偏好等）
对于用户兴趣偏好，一般简单地采用文本 embedding 方法来得到各标签的 embedding 向量，然后根据用户对个标签的偏好程度做向量加权；
对于关系链数据（如同玩好友、游戏中心相互关注等），构造用户关系图，然后  采用基于图的 embedding 方法来得到用户的 embedding 向量；
Item 数据（Item 基本信息数据，如标题、作者、游戏简介、标签等）
对于文本、简介和标签等可以采用基于文本的 embedding 方法来在已有语料上预训练模型，然后得到对应的 embedding 向量（如 word2vec 或者 BERT）；
此外对于有明确关系的（如 item->文本->标签 or 关键词）可以采用对关键词/标签的向量均值来表示 item 的文本向量（这里安利一下 FaceBook 开源的StarSpace）；
User 行为数据（用户在场景中的行为数据，如点击、互动、下载等）
针对用户对 Item 的操作（如点击、互动、下载）构造用户->item+Item 标签体系，构造用户-item-tag 的异构网络，然后可以采用 Metapath2vec 来得到各节点的 embedding 向量；
通过记录用户在整个场景访问 item，构造 Item-Item 关系图，然后采用 DeepWalk 算法得到 item 的向量，用来挖掘 Item 间的关系特征；
额外数据（外部扩充数据，如用户游戏行为、用户微信其他场景活跃等）
标签型（主要是用户在各场景的兴趣偏好）：
关系链型（如游戏中心好友、游戏内好友、开黑好友）可以采用用户关系构造用户关系图，采用 Graph embedding 方法（如 GraphSAGE）来表示用户抽象特征
现在主流的 Embedding 技术”，主要分三大块：
经典的矩阵分解方法：这里主要是介绍 SVD 方法
基于内容的 embedding 方法：这部分主要涉及到 NLP 相关的文本 embedidng 方法，包括静态向量 embedding（如 word2vec、GloVe 和 FastText）和动态向量 embedding（如 ELMo、GPT 和 BERT）
基于 Graph 的 embedding 方法：这部分主要是介绍图数据的 embedding 方法，包括浅层图模型（如 DeepWalk、Node2vec 和 Metapath2vec）和深度图模型（如基于谱的 GCN 和基于空间的 GraphSAGE）

目前的主要趋势之一是分析金融交易行为，并使用这些信息来预测一个人偿还贷款的可能性。

有许多不同类型的事务行为，其中一些可能包括：
交易频率：包括你在任何特定时期内进行的交易数量。
提款时间：提取现金的时间，金额和频率。例如，大量的深夜现金提取可能对你不利，因为这种行为通常与失业者有关。
现金支出：极端的现金支出可能会给贷方带来不可预测和不可靠的行为。
定期付款/费用：定期付款是借款人承担财务责任的一个很好的指标。注意太多大额的、不灵活的费用，例如电费或租金，贷方可能会担心您承担更多财务责任，因此很难摆脱这些大额支出。
资金用途的类型和位置：你经常去赌场吗？贷款人不喜欢处理赌博类型，所以如果你经常赌博，这可能违背你的信用评分。
客户服务电话：令人惊讶的是，你拨打客户服务的次数可能会说明你的个性类型。过度需要帮助的人可能不是最适合承担贷款责任的人。

自动化埋点+代码埋点的方式实现。自动化埋点，参考阿里的SPM ( Super Position Model 超级位置模型 )+SCM ( Super Content Model 
超级内容模型 )，通过位置模块+内容定义的方式实现成本较低的自动化埋点，解放埋点复杂度高，数据精细化程度要求较高的页面：例如像首页
流量分发的核心页面。如下图所示：通过事件 ( click )+页面 ( index )+模块 ( kingkong )+坑位 ( sequence )+内容 ( type，extra，name等 ) 组合得知用户点击了首页金刚区具体某个坑位，以及这个坑位的具体名称，对应素材图对应的资源信息，通过位置信息和内容信息的封装，实现自动化埋点。

@语法只是装饰器调用的便捷方式：将被装饰函数sleeps作为参数传给装饰器函数，再将装饰器返回值重新绑定到原sleeps变量上。

调额是一个动作，是基于历史一系列的消费，需求或者其他行为的一个操作。动作方向分为平台或银行主动调额和用户主动申请调额。
调额类型狭义讲分为：增额和降额，广义可以增加冻额、解冻、注销、申请等。
基于调额的时效性分为临时性调额和通用性或者永久性调额。
1.平台或者银行主动赋予永久提额
方法：主要考虑用户的消费需求，历史还款表现。一般在还款表现期六个月左右，会考虑给用户提升额度。
1).基于个人建立额度管理模型，提升推荐额度。模型一般基于原始额度和提升额度后，用户的额度使用率以及还款表现去确定目标变量；
2).基于客户群体，主要基于风险敞口，针对不同客群设计不同提额策略。
2.平台或银行主动赋予临时额度
赋予临时较高额度一般都是在传统或者人造节日的噱头下，用户较往日除日常需求外，有更加强烈的消费或者借款需求，
同时金融机构临时授予较高额度也可以刺激客户进行消费借贷。
临时赋予额度的大小同样基于用户日常的还款基数和利用算法推测在某节日可能的需求欲望。
3.用户主动申请提额
一般体现在信用卡类比较突出，比如初始授信额度偏低，
这种用户主动申请提额往往不是源于用户有较高的借款需求，而是为了提升额度后以备不时之需。
4.系统降额或者冻额
该行为主要表现在两方面，一是用户有不良还款表现，比如逾期。或者用户长时间不使用授信额度等，
对于长期不使用额度突然使用的情况，这种不良的风险性较大。
5.用户主动申请降额
该行为主要有两方面考虑：
1).用户希望控制自己的借款或者消费欲望，将额度控制在一定范围内，对自己的消费和借款控制在合理范围内，该部分客群实际上是优质客户。
2).用户处于安全性考虑，不希望额度账号被盗刷、盗用，或者受到其他新兴诈骗或者欺骗行为。