#### 特征工程

##### 特征构造

特征构造的时候需要考虑数值特征，类别特征，时间特征。

- 数值特征，一般会尝试一些它们之间的加减组合（当然不要乱来，根据特征表达的含义）或者提取一些统计特征
- 类别特征，我们一般会尝试之间的交叉组合，embedding也是一种思路
- 时间特征，这一块又可以作为一个大专题来学习，在时间序列的预测中这一块非常重要，也会非常复杂，需要就尽可能多的挖掘时间信息，会有不同的方式技巧。

##### 特征选择

特征选择主要有两个功能：

- 减少特征数量、降维，使模型泛化能力更强，减少过拟合
- 增强对特征和特征值之间的理解



通常来说，从两个方面考虑来选择特征：

- 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。
- 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。



根据特征选择的形式又可以将特征选择方法分为3种：

- 过滤法(Filter)：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。
- 包装法(Wrapper)：根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
- 嵌入法(Embedded)：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。

##### 特征降维

数据特征维度太高，首先会导致计算很麻烦，其次增加了问题的复杂程度，分析起来也不方便。但盲目减少数据的特征会损失掉数据包含的关键信息，容易产生错误的结论，对分析不利。

PCA降维方法，既可以减少需要分析的指标，而且尽可能多的保持了原来数据的信息。

但要注意一点， 特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，而PCA，将已存在的特征压缩，降维完毕后不是原来特征的任何一个，也就是PCA降维之后的特征我们根本不知道什么含义了。