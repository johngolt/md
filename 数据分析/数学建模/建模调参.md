当有多个混淆矩阵（多次训练、多个数据集、多分类任务）时，有两种方式估算 “全局” 性能：

- macro 方法：先计算每个 PR，取平均后，再计算 F1
- micro 方法：先计算混淆矩阵元素的平均，再计算 PR 和 F1

与 P-R 曲线相比，ROC 曲线有一个特点：当正负样本的分布发生变化时，ROC 曲线形状能基本保持不变，而 P-R 曲线的形状一般会发生比较剧烈的变化。因此，当数据不均匀时，ROC 曲线更能够反映模型好坏。

##### 模型对比

| 模型       | 优点                                                         | 缺点                                                         |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 决策树     | 易于理解和解释，容易提取出规则；可以同时处理标类别和数值型数据；模型效果好；可以有效处理非线性。 | 对缺失数据处理比较困难；容易出现过拟合问题；忽略数据集中属性的相互关联；不适合处理高维数据 |
| 随机森林   | 可以计算和比较哪些特征比较重要；训练可以并行化，速度快；数据适应能力强，可以处理连续和离散变量；随机性的引入，抗过拟合能力比较强； | 模型的可解释性比较差；对于小数据或者低维数据效果不会太好；在噪声较大的数据集上容易过拟合 |
| `KNN`      | `KNN`是一种在线技术，新数据可以直接加入数据集而不必进行重新训练;`KNN`理论简单，容易实现 | 对于样本容量大的数据集计算量比较大；样本不平衡时，预测偏差比较大；`KNN`每一次分类都会重新进行一次全局运算；k值大小的选择。 |
| `SVM`      | 解决小样本下机器学习问题。解决非线性问题；无局部极小值问题；可以很好的处理高维数据集；泛化能力比较强。 | 对于核函数的高维映射解释力不强，尤其是径向基函数；对缺失数据敏感。 |
| 朴素贝叶斯 | 对大数量训练和查询时具有较高的速度；支持增量式运算，即可以实时的对新增的样本进行训练；朴素贝叶斯对结果解释容易理解。 | 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 |
| 逻辑回归   | 实现简单，速度快，模型可解释性好，模型稳定，鲁棒性好，易于部署。 | 容易产生欠拟合，分类精度不高；不能很好地处理大量多类特征或变量；  对数据要求高，缺失值，异常值共线性敏感 |
| `Adaboost` | 分类精度高，构造简单，结果可理解；不容易过拟合；可以使用各种分类模型来构建若学习器，灵活性高。 | 过度偏向分类困难的数据，容易收到噪声数据干扰；依赖于弱学习器，训练时间可能较长 |
| `GBDT`     | 泛化性能比较好；可以灵活处理各种类型数据                     | 对异常值比较敏感；分类器之间存在依赖关系，难以进行并行计算   |
|            |                                                              |                                                              |

##### 随机森林

先假设我们有N个样本，M个特征，那么在这种情况下我们是如何构建随机森林的：

a、构建一棵树，我们利用自助法（bootstrap）从N个样本中选取N个样本，需要注意的是，这N个样本是大概率会有重复的，选取的这N个样本就是根节点待分裂的样本；

b、在每个待分裂节点上，我们从M个特征中随机选取m个特征（<<M，通常是log2或sqrt的数量），从这m个属性中根据某种策略（如gini减少或信息增益等）确定分裂属性；

c、重复b步骤，直到不能分裂或达到我们设定的阈值（如叶子节点数或树的深度），此时建立了一颗决策树；

d、重复上面的a、b、c步骤，直到达到预定树的颗数为止。

随机森林的变量重要性衡量的并不完全是变量对目标变量预测的贡献能力，而是在这个模型中对目标变量预测的贡献能力。常见的计算方法有两种，一种是mean decrease impurity，即平均不纯度的减少；一种是mean decrease accuracy，即平均准确率的减少，常用袋外误差率去衡量。

#### 模型调参

###### GBM

GBM的参数可以分为三类：

1、与单颗树构建有关的参数，即影响单颗树的结构；

2、与boosting相关的参数，会影响模型的结构和迭代；

3、其他参数；

1、min_samples_split

最小可分样本数，即到达某节点时，确定是否需要再分下去，如果这个节点的样本数小于阈值，则停止分裂。

这个参数可以用来降低过拟合，设置较大值时，可以有效减少因某些非重要特征作为分裂点；

当这个参数设置过大时，可能会造成模型欠拟合，具体设置需要考虑样本数、样本均衡性和CV确定；

2、min_samples_leaf

最小叶节点样本数，即某叶节点的样本数过小时，应该回退到上一节点，相当于剪枝；

同样可以用来降低过拟合风险；

当样本不均衡时，尤其需要注意这个参数的设置，因为这意味着比例过小的类能否有效分出来；

3、min_weight_fraction_leaf

与min_samples_leaf类似，但这里设置的不是样本数，而是整体样本的比例；

4、max_depth

树的深度；

用来防止过拟合，单颗树过深可能会学到无关特征；

CV调参；

5、max_leaf_nodes

树的叶节点树；

用来防止过拟合；

如果设置该参数，则max_depth会被忽略；

6、max_features

待分裂的特征数，GBM参考了随机森林的做法，分裂时只选用了一部分特征来降低树之间的相关性；

用来降低过拟合；

一般用log，平方根的特征数目作为参数候选值；



二、booting相关的参数

1、learning_rate

学习率，即控制基模型带来拟合效果的权重；

较低的学习率通常有较好的拟合效果，毕竟步子迈精细一点，在较高的误差处发生震荡的可能性变小了；

较低的学习率通常需要辅以较多的基学习器，这也意味着学习效率会降低；

2、n_estimators

基学习器的个数，这里是数的颗数；

当学习率不变时，较多的基学习器会带来过拟合的风险，一般该参数需要与learning_rate结合调整；

3、subsample

子采样数，每棵树的构建并不会取全部的样本，而是随机抽取一部分样本，参考的也是随机森林的思想，但这里的抽样是不放回抽样；

这种方法也是用来降低过拟合的风险；

通常取值0.8左右；



三、其他参数

1、loss

即损失函数；

依分类问题和回归问题，损失函数选取不一样，通常选默认的损失函数即可；

2、init

这个参数的输入是模型变量，即GBM的启动模型；

3、random_state

随机状态参数，即随机种子；

当调参时，该参数需固定，否则根据CV的调参会产生影响；

4、verbose

决定日志（训练过程）是否需要打印，默认不打印；

5、warm_start

即热启动，当你训练GBM到一定程度停止时，如果你想在这个基础上接着训练，就需要用到该参数（True）减少重复训练；

###### xgboost

下一部分会详细讲讲xgboost的参数，他们可以分成三个部分：

**1、General Parameters，即与整个模型属基调相关的参数；**

**2、Booster Parameters，即与单颗树生成有关的参数；**

**3、Learning Task Parameters，与模型调优相关的参数；**



**一、General Parameters**

1、booster [default=gbtree]

即xgboost中基学习器类型，有两种选择，分别是树模型（gbtree）和线性模型（linear models）

2、silent [default=0]

即控制迭代日志的是否输出，默认输出；

3、nthread [default to maximum number of threads available if not set]

即控制模型训练调用机器的核心数，与sklearn中*n_jobs的含义相似；*



***二、\*Booster parameters**

因为booster有两种类型，常用的一般是树模型，这里只列树模型相关的参数：

1、eta [default=0.3]

学习率，这个相当于sklearn中的learning_rate

常见的设置范围在0.01-0.2之间

2、min_child_weight [default=1]

叶节点的最小权重值；

这个参数与GBM（sklearn）中的“min_samples_leaf”很相似，只不过这里不是样本数，而是权重值，如果样本的权重都是1，这两个参数是等同的；

这个值设置较大时，通常树不会太深，可以控制过拟合，但太大时，容易造成欠拟合的现象，具体调参需要cv；

3、max_depth

树的最大深度，含义很直白，控制数的复杂性；

通常取值范围在3-10；

4、max_leaf_nodes

最大叶节点数；

一般这个参数与max_depth二选一控制即可；

5、gamma [default=0]

分裂收益阈值；

即用来比较每次节点分裂带来的收益，有效控制节点的过度分裂；

这个参数的变化范围受损失函数的选取影响；

6、max_delta_step [default=0]

这个参数暂时不是很理解它的作用范围，一般可以忽略它；

7、subsample [default=1]

采样比例；

与sklearn中的参数一样，即每颗树的生成可以不去全部样本，这样可以控制模型的过拟合；

通常取值范围0.5-1；

8、colsample_bytree [default=1]

特征采样的比例（每棵树）；

即每棵树不使用全部的特征，控制模型的过拟合；

通常取值范围0.5-1；

9、colsample_bylevel [default=1]

特征采样的比例（每次分裂）；

这个与随机森林的思想很相似，即每次分裂都不取全部变量；

当7、8的参数设置较好时，该参数可以不用在意；

10、lambda [default=1]

L2范数的惩罚系数，叶子结点的分数？；

11、alpha [default=0]

L1范数的惩罚系数，叶子结点数？；

12、scale_pos_weight [default=1]

这个参数也不是很理解，貌似与类别不平衡的问题相关；



**三、Learning Task Parameters**

1、objective [default=reg:linear]

你可以认为是目标函数；

通常的选项分别是：binary:logistic，用于二分类，产生每类的概率值；multi:softmax，用于多分类，但不产生概率值，直接产生类别结果；multi:softprob，类似softmax，但产生多分类的概率值；

2、eval_metric [ default according to objective ]

即评价指标；

当你给模型一个验证集时，会输出对应的评价指标值；

一般有：rmse ，均方误差；mae ，绝对平均误差；logloss ，对数似然值；error ，二分类错误率；merror ，多分类错误率；mlogloss ；auc

3、seed

即随机种子；

##### 调参方法

贪心调参 （坐标下降）

坐标下降法是一类优化算法，其最大的优势在于不用计算待优化的目标函数的梯度。最容易想到一种特别朴实的类似于坐标下降法的方法，与坐标下降法不同的是，不是循环使用各个参数进行调整，而是贪心地选取了对整体模型性能影响最大的参数。参数对整体模型性能的影响力是动态变化的，故每一轮坐标选取的过程中，这种方法在对每个坐标的下降方向进行一次直线搜索

网格调参

作用是在指定的范围内可以自动调参，只需将参数输入即可得到最优化的结果和参数。

贝叶斯调参

贝叶斯优化通过基于目标函数的过去评估结果建立替代函数（概率模型），来找到最小化目标函数的值。贝叶斯方法与随机或网格搜索的不同之处在于，它在尝试下一组超参数时，会参考之前的评估结果，因此可以省去很多无用功。



##### GBDT调参

##### xgboost调参

参数调优的一般步骤：

- 1.确定（较大）学习速率和提升参数调优的初始值
- 2.max_depth 和 min_child_weight 参数调优
- 3.gamma参数调优
- 4.subsample 和 colsample_bytree 参数优
- 5.正则化参数alpha调优
- 6.降低学习速率和使用更多的决策树

###### 通用参数

| 参数名称               | 作用                                                         |
| ---------------------- | ------------------------------------------------------------ |
| `booster`              | 使用哪个弱学习器训练，默认gbtree，可选gbtree，gblinear 或dart |
| `eta/learning_rate`    | 在更新中使用步长收缩以防止过度拟合，默认= 0.3，范围：[0,1]；典型值一般设置为：0.01-0.2 |
| `max_depth`            | 默认6，一棵树的最大深度。增加此值将使模型更复杂，并且更可能过度拟合。 |
| `min_child_weight`     | 默认值1，如果新分裂的节点的样本权重和小于`min_child_weight`则停止分裂 。用来防止过拟合，如果过大，会造成欠拟合。 |
| `gamma/min_split_loss` | 默认0，分裂节点时，损失函数减小值只有大于等于$\gamma$节点才分裂，$\gamma$值越大，越不容易过拟合，但是容易欠拟合。 |
| `subsample`            | 默认值1，构建每棵树对样本的采样率，如果设置成0.5，随机选择一半的样本作为训练集。 |
| `colsample_bytree`     | 默认1，列采样率，也就是特征采样率                            |
| `alpha/reg_alpha`      | 默认0，权重的$L_1$正则化项。                                 |
| `lambda/reg_lambda`    | 默认1，$L_2$正则化权重项。                                   |

- sampling_method：默认= uniform，用于对训练实例进行采样的方法。

- - uniform：每个训练实例的选择概率均等。
  - gradient_based：每个训练实例的选择概率与规则化的梯度绝对值成正比，具体来说就是$\sqrt{g^2+\lambda h^2}$。

- tree_method：默认=auto，XGBoost中使用的树构建算法。

- - auto：使用启发式选择最快的方法。

- - exact：精确的贪婪算法。枚举所有拆分的候选点。
  - approx：使用分位数和梯度直方图的近似贪婪算法。
  - hist：更快的直方图优化的近似贪婪算法。
  
- scale_pos_weight:控制正负权重的平衡，这对于不平衡的类别很有用。一般设置sum(negative instances) / sum(positive instances)，在类别高度不平衡的情况下，将参数设置大于0，可以加快收敛。


###### 任务参数

- objective：默认=reg:squarederror，表示最小平方误差。

- - reg:squarederror,最小平方误差。
  - reg:squaredlogerror,对数平方损失。
  - reg:logistic,逻辑回归
  - reg:pseudohubererror,使用伪Huber损失进行回归，这是绝对损失的两倍可微选择。
  - binary:logistic,二元分类的逻辑回归，输出概率。
  - binary:logitraw：用于二进制分类的逻辑回归，逻辑转换之前的输出得分。
  - binary:hinge：二进制分类的铰链损失。这使预测为0或1，而不是产生概率。（SVM就是铰链损失函数）
  - count:poisson –计数数据的泊松回归，泊松分布的输出平均值。
  - survival:cox：针对正确的生存时间数据进行Cox回归（负值被视为正确的生存时间）。
  - survival:aft：用于检查生存时间数据的加速故障时间模型。
  - aft_loss_distribution：survival:aft和aft-nloglik度量标准使用的概率密度函数。
  - multi:softmax：设置XGBoost以使用softmax目标进行多类分类，还需要设置num_class（类数）
  - multi:softprob：与softmax相同，但输出向量，可以进一步重整为矩阵。结果包含属于每个类别的每个数据点的预测概率。
  - rank:pairwise：使用LambdaMART进行成对排名，从而使成对损失最小化。
  - rank:ndcg：使用LambdaMART进行列表式排名，使标准化折让累积收益（NDCG）最大化。
  - rank:map：使用LambdaMART进行列表平均排名，使平均平均精度（MAP）最大化。
  - reg:gamma：使用对数链接进行伽马回归。输出是伽马分布的平均值。
  - reg:tweedie：使用对数链接进行Tweedie回归。
  - 自定义损失函数和评价指标：

- eval_metric：验证数据的评估指标，将根据目标分配默认指标（回归均方根，分类误差，排名的平均平均精度），用户可以添加多个评估指标

- - rmse，均方根误差；rmsle：均方根对数误差；mae：平均绝对误差；mphe：平均伪Huber错误；logloss：负对数似然；error：二进制分类错误率；
  - merror：多类分类错误率；mlogloss：多类logloss；auc：曲线下面积；aucpr：PR曲线下的面积；ndcg：归一化累计折扣；map：平均精度；








但如果找不到产生异常点的原因，它可能就是一个正常数据，此时可以考虑补充抽样，看看能不能把异常点与大多数数据中的空间填补上。

双峰（多峰）数据：把两组(或多组)数据混到一起了，可能每组数据都服从正态分布，做数据分析时尽可能把数据按不同属性分开分析。

平顶数据：平顶的数据是指在直方图上看到的图形是相对比较平坦的。原因：不同均值的数据混在一起（按其属性分开），或者是数据收集的周期过长，过程发生了缓慢的移动（只取近期的数据进行分析）。

##### 特征归一化/标准化

如果你的数据包含许多异常值，使用均值和方差缩放可能并不是一个很好的选择。这种情况下，你可以使用`robust_scale`以及`RobustScaler`作为替代品。它们对你的数据的中心和范围使用更有鲁棒性的估计。中心化稀疏数据会破坏数据的稀疏结构，因此很少有一个比较明智的实现方式。但是缩放稀疏输入是有意义的，尤其是当几个特征在不同的量级范围时，最推荐的缩放方式是采用最大绝对值缩放。

样本分布非正态的原因：数据分布本身就不是正态的；存在异常点；双峰（多峰）数据：可能是把两组(或多组)数据混到一起了，可能每组数据都服从正态分布；平顶的数据



对偏态分布进行处理的原因：而很多模型要求：误差服从独立同分布，时间序列平稳。这需要寻找一种方式让数据尽量满足假设，让方差恒定，即让波动率相对稳定。右偏的，取对数可以将大于中位数的值按一定比例缩小，从而形成正态分布的数据

###### 非线性转换

**映射到均分分布上的转换**：利用分位点信息来转换特征使之符合均匀分布，这种转换倾向于将最常见的数值打散，如此能减少异常值的影响。 然而，该转换确实扭曲了特征内部和特征之间的相关性和距离。**映射到正态分布上的转换**：如果数据不是正态分布的，比如说出现长尾现象的，尤其是数据的平均数和中位数相差很大的时候。这里主要采用一种叫做$\text{Power Transformer}$的方法，这种转换通过一些列参数单调变换使得数据更符合正态分布。$\text{PowerTransformer}$现在支持两种转换，两者都有一个参数 $λ$需要设定：$\text{Box-Cox}$转换：要求输入数据严格为正数。$\text{Yeo-Johnson}$变换：正数或负数。

#### stats

数据变换：`box-cox`, `yeo`

数据截断：箱型：`sigmaclip`；分位数:`trimboth`, `trim1`

##### 正态检

| 正态检验                             | `null hypothesis`                      |
| ------------------------------------ | -------------------------------------- |
| `probplot`                           |                                        |
| `normaltest, kurtosistest, skewtest` |                                        |
| `anderson`                           | 样本服从某个特定的分布                 |
| `shapiro`                            | 样本取自正态分布                       |
| `jarque_bera`                        | 样本的偏度和峰度是正态分布             |
| `ks_1samp, kstest`                   | 样本的分布$F(x)$与给定的分布$G(x)$一致 |

##### 相关性分析

###### 数值*数值

linregress: Calculate a linear least-squares regression for two sets of measurements.

pearsonr: Pearson correlation coefficient and p-value for testing non-correlation.

spearmanr: Calculate a Spearman correlation coefficient with associated p-value.

kendalltau: Calculate Kendall’s tau, a correlation measure for ordinal data.

siegelslopes: Computes the Siegel estimator for a set of points (x, y).

theilslops: Computes the Theil-Sen estimator for a set of points (x, y).



###### 数值*类别

mood：Mood’s two-sample test for scale parameters is a non-parametric test for the null hypothesis that two samples are drawn from the same distribution with the same scale parameter.

pointbiserialr: The point biserial correlation is used to measure the relationship between a binary variable, x, and a continuous variable, y. 

anderson_ksamp: It tests the null hypothesis that k-samples are drawn from the same population without having to specify the distribution function of that population.

wilcoxon:  tests the null hypothesis that two related paired samples come from the same distribution.

epps_singleton_2sample: Test the null hypothesis that two samples have the same underlying probability distribution.

ks_2samp: This is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution. 

ttest_ind: the null hypothesis that 2 independent samples have identical average (expected) values. xThis test assumes that the populations have identical variances by default.

###### 类别*类别

binon_test：This is an exact, two-sided test of the null hypothesis that the probability of success in a Bernoulli experiment is *p*. 查看各个类别的坏样本率是否符合总体的坏样本率，判断特征对于建模是否有用的方式。

power_divergence: This function tests the null hypothesis that the categorical data has the given frequencies, using the Cressie-Read power divergence statistic.

chisquare: tests the null hypothesis that the categorical data has the given frequencies.x



| 方差检验          | null hypothesis                    |
| ----------------- | ---------------------------------- |
| `kruskal`         | 不同群组的中位数相等               |
| `obrientransform` | 将不同方差的数据集变换为相同方差的 |
| `bartlett`        | 所有输入样本均来自方差相等的总体   |
| `levene`          | 所有输入样本均来自方差相等的总体   |
| `flingner`        | 所有输入样本均来自方差相等的总体   |

Levene检验 与Bartlette检验(巴特莱多)的区别：

1）对于正态分布的样本，Bartlette检验极其灵敏，但是对于非正态分布的样本，检验非常不准确；

2）Levene检验是一种更为稳健的检验方法，既可用于正态分布的样本，也可用于非正态分布的样本，同时对比较的各组样本量可以相等或不等；

3）两者的检验原理不同，Bartlette检验是对原始数据检验其方差是否齐性，而

Levene检验是检验组间残差是否齐性，而且一般认为要求残差的方差齐性

**方差齐性**是t检验和方差分析的前提条件

#### Boosting/Bagging

| 名称         | 定义                                                         |
| ------------ | ------------------------------------------------------------ |
| 平均精度`AP` | P-R曲线围起来的面积，通常来说一个越好的分类器，AP值越高。    |
| `mAP`        | 即是把每个类别的AP都单独拿出来，然后计算所有类别AP的平均值，代表着对检测到的目标平均精度的一个综合度量。 |
|              |                                                              |

秩相关系数，秩是指样本值的大小在全体样本从小到大排序后所占的次序。对于一对数$(X_1, Y_1)$和$(X_2, Y_2)$，如果$X_1>X_2$且$Y_1> Y_2$或者$X_1<X_2$且$Y_1<Y_2$，则称$(X_1,Y_1)$和$(X_2, Y_2)$是一致的；如果$X_1>X_2$且$Y_1<Y_2$或者$X_1<X_2$且$Y_1>Y_2$，则称$(X_1,Y_1)$和$(X_2, Y_2)$是不一致的；如果$X_1=X_2$或$Y_1=Y_2$，则称$(X_1, Y_1)$和$(X_2,Y_2)$是一个`tie`。记$n_c$为一致对的个数，$n_d$为不一致对的个数，$n_t$为$X$值不等而$Y$值相等的tie的个数，$N$为观测值的个数，可以有以下几个秩相关系数： 

一致性指标：$c=\frac{n_{c}+0.5 n_{t}}{n_{c}+n_{d}+n_{t}}$

`Gini coefficient`：$D_{Y X}=\frac{n_{c}-n_{d}}{n_{c}+n_{d}+n_{t}}$

`Goodman-Krustal Gamma`：$\Gamma=\frac{n_{c}-n_{d}}{n_{c}+n_{d}}$

`Kendall`：$\tau=\frac{n_{c}-n_{d}}{N(N-1) / 2}$

`ROC`：那么一个模型的特异度可以定义为`TNR=TN/(FP+TN)`，灵敏度可以定义为`TPR=TP/(TP+FN)`。而`ROC`曲线的横坐标是1-特异度=`1-TNR=FP/(FP+TN)=FPR`，纵坐标是灵敏度即`TPR`。`ROC`曲线以`FPR`为横轴，`TPR`为纵轴，而`KS`曲线以阈值为横轴，`TPR`、`FPR`为纵轴。 

$\text{KS}$：KS用于模型风险区分能力进行评估， 指标衡量的是好坏样本累计分部之间的差值。 好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。KS的计算步骤如下： 计算每个评分区间的好坏账户数。 计算每个评分区间的累计好账户数占总好账户数比率和累计坏账户数占总坏账户数比率。 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值，然后对这些绝对值取最大值即得此评分卡的K-S值。

提升度曲线：可以衡量使用这个模型比随机选择对坏样本的预测能力提升了多少倍。通常计算`LIFT`的时候会把模型的最终得分按照从低到高，排序并等频分为10组，计算分数最低的一组对应的`累计坏样本占比/累计总样本占比`就等于`LIFT`值了。从直观上理解，累计坏样本占比相当于是使用模型的情况下最差的这一组能够从所有的坏样本中挑出多少比例的坏样本，而累计总样本占比等于随机抽样的情况下从所有坏样本抽取了多少比例的坏样本。



模型特征重要性分析：`LGB/XGB`等的`importance`、`LR、SVM`的`coeff`等；特征重要性可以结合业务理解，有些奇怪的特征在模型中起着关键的作用，这些可以帮助我们更好地理解我们的业务，同时如果有些特征反常规，我们也可以看出来；可能这些就是过拟合的特征等等；     

模型分割方式分析：可视化模型的预测，包括`LGB`的每一颗数等；这些可以帮助我们很好的理解我们的模型，模型的分割方式是否符合常理也可以结合业务知识一起分析，帮助我们更好的设计模型；       

模型结果分析：这个在回归问题就是看预测的结果的分布；分类一般看混淆矩阵等。这么做可以帮助我们找到模型做的不好的地方，从而更好的修正我们的模型。

根据loss对样本加权的工作就已经有很多。神奇的是，其实在一条线上有着截然相反的想法的研究：第一类工作的想法是如果一个样本训练得不够好，也就是loss高的话，那么说明现在的模型没有很好fit到这样的数据，所以应该对这样的样本给予更高的权重。这一类工作就对应到经典的Hard Negative (Example) Mining，近期的工作如Focal Loss也是这个思想。另一类工作的想法是学习需要循序渐进，应该先学习简单的样本，逐渐加大难度，最终如果仍然后Loss很大的样本，那么认为这些样本可能是Outlier，强行fit这些样本反而可能会使泛化性能下降。这一类中对应的是Curriculum Learning或者Self-Paced Learning类型的工作。本质上，这两个极端对应的是对训练数据本身分布的不同假设。第一类方法认为那些fit不好的样本恰恰是模型应当着重去学习的，第二类方法认为那些fit不上的样本则很可能是训练的label有误。

###### K-S检验

KS检验，是统计学中的一种非参数假设检验，用来检测单样本是否服从某一分布，或者两样本是否服从相同分布。在单样本的情况下，我们想检验这个样本是否服从某一分布函数$F_0(x)$，记$F_1(x)$是该样本的经验分布函数。我们构造KS统计量：$D_n=\max_x|F_1(x)-F_0(x)|$

经验分布函数与目标分布的累积分布函数的最大差值就是我们要求的KS统计量：95%置信度的KS统计量的临界值由$D_n=\frac{1.36}{\sqrt{n}}$。两样本的KS检验，95%置信度的临界值为$D_n=1.36\sqrt{\frac{1}{n_x}+\frac{1}{n_y}}$，如果我们根据样本得到的KS统计量的值小于$D_n$，那么我们就接收原假设。否则，拒绝原假设。

#### 类别不平衡学习

不平衡数据集上分类困难的原因：①过多的少数类样本出现在多数类样本密集的区域；②类别之间的分布严重重叠；③数据中本身存在的噪声，尤其是少数类的噪声；④少数类分布的稀疏性以及稀疏性导致的拆分多个子概念并且每个子概念仅含有较少的样本数量。①②③都归因为一个因素：噪声，④又被称为small disjuncts问题，在同样的特征空间中，相比于只有一个cluster的简单少数类分布，具有多个子概念的少数类分布需要模型给出更复杂的决策边界来获得良好的预测。在模型复杂度不变的情况下，分类性能会因子概念个数的增多而变差。因此该问题的解决办法也较为简单：上更大容量的模型

#####  处理方法

###### 异常检测

分类问题的一个隐含假设是各个类别的数据都有自己的分布，当某类数据少到难以观察结构的时候，我们可以考虑抛弃该类数据，转而学习更为明显的多数类模式，而后将不符合多数类模式的样本判断为少数类，某些时候会有更好的效果。此时该问题退化为异常检测问题。

| 方法     | 说明                                                         | 优点                                                         | 缺点                                                         |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 数据级   | 该类方法关注于通过修改训练数据集以使得标准学习算法也能在其上有效训练。 | 该类方法能够去除噪声/平衡类别分布；欠采样方法减小数据集规模，降低计算量 | 采样过程计算效率低下；易被噪声影响；过采样方法生成过多数据；不适用于无法计算距离的复杂数据集： |
| 算法级   | 代价敏感学习给少数类样本分配较高的误分类代价，而给多数类样本分配较小的误分类代价。代价敏感学习在学习器的训练过程中人为提高了少数类别样本的重要性，以此减轻分类器对多数类的偏好。 | 不增加训练复杂度；可直接用于多分类问题                       | 需要领域先验知识，代价敏感学习中的代价矩阵需要由领域专家根据任务的先验知识提供；不能泛化到不同任务：对于特定问题设计的代价矩阵只能用于该特定任务；依赖于特定分类器 |
| 集成学习 | 专注于将一种数据级或算法级方法与集成学习相结合，一种是基于某种特定的集成学习算法并在集成的过程中嵌入一种其他的不平衡学习方法。另一种是集成学习的基学习器也是集成学习器。 | 效果通常较好；可使用迭代过程中的反馈进行动态调整             | 包含所使用的不平衡学习方法的缺点；过采样+集成进一步增大计算开销；对噪声不鲁棒 |

数据级方法可被进一步分类为：从多数类别中删除样本的方法（欠采样）；为少数类别生成新样本的方法（过采样）；结合上述两种方案的混合类方法（过采样+欠采样去噪，如SMOTE+ENN等）

标准的随机重采样方法使用随机方法来选择用于预处理的目标样本。然而随机方法可能会导致丢弃含有重要信息的样本（随机欠采样）或者引入无意义的甚至有害的新样本（随机过采样），因此有的方法试图根据根据数据的分布信息来在进行重采样的同时保持原有的数据结构。

` Edited Nearest Neighbours`：对于属于多数类的一个样本，如果其K个近邻点有超过一半都不属于多数类，则这个样本会被剔除。这个方法的另一个变种是所有的K个近邻点都不属于多数类，则这个样本会被剔除。

`Near Miss`有三种变种：选择到最近的K个少数类样本平均距离最近的多数类样本；选择到最远的K个少数类样本平均距离最近的多数类样本；对于每个少数类样本选择K个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围



对样本进行过采样或者降采样时需要在`cross-validation`之后，避免数据泄露问题





###### 相关分析

研究现象之间是否存在某种依存关系，对具体有依存关系的现象探讨相关方向及相关程度。

1. 单相关：两个因素之间的相关关系叫单相关，即研究时只涉及一个自变量和一个因变量；

2. 复相关 ：三个或三个以上因素的相关关系叫复相关，即研究时涉及两个或两个以上的自变量和因变量相关；

3. 偏相关：在某一现象与多种现象相关的场合，当假定其他变量不变时，其中两个变量之间的相关关系称为偏相关。

###### 方差分析

使用条件：各样本须是相互独立的随机样本；各样本来自正态分布总体；各总体方差相等。

- 单因素方差分析：一项试验只有一个影响因素，或者存在多个影响因素时，只分析一个因素与响应变量的关系
- 多因素有交互方差分析：一项实验有多个影响因素，分析多个影响因素与响应变量的关系，同时考虑多个影响因素之间的关系
- 多因素无交互方差分析：分析多个影响因素与响应变量的关系，但是影响因素之间没有影响关系或忽略影响关系
- 协方差分析：传统的方差分析存在明显的弊端，无法控制分析中存在的某些随机因素，使之影响了分析结果的准确度。协方差分析主要是在排除了协变量的影响后再对修正后的主效应进行方差分析，是将线性回归与方差分析结合起来的一种分析方法。

> **横型诊断方法**
>
> 残差检验：观测值与估计值的差值要跟从正态分布
>
> 强影响点判断：寻找方式一般分为标准误差法、Mahalanobis距离法
>
> 共线性诊断：诊断方式：容忍度、方差扩大因子法(又称膨胀系数VIF)、特征根判定法、条件指针CI、方差比例，处理方法：增加样本容量或选取另外的回归如主成分回归等



 

惯例： `scikit-learn estimator`遵守以下惯例：

 

\- 除非显式指定数据类型，否则所有的输入数据都被转换成 `float64`

\- 回归问题的输出被转换成 `float64`；分类问题的输出不被转换

\- `estimator`的参数可以更新：

 \- `estimator.set_params(...)`方法可以显式更新一个`estimator`的参数值

 \- 多次调用`estimator.fit(...)`方法可以隐式更新一个`estimator`的参数值。最近的一次训练学到的参数会覆盖之前那次训练学到的参数值。

