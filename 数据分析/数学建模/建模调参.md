长尾分布：这种分布会使得采样不准，估值不准，因为尾部占了很大部分。另一方面，尾部的数据少，人们对它的了解就少，那么如果它是有害的，那么它的破坏力就非常大，因为人们对它的预防措施和经验比较少。

##### 模型对比

| 模型       | 优点                                                         | 缺点                                                         |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 决策树     | 易于理解和解释，可以可视化分析，容易提取出规则；可以同时处理标称型和数值型数据；测试数据集时，运行速度比较快；决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。 | 对缺失数据处理比较困难；容易出现过拟合问题；忽略数据集中属性的相互关联；`ID3`算法计算信息增益时结果偏向数值比较多的特征。 |
| 随机森林   | 可以计算和比较哪些特征比较重要；训练速度快，容易做成并行化方法；在训练过程中，能够检测到特征之间的影响；对于不平衡数据集来说，随机森林能处理不平衡数据集的有效方法；RF算法可以很好的处理缺失值的情况；抗过拟合能力比较强； | 在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合；对于小数据或者低维数据，可能不能产生很好的分类； |
| `KNN`      | `KNN`是一种在线技术，新数据可以直接加入数据集而不必进行重新训练;`KNN`理论简单，容易实现 | 对于样本容量大的数据集计算量比较大；样本不平衡时，预测偏差比较大；`KNN`每一次分类都会重新进行一次全局运算；k值大小的选择。 |
| `SVM`      | 解决小样本下机器学习问题。解决非线性问题；无局部极小值问题；可以很好的处理高维数据集；泛化能力比较强。 | 对于核函数的高维映射解释力不强，尤其是径向基函数；对缺失数据敏感。 |
| 朴素贝叶斯 | 对大数量训练和查询时具有较高的速度；支持增量式运算，即可以实时的对新增的样本进行训练；朴素贝叶斯对结果解释容易理解。 | 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 |
| 逻辑回归   | 计算代价不高，易于理解和实现                                 | 容易产生欠拟合。分类精度不高；不能很好地处理大量多类特征或变量；  对于非线性特征，需要进行转换。 |
| `Adaboost` | 很好的利用了弱分类器进行级联。可以将不同的分类算法作为弱分类器。`AdaBoost`具有很高的精度。相对于bagging算法和Random Forest算法，`AdaBoost`充分考虑的每个分类器的权重。 | `AdaBoost`迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。数据不平衡导致分类精度下降。训练比较耗时，每次重新选择当前分类器最好切分点。 |
| `GBDT`     |                                                              |                                                              |
|            |                                                              |                                                              |

#### 模型调参

##### 调参方法

贪心调参 （坐标下降）

坐标下降法是一类优化算法，其最大的优势在于不用计算待优化的目标函数的梯度。最容易想到一种特别朴实的类似于坐标下降法的方法，与坐标下降法不同的是，不是循环使用各个参数进行调整，而是贪心地选取了对整体模型性能影响最大的参数。参数对整体模型性能的影响力是动态变化的，故每一轮坐标选取的过程中，这种方法在对每个坐标的下降方向进行一次直线搜索

网格调参

作用是在指定的范围内可以自动调参，只需将参数输入即可得到最优化的结果和参数。

贝叶斯调参

贝叶斯优化通过基于目标函数的过去评估结果建立替代函数（概率模型），来找到最小化目标函数的值。贝叶斯方法与随机或网格搜索的不同之处在于，它在尝试下一组超参数时，会参考之前的评估结果，因此可以省去很多无用功。

##### Tuning the hyper-parameters of an estimator

Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use: `estimator.get_params()`
A search consists of:

- an estimator
- a parameter space;
- a method for searching or sampling candidates;
- a cross-validation scheme
- a score function.

###### Exhaustive Grid Search

The grid search provided by `GridSearchCV` exhaustively generates candidates from a grid of parameter values specified with the `param_grid` parameter.

###### Randomized Parameter Optimization

`RandomizedSearchCV` implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:

- A budget can be chosen independent of the number of parameters and possible values.
- Adding parameters that do not influence the performance does not decrease efficiency.

Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for `GridSearchCV`. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the `n_iter` parameter. For each parameter, either a distribution over possible values or a list of discrete choices can be specified:

```python
{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),
  'kernel': ['rbf'], 'class_weight':['balanced', None]}
```

###### Tips for Parameter search

Specifying an objective metric. By default, parameter search uses the `score` function of the estimator to evaluate a parameter setting. An alternative scoring function can be specified via the `scoring` parameter 

Specifying multiple metrics for evaluation. `GridSearchCV` and `RandomizedSearchCV` allow specifying multiple metrics for the `scoring` parameter. When specifying multiple metrics, the `refit` parameter must be set to the metric (string) for which the `best_params_` will be found and used to build the `best_estimator_` on the whole dataset. If the search should not be refit, set `refit=False`. Leaving refit to the default value `None` will result in an error when using multiple metrics.

Parallelism. Computations can be run in parallel if your OS supports it, by using the keyword `n_jobs=-1`.

Robustness to failure. Some parameter settings may result in a failure to `fit` one or more folds of the data. By default, this will cause the entire search to fail, even if some parameter settings could be fully evaluated. Setting `error_score=0` (or `=np.NaN`) will make the procedure robust to such failure, issuing a warning and setting the score for that fold to 0 (or `NaN`), but completing the search.

##### GBDT调参

##### xgboost调参

参数调优的一般步骤：

- 1.确定（较大）学习速率和提升参数调优的初始值
- 2.max_depth 和 min_child_weight 参数调优
- 3.gamma参数调优
- 4.subsample 和 colsample_bytree 参数优
- 5.正则化参数alpha调优
- 6.降低学习速率和使用更多的决策树

###### 通用参数

| 参数名称               | 作用                                                         |
| ---------------------- | ------------------------------------------------------------ |
| `booster`              | 使用哪个弱学习器训练，默认gbtree，可选gbtree，gblinear 或dart |
| `nthread`              | 用于运行XGBoost的并行线程数，默认为最大可用线程数            |
| `eta/learning_rate`    | 在更新中使用步长收缩以防止过度拟合，默认= 0.3，范围：[0,1]；典型值一般设置为：0.01-0.2 |
| `max_depth`            | 默认6，一棵树的最大深度。增加此值将使模型更复杂，并且更可能过度拟合。 |
| `min_child_weight`     | 默认值1，如果新分裂的节点的样本权重和小于`min_child_weight`则停止分裂 。用来防止过拟合，如果过大，会造成欠拟合。 |
| `gamma/min_split_loss` | 默认0，分裂节点时，损失函数减小值只有大于等于$\gamma$节点才分裂，$\gamma$值越大，越不容易过拟合，但是容易欠拟合。 |
| `subsample`            | 默认值1，构建每棵树对样本的采样率，如果设置成0.5，随机选择一半的样本作为训练集。 |
| `colsample_bytree`     | 默认1，列采样率，也就是特征采样率                            |
| `alpha/reg_alpha`      | 默认0，权重的$L_1$正则化项。                                 |
| `lambda/reg_lambda`    | 默认1，$L_2$正则化权重项。                                   |

- - max_delta_step：默认= 0，允许每个叶子输出的最大增量步长。如果将该值设置为0，则表示没有约束。如果将其设置为正值，则可以帮助使更新步骤更加保守。通常不需要此参数，但是当类极度不平衡时，它可能有助于逻辑回归。将其设置为1-10的值可能有助于控制更新。范围：[0，∞]

  - sampling_method：默认= uniform，用于对训练实例进行采样的方法。

  - - uniform：每个训练实例的选择概率均等。通常将subsample> = 0.5 设置 为良好的效果。
    - gradient_based：每个训练实例的选择概率与规则化的梯度绝对值成正比，具体来说就是$\sqrt{g^2+\lambda h^2}$，subsample可以设置为低至0.1，而不会损失模型精度。

  - tree_method：默认=auto，XGBoost中使用的树构建算法。

  - - auto：使用启发式选择最快的方法。

  - - - 对于小型数据集，exact将使用精确贪婪（）。
      - 对于较大的数据集，approx将选择近似算法。它建议尝试hist，gpu_hist，用大量的数据可能更高的性能。

  - - exact：精确的贪婪算法。枚举所有拆分的候选点。
    - approx：使用分位数和梯度直方图的近似贪婪算法。
    - hist：更快的直方图优化的近似贪婪算法。
    - gpu_hist：GPU hist算法的实现。

  - scale_pos_weight:控制正负权重的平衡，这对于不平衡的类别很有用。一般设置sum(negative instances) / sum(positive instances)，在类别高度不平衡的情况下，将参数设置大于0，可以加快收敛。

  - num_parallel_tree：默认=1，每次迭代期间构造的并行树的数量。此选项用于支持增强型随机森林。

  - monotone_constraints：可变单调性的约束，在某些情况下，如果有非常强烈的先验信念认为真实的关系具有一定的质量，则可以使用约束条件来提高模型的预测性能。（例如params_constrained['monotone_constraints'] = "(1,-1)"，(1,-1)我们告诉XGBoost对第一个预测变量施加增加的约束，对第二个预测变量施加减小的约束。）

- Linear Booster的参数：

- - lambda（reg_lambda）：默认= 0，L2正则化权重项。增加此值将使模型更加保守。归一化为训练示例数。

  - alpha（reg_alpha）：默认= 0，权重的L1正则化项。增加此值将使模型更加保守。归一化为训练示例数。

  - updater：默认= shotgun。

  - - shotgun：基于shotgun算法的平行坐标下降算法。使用“ hogwild”并行性，因此每次运行都产生不确定的解决方案。
    - coord_descent：普通坐标下降算法。同样是多线程的，但仍会产生确定性的解决方案。

  - feature_selector：默认= cyclic。特征选择和排序方法

  - - cyclic：通过每次循环一个特征来实现的。
    - shuffle：类似于cyclic，但是在每次更新之前都有随机的特征变换。
    - random：一个随机(有放回)特征选择器。
    - greedy：选择梯度最大的特征。（贪婪选择）
    - thrifty：近似贪婪特征选择（近似于greedy）

  - top_k：要选择的最重要特征数（在greedy和thrifty内）

###### 任务参数

- objective：默认=reg:squarederror，表示最小平方误差。

- - reg:squarederror,最小平方误差。
  - reg:squaredlogerror,对数平方损失。
  - reg:logistic,逻辑回归
  - reg:pseudohubererror,使用伪Huber损失进行回归，这是绝对损失的两倍可微选择。
  - binary:logistic,二元分类的逻辑回归，输出概率。
  - binary:logitraw：用于二进制分类的逻辑回归，逻辑转换之前的输出得分。
  - binary:hinge：二进制分类的铰链损失。这使预测为0或1，而不是产生概率。（SVM就是铰链损失函数）
  - count:poisson –计数数据的泊松回归，泊松分布的输出平均值。
  - survival:cox：针对正确的生存时间数据进行Cox回归（负值被视为正确的生存时间）。
  - survival:aft：用于检查生存时间数据的加速故障时间模型。
  - aft_loss_distribution：survival:aft和aft-nloglik度量标准使用的概率密度函数。
  - multi:softmax：设置XGBoost以使用softmax目标进行多类分类，还需要设置num_class（类数）
  - multi:softprob：与softmax相同，但输出向量，可以进一步重整为矩阵。结果包含属于每个类别的每个数据点的预测概率。
  - rank:pairwise：使用LambdaMART进行成对排名，从而使成对损失最小化。
  - rank:ndcg：使用LambdaMART进行列表式排名，使标准化折让累积收益（NDCG）最大化。
  - rank:map：使用LambdaMART进行列表平均排名，使平均平均精度（MAP）最大化。
  - reg:gamma：使用对数链接进行伽马回归。输出是伽马分布的平均值。
  - reg:tweedie：使用对数链接进行Tweedie回归。
  - 自定义损失函数和评价指标：

- eval_metric：验证数据的评估指标，将根据目标分配默认指标（回归均方根，分类误差，排名的平均平均精度），用户可以添加多个评估指标

- - rmse，均方根误差；rmsle：均方根对数误差；mae：平均绝对误差；mphe：平均伪Huber错误；logloss：负对数似然；error：二进制分类错误率；
  - merror：多类分类错误率；mlogloss：多类logloss；auc：曲线下面积；aucpr：PR曲线下的面积；ndcg：归一化累计折扣；map：平均精度；

- seed ：随机数种子，[默认= 0]。

##### LightGBM调参



##### 线性回归

```python
sklearn.linear_model.LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)
model = LinearRegression(normalize=True)
model.fit(data_x, data_y)

model.intercept_, model.coef_
'intercept:'+ str(model.intercept_)
sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True)
```

价格大于90%分位数的部分截断了,就是长尾分布截断

进行log变化

 进行可视化，发现预测结果与真实值较为接近，且未出现异常状况。

#### 模型性能验证

##### 嵌入式特征选择



#### 线性模型回归

线性回归对于特征的要求

标签变化、处理长尾分布

理解线性回归模型

##### 正态性检验

回归分析的因变量要求需服从正态分布，如果出现数据不正态，可以进行对数处理。

##### 散点图和相关分析

回归分析之前需要做相关分析，原因在于相关分析可以先了解是否有关系，回归分析是研究有没有影响关系，有相关关系但并不一定有回归影响关系。当然回归分析之前也可以使用散点图查看数据关系。

###### 相关分析

1. 数据类型：类别、类别：卡方；类别、数值：方差分析
2. 数据正态性（正态图、正态性检验）：非正态（spearman相关系数）
3. 查看线性趋势（散点图）：非线性关系（数据转换、spearman相关系数）
4. 检验异常值（散点图）：存在异常值（异常值剔除）
5. 选择相关系数（Pearson）：非正态：spearman、kendall相关系数

##### 异常值

###### 异常值检测

- 箱线图：实验研究时经常使用，非常直观的展示出异常数据；
- 散点图：研究X和Y的关系时，可直观展示查看是否有异常数据；
- 描述分析：可通过最大最小值等各类指标大致判断数据是否有异常；
- 其它：比如结合正态分布图，频数分析等判断是否有异常值。

###### 异常值判定

- 缺失数字
- 小于设定标准的数字
- 大于设定标准的数字
- 大于3个标准差

###### 异常值处理

- 设置为Null值；此类处理最简单，而且绝大多数情况下均使用此类处理；直接将异常值“干掉”，相当于没有该异常值。如果异常值不多时建议使用此类方法；
- 填补；如果异常值非常多时，则可能需要进行填补设置，SPSSAU共提供平均值，中位数，众数和随机数共四种填补方式。建议使用平均值填补方式。
- 平均值填补：将不满足判断标准外（即正常数据）数据取平均值，对异常数据填补；
- 中位数填补：将不满足判断标准外（即正常数据）数据取中位数，对异常数据填补；
- 众数填补：将不满足判断标准外（即正常数据）数据取众数，对异常数据填补；
- 随机数填补：将不满足判断标准外（即正常数据）数据取随机数（最小和最大值之间），对异常数据填补；

##### 模型后检验

###### 多重共线性

检验多重共线性，可查看分析结果中的VIF值。VIF>5说明存在共线性问题，VIF>10说明存在严重的多重共线性问题，模型构建较差，需要进行处理。

1. 增加分析的样本量，是解释共线性问题的一种办法，但在实际操作中较难实现。

（2）对自变量进行相关分析，找出相关系数高的变量，手工移出后再做线性回归分析。

（3）采用逐步回归法，让系统自动筛选出最优分析项，剔除引起多重共线性的变量。

（4）如果不想涉及核心自变量，不希望剔除，使用岭回归

###### 自相关

残差独立性是线性回归方程的基本前提之一。D-W值可用于判断自相关性，判断标准是2附近即可(1.8~2.2之间)，如果达标说明没有自相关性，即样本之间并没有干扰关系。

如有自相关问题时建议查看因变量Y的数据。

###### 残差正态性

残差正态性也是线性回归方程的基本前提之一。在分析时可保存残差项，然后使用“正态图”直观检测残差正态性情况。

如果残差直观上满足正态性，说明模型构建较好，反之说明模型构建较差。如果残差正态性非常糟糕，建议重新构建模型，比如对Y取对数后再次构建模型等。

###### 异方差

方差齐性可以通过散点图来考察，在分析时可保存残差项，以模型自变量X或因变量Y为横坐标，残差值为纵坐标，作散点图。如果随着预测值的增加，残差值保持相同的离散程度，则说明方差齐。如果残差值随着预测值的增加而变宽或变窄，则说明有异方差问题。关于异方差的检验上，SPSSAU提供两种检验方法，分别是怀特(White)异方差检验和BP检验，通常情况下我们使用怀特(White)异方差检验即可。另外，处理异方差问题有三种办法，分别是数据处理、稳健标准误回归、FGLS回归（可行广义最小二乘法回归）。分别如下：

处理异方差问题有三种办法，分别是数据处理、稳健标准误回归、FGLS回归（可行广义最小二乘法回归）。

- 数据处理

- 针对连续且大于0的原始自变量X和因变量Y，进行取自然对数（或10为底对数）操作，如果是定类数据则不处理。取对数可以将原始数据的大小进行‘压缩’，这样会减少异方差问题。事实上多数研究时默认就进行此步骤处理，而不需要先异方差检验发现有异方差再进行处理。负数不能直接取对数，如果数据中有负数，研究人员可考虑先对小于0的负数，先取其绝对值再求对数，然后加上负数符号。
- Robust稳健标准误回归

- 如果检验显示有异方差问题，可使用Robust稳健标准误回归法进行研究。此种研究方法是当前最为流行也最为有效的处理办法。
- FGLS回归

- 如果发现有异方差问题，还可使用FGLS法进行分析，以处理异方差问题。FGLS是这样的一类思路，即对于残差值越大的点，给予越小的权重，从而解决异方差问题，FGLS回归事实上一系列数据处理的过程，并且它是一种思路。从分析上看，它依然还是使用OLS回归方法进行，具体在案例里面会详细讲解。

#### 模型性能验证

##### 评价函数与目标函数

##### Model Evaluation: quantifying the quality of prediction

For the most common use cases, you can designate a scorer object with the `scoring` parameter; the table below shows all possible values. All scorer objects follow the convention that higher return values are better than lower return values.

The module `sklearn.metrics` also exposes a set of simple functions measuring a prediction error given ground truth and prediction:

- functions ending with `_score` return a value to maximize, the higher the better.
- functions ending with `_error` or `_loss` return a value to minimize, the lower the better. When converting into a scorer object using `make_scorer`, set the `greater_is_better` parameter to `False` 

```python
from sklearn.metrics import fbeta_score, make_scorer
ftwo_scorer = make_scorer(fbeta_score, beta=2)
from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC
grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
                    scoring=ftwo_scorer, cv=5)
```

The second use case is to build a completely custom scorer object from a simple python function using `make_scorer`, which can take several parameters:

- the python function you want to use 
- whether the python function returns a score (greater_is_better=True, the default) or a loss (greater_is_better=False). If a loss, the output of the python function is negated by the scorer object, conforming to the cross validation convention that scorers return higher values for better models.
- for classification metrics only: whether the python function you provided requires continuous decision certainties (needs_threshold=True). The default value is False.
- any additional parameters,

```python
import numpy as np
def my_custom_loss_func(y_true, y_pred):
    diff = np.abs(y_true - y_pred).max()
    return np.log1p(diff)
score = make_scorer(my_custom_loss_func, greater_is_better=False)
```

##### 交叉验证

The performance measure reported by *k*-fold cross-validation is then the average of the values computed in the loop.

![](../../picture/work/26.png)

###### Computing cross-validated metrics

The simplest way to use cross-validation is to call the `cross_val_score` helper function on the estimator and the dataset.
By default, the score computed at each `CV` iteration is the score method of the estimator. It is possible to change this by using the scoring parameter

```python
from sklearn import metrics
scores = cross_val_score(clf, iris.data, iris.target, cv=5, scoring='f1_macro')
```

When the `cv` argument is an integer, `cross_val_score` uses the `KFold` or `StratifiedKFold` strategies by default,
It is also possible to use other cross validation strategies by passing a cross validation iterator instead,

```python
from sklearn.model_selection import ShuffleSplit
cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
cross_val_score(clf, iris.data, iris.target, cv=cv) 
```

###### The `cross_validate` function and multiple metric evaluation

The `cross_validate` function differs from `cross_val_score` in two ways:

- It allows specifying multiple metrics for evaluation.
- It returns a `dict` containing fit-times, score-times in addition to the test score.

For single metric evaluation, where the scoring parameter is a string, callable or None, the keys will be - `['test_score', 'fit_time', 'score_time']`

And for multiple metric evaluation, the return value is a `dict` with the following keys - `['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']`

###### Obtaining predictions by cross-validation

The function `cross_val_predict` has a similar interface to `cross_val_score`, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only `cross-validation` strategies that assign all elements to a test set exactly once can be used.
the function `cross_val_predict` is appropriate for:

- Visualization of predictions obtained from different models.
- Model blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.

###### Cross validation iterators

Assuming that some data is Independent and Identically Distributed is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples. The following `cross-validators` can be used in such cases.

**K-fold**

![](../../picture/work/27.png)

**Repeated K-Fold**: `RepeatedKFold` repeats `K-Fold` n times. It can be used when one requires to run `KFold` n times, producing different splits in each repetition.

Similarly, `RepeatedStratifiedKFold` repeats Stratified K-Fold n times with different randomization in each repetition

**Leave One Out**: `LeaveOneOut` is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. 

**Leave P Out:**`LeavePOut` is very similar to `LeaveOneOut` as it creates all the possible training/test sets by removing  samples from the complete set. 

**Shuffle Split**: The `ShuffleSplit` iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.

![](../../picture/work/28.png)

**Stratified K-Fold**: `StratifiedKFold` is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.

![](../../picture/work/29.png)

**Stratified Shuffle Split**: `StratifiedShuffleSplit` is a variation of `ShuffleSplit`, which returns stratified splits, which creates splits by preserving the same percentage for each target class as in the complete set.

![](../../picture/work/30.png)

###### Cross-validation iterators for grouped data.

The `i.i.d.` assumption is broken if the underlying generative process yield groups of dependent samples.

**Group K-Fold**: `GroupKFold` is a variation of k-fold which ensures that the same group is not represented in both testing and training sets.

![](../../picture/work/31.png)

**Time Series Split**: `TimeSeriesSplit` is a variation of k-fold which returns first  folds as train set and the  th fold as test set. Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. 

![](../../picture/work/32.png)

###### 针对时间序列问题的验证

###### 绘制学习率曲线

A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data.

```python
from sklearn.model_selection import learning_curve
from sklearn.svm import SVC

train_sizes, train_scores, valid_scores = learning_curve(
    SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5) 
```

###### 绘制验证曲线

plot the influence of a single hyper-parameter on the training score and the validation score to find out whether the estimator is over-fitting or under-fitting for some hyper-parameter values.

```python
import numpy as np
from sklearn.model_selection import validation_curve
from sklearn.datasets import load_iris
from sklearn.linear_model import Ridge

np.random.seed(0)
iris = load_iris()
X, y = iris.data, iris.target
indices = np.arange(y.shape[0])
np.random.shuffle(indices)
X, y = X[indices], y[indices]

train_scores, valid_scores = validation_curve(Ridge(), X, y, "alpha", np.logspace(-7, 3, 3), cv=5)
```



