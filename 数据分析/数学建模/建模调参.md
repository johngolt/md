长尾分布：这种分布会使得采样不准，估值不准，因为尾部占了很大部分。另一方面，尾部的数据少，人们对它的了解就少，那么如果它是有害的，那么它的破坏力就非常大，因为人们对它的预防措施和经验比较少。

##### 调参方法

贪心调参 （坐标下降）

坐标下降法是一类优化算法，其最大的优势在于不用计算待优化的目标函数的梯度。最容易想到一种特别朴实的类似于坐标下降法的方法，与坐标下降法不同的是，不是循环使用各个参数进行调整，而是贪心地选取了对整体模型性能影响最大的参数。参数对整体模型性能的影响力是动态变化的，故每一轮坐标选取的过程中，这种方法在对每个坐标的下降方向进行一次直线搜索（line search）

网格调参GridSearchCV

作用是在指定的范围内可以自动调参，只需将参数输入即可得到最优化的结果和参数。相对于人工调参更省时省力，相对于for循环方法更简洁灵活，不易出错。

贝叶斯调参

贝叶斯优化通过基于目标函数的过去评估结果建立替代函数（概率模型），来找到最小化目标函数的值。贝叶斯方法与随机或网格搜索的不同之处在于，它在尝试下一组超参数时，会参考之前的评估结果，因此可以省去很多无用功。

#### 建模与调参

##### 线性回归

```python
sklearn.linear_model.LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)
model = LinearRegression(normalize=True)
model.fit(data_x, data_y)

model.intercept_, model.coef_
'intercept:'+ str(model.intercept_)
sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True)
```

价格大于90%分位数的部分截断了,就是长尾分布截断

进行log变化

 进行可视化，发现预测结果与真实值较为接近，且未出现异常状况。

##### 模型性能验证

###### 评价函数与目标函数

###### 交叉验证

###### 针对时间序列问题的验证

###### 绘制学习率曲线

###### 绘制验证曲线

##### 嵌入式特征选择

##### 模型对比

##### 非线性模型

SVM通过寻求结构化风险最小来提高学习机泛化能力,基本模型定义为特征空间上的间隔最大的线性分类器支持向量机的学习策略便是间隔最大化。SVR：用于标签连续值的回归问题SVC：用于分类标签的分类问题

Boosting一堆弱分类器的组合就可以成为一个强分类器；不断地在错误中学习，迭代来降低犯错概率通过一系列的迭代来优化分类结果，每迭代一次引入一个弱分类器，来克服现在已经存在的弱分类器组合的短板。
Adaboost整个训练集上维护一个分布权值向量W，用赋予权重的训练集通过弱分类算法产生分类假设（基学习器）y(x)， 然后计算错误率,用得到的错误率去更新分布权值向量w，对错误分类的样本分配更大的权值,正确分类的样本赋予更小的权值，每次更新后用相同的弱分类算法产生新的分类假设,这些分类假设的序列构成多分类器，对这些多分类器用加权的方法进行联合,最后得到决策结果
Gradient Boosting迭代的时候选择梯度下降的方向来保证最后的结果最好。损失函数用来描述模型的'靠谱'程度,假设模型没有过拟合,损失函数越大,模型的错误率越高。如果我们的模型能够让损失函数持续的下降,最好的方式就是让损失函数在其梯度方向下降。
GradientBoostingRegressor()loss - 选择损失函数，默认值为ls(least squres),即最小二乘法,对函数拟合learning_rate - 学习率n_estimators - 弱学习器的数目,默认值100max_depth - 每一个学习器的最大深度,限制回归树的节点数目,默认为3min_samples_split - 可以划分为内部节点的最小样本数,默认为2min_samples_leaf - 叶节点所需的最小样本数,默认为1
MLPRegressor()
参数详解
hidden_layer_sizes - hidden_layer_sizes=(50, 50),表示有两层隐藏层，第一层隐藏层有50个神经元,第二层也有50个神经元activation - 激活函数   {‘identity’, ‘logistic’, ‘tanh’, ‘relu’},默认relu
identity - f(x) = x
logistic - 其实就是sigmod函数,f(x) = 1 / (1 + exp(-x))
tanh - f(x) = tanh(x)
relu - f(x) = max(0, x)
solver - 用来优化权重     {‘lbfgs’, ‘sgd’, ‘adam’},默认adam,
lbfgs - quasi-Newton方法的优化器:对小数据集来说,lbfgs收敛更快效果也更好
sgd - 随机梯度下降
adam - 机遇随机梯度的优化器
alpha - 正则化项参数,可选的，默认0.0001
learning_rate - 学习率,用于权重更新,只有当solver为’sgd’时使用
max_iter - 最大迭代次数,默认200
shuffle - 判断是否在每次迭代时对样本进行清洗,默认True,只有当solver=’sgd’或者‘adam’时使用

XGBRegressor梯度提升回归树,也叫梯度提升机采用连续的方式构造树,每棵树都试图纠正前一棵树的错误与随机森林不同,梯度提升回归树没有使用随机化,而是用到了强预剪枝从而使得梯度提升树往往深度很小,这样模型占用的内存少,预测的速度也快

```
虽然随机森林模型在此时取得较好的效果，但LGB的效果与其相差不大。对LGB进行调参后结果会得到提高，下面对LGB进行简介。
**LightGBM**``使用的是histogram算法，占用的内存更低，数据分隔的复杂度更低。思想是将连续的浮点特征离散成k个离散值，并构造宽度为k的Histogram。然后遍历训练数据，统计每个离散值在直方图中的累计统计量。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点。``````LightGBM采用leaf-wise生长策略：每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。
```

\```Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。`

```
参数:num_leaves - 控制了叶节点的数目，它是控制树模型复杂度的主要参数,取值应 <= 2 ^（max_depth）
bagging_fraction - 每次迭代时用的数据比例,用于加快训练速度和减小过拟合
feature_fraction - 每次迭代时用的特征比例,例如为0.8时，意味着在每次迭代中随机选择80％的参数来建树，boosting为random forest时用
min_data_in_leaf - 每个叶节点的最少样本数量。它是处理leaf-wise树的过拟合的重要参数。将它设为较大的值，可以避免生成一个过深的树。但是也可能导致欠拟合
max_depth - 控制了树的最大深度,该参数可以显式的限制树的深度
n_estimators - 分多少颗决策树(总共迭代的次数)
objective - 问题类型
regression - 回归任务,使用L2损失函数
regression_l1 - 回归任务,使用L1损失函数
huber - 回归任务,使用huber损失函数fair - 回归任务,使用fair损失函数
mape (mean_absolute_precentage_error) - 回归任务,使用MAPE损失函数
```