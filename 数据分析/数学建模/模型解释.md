#### 模型解释

事实上，每个分类问题的机器学习流程中都应该包括模型理解和模型解释，下面是几个原因：

模型理解和模型解释对于机器学习的作用：模型改进，理解指标特征、分类、预测，进而理解为什么一个机器学习模型会做出这样的决定、什么特征在决定中起最重要作用，能让我们判断模型是否符合常理；模型可信性与透明度；识别和防止偏差。

可解释性特质：

重要性：了解“为什么”可以帮助更深入地了解问题，数据以及模型可能失败的原因。
分类：建模前数据的可解释性、建模阶段模型可解释性、运行阶段结果可解释性。
范围：全局解释性、局部解释性、模型透明度、模型公平性、模型可靠性。
评估：内在还是事后？模型特定或模型不可知？本地还是全局？
特性：准确性、保真性、可用性、可靠性，鲁棒性、通用性等。
人性化解释：人类能够理解决策原因的程度，人们可以持续预测模型结果的程度标示。

模型解释的三个最重要的方面解释如下。

是什么驱动了模型的预测？我们应该能够查询我们的模型并找出潜在的特征交互，以了解哪些特征在模型的决策策略中可能是重要的。这确保了模型的公平性。
为什么模型会做出某个决定？我们还应该能够验证并证明为什么某些关键特征在预测期间驱动模型所做出的某些决策时负有责任。这确保了模型的可靠性。
我们如何信任模型预测？我们应该能够评估和验证任何数据点以及模型如何对其进行决策。对于模型按预期工作的关键利益相关者而言，这应该是可证明且易于理解的。这确保了模型的透明度。

###### 可解释性的标准

内在还是事后？内在可解释性就是利用机器学习模型，该模型本质上是可解释的。事后可解释性意味着选择和训练黑匣子模型 并在训练后应用可解释性方法。
模型特定或模型不可知？特定于模型的解释工具非常特定于内在模型解释方法，这些方法完全依赖于每个模型的功能和特征。这可以是系数，p值，与回归模型有关的AIC分数，来自决策树的规则等等。与模型无关的工具与事后方法更相关，可用于任何机器学习模型。这些不可知方法通常通过分析特征输入和输出对来操作。根据定义，这些方法无法访问任何模型内部，如权重，约束或假设。
本地还是全局？这种解释分类讨论了解释方法是解释单个预测还是整个模型行为？或者如果范围介于两者之间？我们将很快谈论全球和地方的解释。

全局可解释：就是试图理解“模型如何进行预测？”和“模型的子集如何影响模型决策？”。要立即理解和解释整个模型，我们需要全局可解释性。全局可解释性是指能够基于完整数据集上的依赖（响应）变量和独立（预测变量）特征之间的条件交互来解释和理解模型决策。尝试理解特征交互和重要性始终是理解全球解释的一个很好的一步。当然，在尝试分析交互时，在超过两维或三维之后可视化特征变得非常困难。因此，经常查看可能影响全局知识模型预测的模块化部分和特征子集会有所帮助。全局解释需要完整的模型结构，假设和约束知识。

###### 可解释范围

局部解释：试图理解“为什么模型为单个实例做出具体决策？”和“为什么模型为一组实例做出具体决策？”。对于本地可解释性，我们不关心模型的固有结构或假设，我们将其视为黑盒子。为了理解单个数据点的预测决策，我们专注于该数据点并查看该点周围的特征空间中的局部子区域，并尝试基于该局部区域理解该点的模型决策。本地数据分布和特征空间可能表现完全不同，并提供更准确的解释而不是全局解释。局部可解释模型 - 不可知解释（LIME）框架是一种很好的方法，可用于模型不可知的局部解释。我们可以结合使用全局和局部解释来解释一组实例的模型决策。

模型透明度：为试图理解“如何根据算法和特征创建模型？”。我们知道，通常机器学习模型都是在数据特征之上利用算法来构建将输入映射到潜在输出（响应）的表示。模型的透明度可能试图了解模型的构建方式以及可能影响其决策的更多技术细节。这可以是神经网络的权重，CNN滤波器的权重，线性模型系数，决策树的节点和分裂。但是，由于业务可能不太精通这些技术细节，因此尝试使用不可知的局部和全局解释方法来解释模型决策有助于展示模型透明度



一致性：指的是一个模型的特征重要度，不会因为我们更改了某个特征，而改变其重要度。不一致性可能会导致具有重要度较大的特征比具有重要度较小的特征更不重要。
个体化：指的是重要度的计算是可以针对个体，而不需要整个数据集一起计算。

Split Count：即分裂次数统计，指的是给定特征被用于分裂的次数

##### LIME

###### 问题

 **trusting a prediction**, whether a user trusts an individual prediction sufficiently to take some action based on it, and **trusting a model**, whether the user trusts a model to behave in reasonable ways if deployed. 

Humans usually have prior knowledge about the application domain, which they can use to accept or reject a prediction if they understand the reasoning behind it. 

###### Desired Characteristics for Explainers

1. An essential criterion for explanations is that they must be interpretable, provide qualitative understanding between joint values of input variables and the resulting predicted response value

2. Another essential criterion is local fidelity. Although it is often impossible for an explanation to be completely faithful unless it is the complete description of the model itself, for an explanation to be meaningful it must at least be locally faithful, it must correspond to how the model behaves in the vicinity of the instance being predicted. We note that local fidelity does not imply global fidelity: features that are globally important may not be important in the local context, and vice versa.

3. an explainer must be able to explain any model, and thus be model-agnostic

4. In addition to explaining predictions, providing a global perspective is important to ascertain trust in the model. 

###### LIME

We denote $x\in R^d$ be the original representation of an instance being explained, and we use $x{\prime}\in \{0, 1\}^{d^{\prime}}$ to denote a binary vector for its interpretable representation. we define an explanation as a model$g \in G$, where $G$ is a class of potentially interpretable models, . Note that the domain of $g$ is $\{0, 1\}^{d^\prime}$, $g$ acts over absence/presence of the interpretable components. $Ω(g)$ be a measure of complexity of the explanation $g \in G$. the model being explained be denoted $f :R^d → R$. $Π_x(z)$ as a proximity measure between an instance $z$ to $x$,  $L(f, g, Π_x)$ be a measure of how unfaithful g is in approximating f in the locality defined by $Π_x$

$$
\xi(x)=\operatorname{argmin}_{g \in G} \mathcal{L}\left(f, g, \Pi_{x}\right)+\Omega(g)
$$
in order to learn the local behavior of $f$ as the interpretable inputs vary, we approximate $L(f, g, Π_x)$ by drawing samples, weighted by $Π_x$. We sample instances around $x^\prime$ by drawing nonzero elements of $x^\prime$ uniformly at random. Given a perturbed sample $z^\prime \in \{0, 1\}^{d\prime}$ (which contains a fraction of the nonzero elements of $x^\prime$), we recover the sample in the original representation $z \in R^d$ and obtain $f(z)$, which is used as a label for the explanation model. Given this dataset $\mathcal{Z}$ of perturbed samples with the associated labels, we optimize$ξ(x)$ to get an explanation $ξ(x)$. 

<img src="D:/MarkDown/picture/work/9.png" style="zoom:80%;" />

We propose to give a global understanding of the model by explaining a set of individual instances. This approach is still model agnostic, and is complementary to computing summary statistics such as held-out accuracy. We represent the time and patience that humans have by a budget $B$ that denotes the number of explanations they are willing to look at in order to understand a model. Given a set of instances $X$, we define the pick step as the task of selecting $B$ instances for the user to inspect.

Given all of the explanations for a set of instances $X$, we construct an $n\times d^\prime$ explanation matrix $W$ that
represents the local importance of the interpretable components for each instance. When using linear models as explanations, for an instance $x_i$ and explanation $g_i = ξ(x_i)$, we set $W_{ij} = |w_{g_{ij}} |$. Further, for each component $j$ in $W$, we let $I_j$ denote the global importance, or representativeness of that component in the explanation space. Intuitively, we want $I$ such that features that explain many different instances have higher importance scores. Concretely for the text applications, we set $I_j = \sqrt{\sum_{i=1}^{n}W_{ij}}$ .

While we want to pick instances that cover the important components, the set of explanations must not
be redundant in the components they show the users, avoid selecting instances with similar explanations.

 define coverage as the set function c, given W and I, which computes the total importance of the features that appear in at least one instance in a set V .
$$
c(V, \mathcal{W}, I)=\sum_{j=1}^{d^{\prime}} \mathbb{1}_{\left[\exists i \in V : \mathcal{W}_{i j}>0\right]} I_{j}
$$
finding the set $V$, $|V | ≤ B$ that achieves highest coverage.
$$
\operatorname{Pick}(\mathcal{W}, I)=\operatorname{argmax}_{V,|V| \leq B} c(V, \mathcal{W}, I)
$$
Let $c(V \cup\{i\}, \mathcal{W}, I)-c(V, \mathcal{W}, I)$ be the marginal coverage gain of adding an instance $i$ to a set $V$. 

![](D:/MarkDown/picture/work/10.png)

##### `Eli5`

There are two main ways to look at a classification or a regression model: inspect model parameters and try to figure out how the model works globally; inspect an individual prediction of a model, try to figure out why the model makes the decision it makes. for the first, `ELI5` provides `eli5.show_weights()` function; for the second it provides `eli5.show_prediction()` function.提供两种算法，分别为`Lime`和Permutation importance.

###### `LIME`解释文本分类器

`TextExplainer` generated a lot of texts similar to the document by removing some of the words, and then trained a white-box classifier which predicts the output of the black-box classifier. The explanation we saw is for this white-box classifier. This approach follows the LIME algorithm; for text data the algorithm is actually pretty straightforward:

- generate distorted versions of the text;
- predict probabilities for these distorted texts using the black-box classifier;
- train another classifier which tries to predict output of a black-box classifier on these texts.

The algorithm works because even though it could be hard or impossible to approximate a black-box classifier globally, approximating it in a small neighborhood near a given text often works well, even with simple white-box classifiers.

```python
from sklearn.pipeline import make_pipeline
c = make_pipeline(vectorizer, rf)
from lime.lime_text import LimeTextExplainer
'''Lime explainers assume that classifiers act on raw text, but sklearn classifiers act on vectorized representation of texts.
this explainer works for any classifier you may want to use, as long as it implements predict_proba.'''
explainer = LimeTextExplainer(class_names=class_names)
'''The classifier got this example right (it predicted atheism).
The explanation is presented below as a list of weighted features.'''
exp.as_list()
# Visualizing explanations
fig = exp.as_pyplot_figure()
#The explanations can also be exported as an html page (which we can render here in this notebook)
exp.show_in_notebook(text=False)
'''Finally, we can also include a visualization of the original document, with the words in the explanations highlighted. Notice how the words that affect the classifier the most are all in the email header.'''
exp.show_in_notebook(text=True)
```

###### 多分类

```python
'''Previously, we used the default parameter for label when generating explanation, which works well in the binary case.
For the multiclass case, we have to determine for which labels we will get explanations, via the 'labels' parameter.Below, we generate explanations for labels 0 and 17.'''
from lime.lime_text import LimeTextExplainer
explainer = LimeTextExplainer(class_names=class_names)
idx = 1340
exp = explainer.explain_instance(newsgroups_test.data[idx],
                    c.predict_proba, num_features=6, labels=[0, 17])
'''Another alternative is to ask LIME to generate labels for the top K classes. This is shown below with K=2.
To see which labels have explanations, use the available_labels function.'''
exp = explainer.explain_instance(newsgroups_test.data[idx], 
                    c.predict_proba, num_features=6, top_labels=2)
print(exp.available_labels())
```

###### 类别和数值

As opposed to lime_text.TextExplainer, tabular explainers need a training set. The reason for this is because we compute statistics on each feature. If the feature is numerical, we compute the mean and std, and discretize it into quartiles. If the feature is categorical, we compute the frequency of each value. For this tutorial, we'll only look at numerical features. We use these computed statistics for two things:
To scale the data, so that we can meaningfully compute distances when the attributes are not on the same scale. To sample perturbed instances - which we do by sampling from a Normal(0,1), multiplying by the std and adding back the mean.

```python
explainer = lime.lime_tabular.LimeTabularExplainer(train, 
            feature_names=iris.feature_names, 
            class_names=iris.target_names, discretize_continuous=True)
'''Our explainer takes in numerical data, even if the features are categorical. We thus transform all of the string attributes into integers, using sklearn's LabelEncoder. We use a dict to save the correspondence between the integer values and the original strings, so that we can present this later in the explanations.'''
categorical_names = {}
for feature in categorical_features:
    le = sklearn.preprocessing.LabelEncoder()
    le.fit(data[:, feature])
    data[:, feature] = le.transform(data[:, feature])
    categorical_names[feature] = le.classes_
'''Finally, we use a One-hot encoder, so that the classifier does not take our categorical features as continuous features. We will use this encoder only for the classifier, not for the explainer - and the reason is that the explainer must make sure that a categorical feature only has one value.'''
encoder = sklearn.preprocessing.OneHotEncoder(
    categorical_features=categorical_features)
encoder.fit(data)
encoded_train = encoder.transform(train)
predict_fn = lambda x: rf.predict_proba(encoder.transform(x))
'''We now create our explainer. The categorical_features parameter lets it know which features are categorical (in this case, all of them). The categorical names parameter gives a string representation of each categorical feature's numerical value, as we saw before.'''
explainer = lime.lime_tabular.LimeTabularExplainer(train ,
                            class_names=['edible', 'poisonous'], 
                            feature_names = feature_names,
                            categorical_features=categorical_features, 
                            categorical_names=categorical_names, 
                            kernel_width=3, verbose=False)
'''Now note that the explanations are based not only on features, but on feature-value pairs. For example, we are saying that odor=foul is indicative of a poisonous mushroom. In the context of a categorical feature, odor could take many other values (see below). Since we perturb each categorical feature drawing samples according to the original training distribution, the way to interpret this is: if odor was not foul, on average, this prediction would be 0.24 less 'poisonous'. Let's check if this is the case'''
i = 137
exp = explainer.explain_instance(test[i], predict_fn, num_features=5)
exp.show_in_notebook()
odor_idx = feature_names.index('odor')
explainer.categorical_names[odor_idx]
foul_idx = 4
non_foul = np.delete(explainer.categorical_names[odor_idx], foul_idx)
non_foul_normalized_frequencies = explainer.feature_frequencies[odor_idx].copy()
non_foul_normalized_frequencies[foul_idx] = 0
non_foul_normalized_frequencies /= non_foul_normalized_frequencies.sum()

average_poisonous = 0
for idx, (name, frequency) in enumerate(zip(explainer.categorical_names[
    odor_idx], non_foul_normalized_frequencies)):
    if name == 'foul':
        continue
    temp[odor_idx] = idx
    p_poisonous = predict_fn(temp.reshape(1,-1))[0,1]
    average_poisonous += p_poisonous * frequency
    print('P(poisonous | odor=%s): %.2f' % (name, p_poisonous))
```

###### 回归

```python
explainer = lime.lime_tabular.LimeTabularExplainer(train, 
                        feature_names=boston.feature_names, 
                        class_names=['price'], 
                    categorical_features=categorical_features,
                        verbose=True, mode='regression')
```

###### `SP-LIME`

```python
from lime import submodular_pick
sp_obj = submodular_pick.SubmodularPick(explainer, train, 
                rf.predict, sample_size=20, num_features=14, 
                    num_exps_desired=5)
[exp.as_pyplot_figure() for exp in sp_obj.sp_explanations]
W=pd.DataFrame([dict(this.as_list()) for this in sp_obj.explanations])

exp=explainer.explain_instance(Xtrain[i],rf.predict_proba,top_labels=3)
exp.available_labels()
import pandas as pd
df=pd.DataFrame({})
for this_label in range(3):
    dfl=[]
    for i,exp in enumerate(sp_obj.sp_explanations):
        l=exp.as_list(label=this_label)
        l.append(("exp number",i))
        dfl.append(dict(l))
    dftest=pd.DataFrame(dfl)
    df=df.append(pd.DataFrame(dfl,
    index=[iris.target_names[this_label] 
        for i in range(len(sp_obj.sp_explanations))]))
```

##### `SHAP`

`SHAP`是由`Shapley value`启发的可加性解释模型。对于每个预测样本，模型都产生一个预测值，`SHAP value`就是该样本中每个特征所分配到的数值。 假设第$i$个样本为$x_i$，第$i$个样本的第$j$个特征为$x_{i,j}$，模型对第$i$个样本的预测值为$y_i$，整个模型的基线，通常是所有样本的目标变量的均值为$y_{base}$，那么`SHAP value`服从以下等式。 
$$
y_{i}=y_{\text {base }}+f\left(x_{i, 1}\right)+f\left(x_{i, 2}\right)+\cdots+f\left(x_{i, k}\right)
$$
 其中$f(x_{i,j})$为$x_{i,j}$的`SHAP`值。直观上看，$f(x_{i,j})$就是第$i$个样本中第$j$个特征对最终预测值$y_i$的贡献值，当$f(x_{i,j})>0$，说明该特征提升了预测值，也正向作用；反之，说明该特征使得预测值降低，有反作用。也可以把一个特征对目标变量影响程度的绝对值的均值作为这个特征的重要性。 

特征重要度，特征的风险趋势和样本在特征上的分布。

##### Feature Importance

###### `xgboost`

`weight`: the number of times a feature is used to split the data across all trees；gain: the average gain across all splits the feature is used in. 特征重要性使用特征在作为划分属性时loss平均的降低量；cover: the average coverage across all splits the feature is used in. 特征重要性使用特征在作为划分属性时对样本的覆盖度；total_gain: the total gain across all splits the feature is used in；total_cover: the total coverage across all splits the feature is used in.

######  `lightgbm`

 If “split”, result contains numbers of times the feature is used in a model. If “gain”, result contains total gains of splits which use the feature.

iteration–Limit number of iterations in the feature importance calculation. If None, if the best iteration exists, it is used; otherwise, all trees are used. If <= 0, all trees are used.

######  `GBDT`

判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。其中关于贡献的计算方式可以是基尼指数或袋外数据错误率。特征$j$的全局重要度通过特征$j$在单颗树中的重要度的平均值来衡量。`gbdt`是根据分裂前后节点的impurity减少量来评估特征重要性，criterion分裂标准有：熵、基尼系数、均方误差、平均绝对误差。`gbdt`中的树全部是回归树，所以impurity计算和节点的分裂标准是`MSE`或MAE.

#####  Partial Dependence 

![](D:/MarkDown/picture/1/128.png)

`Partial Dependence Plot`是依赖于模型本身的，所以我们需要先训练模型。假设我们想研究$y$和特征$\mathbf{X}_1$的关系，那么`PDP`就是一个关于$\mathbf{X}_1$和模型预测值的函数。我们先拟合了一个随机森林模型$RF(\mathbf{X})$，然后用$\mathbf{X}^k_i$表示训练集中第$k$个样本的第$i$个特征，那么PDP的函数就是
$$
f\left(X_{1}\right)=\frac{1}{n} \sum_{k=1}^{n} \operatorname{RF}\left(X_{1}, X_{2}^{k}, X_{3}^{k}, \cdots, X_{n}^{k}\right)
$$
也就是说`PDP`在$\mathbf{X}_1$的值，就是把训练集中第一个变量换成$\mathbf{X}_1$之后，原模型预测出来的平均值。根据$\mathbf{X}_1$的不同取值，$f(\mathbf{X}_1)$就可以练成折线，这个折线就是`Partial Dependence Plot`，横轴是$\mathbf{X}_1$，纵轴就是Partial Dependence。 

##### Permutation Importance

With this insight, the process is as follows:

- Get a trained model.
- Shuffle the values in a single column, make predictions using the resulting dataset. Use these predictions and the true target values to calculate how much the loss function suffered from shuffling.
- Now repeat step 2 with the next column in the dataset, until you have calculated the importance of each column.

Like most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the ± measures how performance varied from one-reshuffling to the next. You'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled data happened to be more accurate than the real data. This happens when the feature didn't matter, but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets.

##### Individual Conditional Expectation Plots

 Traditional ICE plots display one curve for each observation in the training set, but plotting a curve for every observation can result in visualization overload even for data sets of moderate size. Fortunately, you can manage the number of curves that are displayed by sampling or clustering.   You can think of each ICE curve as a kind of simulation that shows what would happen to the model’s prediction if you varied one characteristic of a particular observation. As illustrated in Figure 9, the ICE curve for one observation is obtained by replicating the individual observation over the unique values of the plot variable and scoring each replicate.  

![](D:/MarkDown/picture/1/129.png)

##### A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problem

![](D:/MarkDown/picture/work/20.png)

The method that is probably most frequently used today for high-cardinality attributes is clustering. The basic idea is to reduce the original 1-to-N mapping problem to a 1 -to-K mapping problem, with K<<N. To accomplish this, the cardinality of the data is first reduced by grouping individual values into K sets of values. Then each set is represented with a binary derived input. Thus then encoding first identifies the group to which the value belongs and then sets the corresponding bit in the numerical representation.