### 数据清洗

#### 缺失值处理

插值法：以最可能的值来插补缺失值比全部删除不完全样本所产生的信息丢失要少。

关于缺失值处理的方式， 有几种情况：

- 不处理：针对xgboost等树模型，有些模型有处理缺失的机制，所以可以不处理；
- 如果缺失的太多，可以考虑删除该列；
- 插值补全（均值，中位数，众数，建模预测，多重插补等）；
- 分箱处理，缺失值一个箱。

删除、不处理、统计量、建模预测、多重插补、分箱：缺失值作为一个箱

特征值为连续值：按不同的分布类型对缺失值进行补全：偏正态分布，使用均值代替，可以保持数据的均值；偏长尾分布，使用中值代替，避免受`outlier`的影响；特征值为离散值：使用众数代替。

简单的可以是补一个平均值 (mean)、或者众数；对于含异常值的变量，更健壮的做法是补中位数；还可以通过模型预测缺失值。对于竞赛而言最好不要直接删除，最好另作`特殊编码`，或者想办法最大程度保留缺失值所带来的`信息`。：`统计`样本的缺失值数量，作为新的特征；将缺失数量做一个`排序`，如果发现 3 份数据（train、test、unlabeled）都呈阶梯状，于是就可以根据缺失数量将数据划分为若干部分，作为新的特征；使用`随机森林`中的临近矩阵对缺失值进行`插值`，但要求数据的因变量没有缺失值。

感知压缩补全、矩阵补全

```python
# 丢弃观察值
ind_missing = df[df['num_missing'] > 35].index
df_less_missing_rows = df.drop(ind_missing, axis=0)
# 丢弃特征
cols_to_drop = ['hospital_beds_raion']
df_less_hos_beds_raion = df.drop(cols_to_drop, axis=1)
#缺失值填充
med = df['life_sq'].median()
df['life_sq'] = df['life_sq'].fillna(med)
# 替换缺失值
# categorical
df['sub_area'] = df['sub_area'].fillna('_MISSING_')
# numeric
df['life_sq'] = df['life_sq'].fillna(-999)
```

##### Imputing missing data

One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension. By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values.

Missing values can be replaced by the mean, the median or the most frequent value using the basic `sklearn.impute.SimpleImputer`. The median is a more robust estimator for data with high magnitude variables which could dominate results.

轮询调度算法：A more sophisticated approach is to use the `IterativeImputer` class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A `regressor` is fit on (X, y) for known y. Then, the `regressor` is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for `max_iter` imputation rounds. The results of the final imputation round are returned. 

The version implemented assumes Gaussian (output) variables. If your features are obviously non-Normal, consider transforming them to look more Normal so as to potentially improve performance.

This estimator is still experimental for now, To use it, you need to explicitly import `enable_iterative_imputer`. the `sklearn.impute.IterativeImputer` class is very flexible - it can be used with a variety of estimators to do round-robin regression, treating every variable as an output in turn. some estimators for the purpose of missing feature imputation with `sklearn.impute.IterativeImputer`

`BayesianRidge`,`DecisionTreeRegressor`, `ExtraTreesRegressor`, `KNeighborsRegressor`


In the statistics community, it is common practice to perform multiple imputations, generate m separate imputations for a single feature matrix. Each of these m imputations is then put through the subsequent analysis pipeline. The m final analysis results allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. The above practice is called multiple imputation. `IterativeImputer` can also be used for multiple imputations by applying it repeatedly to the same dataset with different random seeds when `sample_posterior=True`. 

The `MissingIndicator` transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative. `NaN` is usually used as the placeholder for missing values. However, it enforces the data type to be float. The parameter `missing_values` allows to specify other placeholder such as integer.

##### 异常值处理

常用的异常值处理操作包括BOX-COX转换（处理有偏分布），箱线图分析删除异常值， 长尾截断等方式， 当然这些操作一般都是处理数值型的数据。

- BOX-COX转换：用于连续的变量不满足正态的时候，在做线性回归的过程中，一般需要做线性模型假定。
- 箱线图分析：依据实际数据绘制，真实、直观地表现出了数据分布的本来面貌，其判断异常值的标准以四分位数和四分位距为基础。

常见处理方法：不处理，例如对于数模型，如`LightGBM`和`XGBoost`，这类对异常值不敏感的算法来说不太需要处理；把异常值的处理用缺失值的处理的思路来处理，比如mean、median进行填补；通过分箱进行泛化处理，在风控系统中，使用`lr`的时候很常用的处理手段；很多可能是业务异常的问题，所以可以结合业务和实际的情况进行处理，比如用户保密填充为-999，还有种是错误的导入导致的；

常用的异常值处理操作包括BOX-COX转换（处理有偏分布），箱线图分析删除异常值， 长尾截断等方式， 当然这些操作一般都是处理数值型的数据。

通过箱线图（$3-\sigma$分析删除异常值

BOX-COX转换（处理有偏分布）

长尾截断

处理连续性数据特征如比例或者百分比类型的特征时，我们不需要高精度的原始数值，通常我们将其舍入近似到数值整型就够用了，这些整型数值可以被视作类别特征或者原始数值（即离散特征）都可以。

对于连续型数值特征，超出合理范围的很可能是噪声，需要截断；在保留重要信息的前提下进行截断，截断后的也可作为类别特征；长尾数据可以先进行对数变换，再截断。这样连续数值就没有那么精细了，也能反映出相互之间的差别。

#### 数据分箱

连续值经常离散化或者分离成“箱子”进行分析, 为什么要做数据分桶呢？

- 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；
- 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；
- LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达 能力，加大拟合；
- 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
- 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化



当然还有很多原因，LightGBM 在改进 XGBoost 时就增加了数据分桶，增强了模型的泛化性。现在介绍数据分桶的方式有：

- 等频分桶：区间的边界值要经过选择,使得每个区间包含大致相等的实例数量。比如说 N=10 ,每个区间应该包含大约10%的实例。
- 等距分桶：从最小值到最大值之间,均分为 N 等份；
- Best-KS分桶：类似利用基尼指数进行二分类；
- 卡方分桶：自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。

连续值经常离散化或者分离成“箱子”进行分析, 为什么要做数据分桶呢？

- 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；
- 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰； 
- LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；
- 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
- 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化

###### 二值化

计数特征可以考虑转换为是否的二值化形式，基于要解决的问题构建模型时，通常原始频数或总数可能与此不相关。比如如果我要建立一个推荐系统用来推荐歌曲，我只希望知道一个人是否感兴趣或是否听过某歌曲。我不需要知道一首歌被听过的次数，因为我更关心的是一个人所听过的各种各样的歌曲。

###### WOE分桶

对需要分桶的情况做一个经验性的总结：连续型数值特征的数值分布有偏向的可以分桶；离散型数值特征的数值跨越了不同的数量级可以分桶。分桶可以将连续性数值特征转换为离散型特征，每一个桶代表了某一个范围的连续性数值特征的密度。

$$
W O E_{i}=\ln \left(\frac{B a d_{i}}{\operatorname{Bad}_{T}} / \frac{\operatorname{Good}_{i}}{\operatorname{Good}_{T}}\right)=\ln \left(\frac{\operatorname{Bad}_{i}}{\operatorname{Bad}_{T}}\right)-\ln \left(\frac{\operatorname{Good}_{i}}{\operatorname{Good}_{T}}\right)
$$

$$
\begin{array}{c}{I V_{i}=\left(\frac{B a d_{i}}{B a d_{T}}-\frac{G o o d_{i}}{G o o d_{T}}\right) * W O E_{i}} \\ {=\left(\frac{B a d_{i}}{B a d_{T}}-\frac{G o o d_{i}}{G o o d_{T}}\right) * \ln \left(\frac{B a d_{i}}{B a d_{T}} / \frac{G o o d_{i}}{G o o d_{T}}\right)} \\ {I V=\sum_{i=1}^{n} I V_{i}}\end{array}
$$

WOE和IV的计算步骤：

- 对于连续型变量，进行分箱，可以选择等频、等距，或者自定义间隔；对于离散型变量，如果分箱太多，则进行分箱合并。
- 计每个分箱里的好人数和坏人数，别除以总的好人数和坏人数，得到每个分箱内的边际好人占比和边际坏人占比。
- 计算每个分箱里的$\text{WOE = ln(margin_bad_rate / margin_good_rate)}$
- 检查每个分箱（除null分箱外）里woe值是否满足单调性，若不满足，返回步骤1。
- 计算每个分箱里的IV，最终求和，即得到最终的IV。

分箱后过程中需要注意：分箱时需要注意样本量充足，保证统计意义；若相邻分箱的WOE值相同，则将其合并为一个分箱；当一个分箱内只有好人或坏人时，可对WOE公式进行修正如下
$$
W O E_{i}=\ln \left(\left(\frac{B a d_{i}+0.5}{G o o d_{i}+0.5}\right) /\left(\frac{B a d_{T}}{G o o d_{T}}\right)\right)
$$

在实践中，我们还需**跨数据集检验WOE分箱的单调性**。如果在训练集上保持单调，但在验证集和测试集上发生**翻转而不单调，**那么说明分箱并不合理，需要再次调整。

###### 卡方分桶

`ChiMerge`是监督的、自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起，直到满足确定的停止准则。 
基本思想：对于精确的离散化，相对类频率在一个区间内应当完全一致。因此，如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。 

- 初始化：根据要离散的属性对实例进行排序：每个实例属于一个区间 
- 合并区间，又包括两步骤：计算每一对相邻区间的卡方值 ；将卡方值最小的一对区间合并 
- 预先设定一个卡方的阈值，在阈值之下的区间都合并，阈值之上的区间保持分区间。 
  卡方的计算公式： $\chi^{2}=\Sigma_{i=1}^{m} \Sigma_{j=1}^{k} \frac{\left(A_{i} j-E_{i} j^){2}\right.}{E_{i j}}$

参数说明：$m=2$每次比较相邻两个区间，2个区间比较。k:类别数目。$A_{ij}$：第i区间第j类的实例数量。$R_i$：第i区间的实例数量$\mathrm{R}_{\mathrm{i}}=\Sigma_{\mathrm{j}=1}^{\mathrm{k}} \mathrm{A}_{\mathrm{i} j}$。$C_j$:第j类的实例数量$c_{j}=\sum_{i=1}^{m} A_{i j}$。$N$总的实例数量$\mathrm{N}=\Sigma_{\mathrm{j}=1}^{\mathrm{k}} \mathrm{C}_{\mathrm{j}}$。$E_{ij}=A_{ij}$的期望$E_{i j}=\frac{N_{i} * C_{j}}{N}$。

卡方阈值的确定：先选择显著性水平，再由公式得到对应的卡方值。得到卡方值需要指定自由度，自由度比类别数量小1。例如，有3类，自由度为2，则90%置信度下，卡方的值为4.6。

###### best-KS分桶

##### 特征归一化/标准化

为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。**注意归一化和标准化的区别**：标准化作用于每个特征列，通过去均值和缩放以方差值的方式将样本的所有特征列转化到同一量纲下；归一化作用于每一数据行，通过缩放以原样本的某个范数使得计算样本间相似度的时候有统一的标准。

如果你的数据包含许多异常值，使用均值和方差缩放可能并不是一个很好的选择。这种情况下，你可以使用`robust_scale`以及`RobustScaler`作为替代品。它们对你的数据的中心和范围使用更有鲁棒性的估计。

中心化**稀疏数据**会破坏数据的稀疏结构，因此很少有一个比较明智的实现方式。但是缩放稀疏输入是有意义的，尤其是当几个特征在不同的量级范围时，最推荐的缩放方式是采用**最大绝对值缩放**。

1. 数据本来就不应该是正态的：如果明确知道样本数据所代表的总体本来就不是正态分布的，可以考虑寻求变换，通常都会找到恰当的变换参数。但有些数据也不一定能够变换成功，如分辨力很低的数据(数据的取值很少)，如像客户满意度调查这样的截尾数据，这时可以采用非参数检验来进行分析。
2. 存在异常点：异常点通常可以用直方图或箱线图来查看。如果发现有异常点，通常的做法是先看看这些异常点是怎么来的，要回头检查一下数据收集的过程。产生异常点的原因非常多，不同的过程其原因也不同。如果确认是异常点，可以考虑剔除。但如果找不到产生异常点的原因，它可能就是一个正常数据，此时可以考虑补充抽样，看看能不能把异常点与大多数数据中的空间填补上。

3. 双峰(多峰)数据：产生这样的数据，可能是把两组(或多组)数据混到一起了，可能每组数据都服从正态分布，但混在一起就不行了。恰当的做法是尽可能把数据按不同属性分开分析。

4. 平顶的数据：平顶的数据是指在直方图上看到的图形是相对比较平坦的。原因：不同均值的数据混在一起，或者是数据收集的周期过长，过程发生了缓慢的移动。对于第一种原因，要考虑尽可能把混在一起的数据按其属性分开，每个属性的数据单独分析。对于第二种原因，可以考虑只取近期的数据进行分析，历史数据在当前可能不那么适用了

偏度衡量随机变量概率分布的不对称性，是相对于平均值不对称程度的度量，通过对偏度系数的测量，我们能够判定数据分布的不对称程度以及方向。偏度的衡量是相对于正态分布来说，正态分布的偏度为0，即若数据分布是对称的，偏度为0。若偏度大于0，则分布右偏，即分布有一条长尾在右；若偏度小于0，则分布为左偏，即分布有一条长尾在左；同时偏度的绝对值越大，说明分布的偏移程度越严重。峰度：是研究数据分布陡峭或者平滑的统计量，通过对峰度系数的测量，我们能够判定数据相对于正态分布而言是更陡峭还是更平缓。若峰度 = 0 , 分布的峰态服从正态分布；若峰度>0,分布的峰态陡峭（高尖）；若峰度<0,分布的峰态平缓；

**对偏态分布进行处理的原因**：而很多模型要求：误差服从独立同分布，时间序列平稳。这需要寻找一种方式让数据尽量满足假设，让方差恒定，即让波动率相对稳定。右偏的。取对数可以将大于中位数的值按一定比例缩小，从而形成正态分布的数据

###### 非线性转换

**映射到均分分布上的转换**：利用分位点信息来转换特征使之符合均匀分布，这种转换倾向于将最常见的数值打散，如此能减少异常值的影响。 然而，该转换确实扭曲了特征内部和特征之间的相关性和距离。**映射到正态分布上的转换**：如果数据不是正态分布的，比如说出现长尾现象的，尤其是数据的平均数和中位数相差很大的时候。这里主要采用一种叫做$\text{Power Transformer}$的方法，这种转换通过一些列参数单调变换使得数据更符合正态分布。$\text{PowerTransformer}$现在支持两种转换，两者都有一个参数 $λ$需要设定：$\text{Box-Cox}$转换：要求输入数据严格为正数。$\text{Yeo-Johnson}$变换：正数或负数。

标准化、归一化、幂律分布处理

数据转换的方式有：

- 数据归一化(MinMaxScaler)；
- 标准化(StandardScaler)；
- 对数变换(log1p)；
- 转换数据类型(astype)；
- 独热编码(OneHotEncoder)；
- 标签编码(LabelEncoder)；
- 修复偏斜特征(boxcox1p)等。

#### stats

##### 正态检验

probplot:

normaltest: 

kurtosistest:

skewtest:

anderson: t tests the null hypothesis that a sample is drawn from a population that follows a particular distribution.

shapiro: The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.

jarque_bera: tests whether the sample data has the skewness and kurtosis matching a normal distribution.

ks_1samp, kstest:  the distribution F(x) of an observed random variable against a given distribution G(x). Under the null hypothesis, the two distributions are identical, F(x)=G(x).

##### 数据转换

boxcox

yeo

##### 相关性分析

###### 数值*数值

linregress: Calculate a linear least-squares regression for two sets of measurements.

pearsonr: Pearson correlation coefficient and p-value for testing non-correlation.

spearmanr: Calculate a Spearman correlation coefficient with associated p-value.

kendalltau: Calculate Kendall’s tau, a correlation measure for ordinal data.

siegelslopes: Computes the Siegel estimator for a set of points (x, y).

theilslops: Computes the Theil-Sen estimator for a set of points (x, y).



###### 数值*类别

mood：Mood’s two-sample test for scale parameters is a non-parametric test for the null hypothesis that two samples are drawn from the same distribution with the same scale parameter.

pointbiserialr: The point biserial correlation is used to measure the relationship between a binary variable, x, and a continuous variable, y. 

anderson_ksamp: It tests the null hypothesis that k-samples are drawn from the same population without having to specify the distribution function of that population.

ansari: a non-parametric test for the equality of the scale parameter of the distributions from which two samples were drawn.

wilcoxon:  tests the null hypothesis that two related paired samples come from the same distribution.

epps_singleton_2sample: Test the null hypothesis that two samples have the same underlying probability distribution.x

ks_2samp: This is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution. 

ttest_ind: the null hypothesis that 2 independent samples have identical average (expected) values. xThis test assumes that the populations have identical variances by default.

###### 类别*类别

binon_test：This is an exact, two-sided test of the null hypothesis that the probability of success in a Bernoulli experiment is *p*. 查看各个类别的坏样本率是否符合总体的坏样本率，判断特征对于建模是否有用的方式。

power_divergence: This function tests the null hypothesis that the categorical data has the given frequencies, using the Cressie-Read power divergence statistic.

chisquare: tests the null hypothesis that the categorical data has the given frequencies.x

##### 数据截断

###### 箱型

sigmaclip

###### 分位数

trimboth, trim1

##### 方差分析

fligner: Fligner’s test tests the null hypothesis that all input samples are from populations with equal variances.  be superior in terms of robustness of departures from normality and power

levene: tests the null hypothesis that all input samples are from populations with equal variances. 

bartlett: tests the null hypothesis that all input samples are from populations with equal variances. x

obrientransform: Used to test for homogeneity of variance prior to running one-way stats. Each array in `*args` is one level of a factor.将不同方差的数据集变换为相同方差的。

kruskal: he null hypothesis that the population median of all of the groups are equal. It is a non-parametric version of ANOVA.