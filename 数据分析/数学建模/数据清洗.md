### 数据清洗

```python
# 丢弃观察值
ind_missing = df[df['num_missing'] > 35].index
df_less_missing_rows = df.drop(ind_missing, axis=0)
# 丢弃特征
cols_to_drop = ['hospital_beds_raion']
df_less_hos_beds_raion = df.drop(cols_to_drop, axis=1)
#缺失值填充
med = df['life_sq'].median()
df['life_sq'] = df['life_sq'].fillna(med)
# 替换缺失值
# categorical
df['sub_area'] = df['sub_area'].fillna('_MISSING_')
# numeric
df['life_sq'] = df['life_sq'].fillna(-999)
```

##### Imputing missing data

One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension. By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values.

Missing values can be replaced by the mean, the median or the most frequent value using the basic `sklearn.impute.SimpleImputer`. The median is a more robust estimator for data with high magnitude variables which could dominate results.

轮询调度算法：A more sophisticated approach is to use the `IterativeImputer` class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A `regressor` is fit on (X, y) for known y. Then, the `regressor` is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for `max_iter` imputation rounds. The results of the final imputation round are returned. 

The version implemented assumes Gaussian (output) variables. If your features are obviously non-Normal, consider transforming them to look more Normal so as to potentially improve performance.

This estimator is still experimental for now, To use it, you need to explicitly import `enable_iterative_imputer`. the `sklearn.impute.IterativeImputer` class is very flexible - it can be used with a variety of estimators to do round-robin regression, treating every variable as an output in turn. some estimators for the purpose of missing feature imputation with `sklearn.impute.IterativeImputer`

`BayesianRidge`,`DecisionTreeRegressor`, `ExtraTreesRegressor`, `KNeighborsRegressor`


In the statistics community, it is common practice to perform multiple imputations, generate m separate imputations for a single feature matrix. Each of these m imputations is then put through the subsequent analysis pipeline. The m final analysis results allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. The above practice is called multiple imputation. `IterativeImputer` can also be used for multiple imputations by applying it repeatedly to the same dataset with different random seeds when `sample_posterior=True`. 

The `MissingIndicator` transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative. `NaN` is usually used as the placeholder for missing values. However, it enforces the data type to be float. The parameter `missing_values` allows to specify other placeholder such as integer.



##### Preprocessing data

###### Standardization, or mean removal and variance scaling

many elements used in the objective function of a learning algorithm assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. The preprocessing module further provides a utility class `StandardScaler` that implements the Transformer `API` to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using `MinMaxScaler` or `MaxAbsScaler`, respectively. The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.
`MaxAbsScaler` works in a very similar fashion, but scales in a way that the training data lies within the range [-1, 1] by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse data. Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales.

`MaxAbsScaler` and `maxabs_scale` were specifically designed for scaling sparse data, and are the recommended way to go about this. However, scale and `StandardScaler` can accept `scipy.sparse` matrices as input, as long as `with_mean=False` is explicitly passed to the constructor. Otherwise a `ValueError` will be raised as silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of memory unintentionally. `RobustScaler` cannot be fitted to sparse inputs, but you can use the transform method on sparse inputs. This Scaler removes the median and scales the data according to the quantile range, defaults to `IQR`
Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the `toarray` method of sparse matrices is another option.

###### Non-linear transformation

Quantile transforms put all features into the same desired distribution based on the formula $G^{-1}(F(X))$ where $F$ is the cumulative distribution function of the feature and $G^{-1}$ the quantile function of the desired output distribution . This formula is using the two following facts: if $X$ is a random variable with a continuous cumulative distribution function $F$ then $F(X)$ is uniformly distributed on ; if $U$ is a random variable with uniform distribution on $[0,1]$ then $G^{-1}(U)$has distribution $G$. By performing a rank transformation, a quantile transform smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features.

 Power transforms are a family of parametric transformations that aim to map data from any distribution to as close to a Gaussian distribution. 

`PowerTransformer` currently provides two such power transformations, the `Yeo-Johnson` transform and the Box-Cox transform. The `Yeo-Johnson` transform is given by:



 while the Box-Cox transform is given by 



Box-Cox can only be applied to strictly positive data. In both methods, the transformation is parameterized by λ, which is determined through maximum likelihood estimation. Here is an example of using Box-Cox to map samples drawn from a `lognormal` distribution to a normal distribution. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective.
It is also possible to map data to a normal distribution using `QuantileTransformer` by setting `output_distribution='normal'`. 

###### Normalization

Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.

###### Discretization

Discretization provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes. One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. 

By default the output is one-hot encoded into a sparse matrix and this can be configured with the `encode` parameter. For each feature, the bin edges are computed during `fit` and together with the number of bins, they will define the intervals. Discretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins.

`KBinsDiscretizer` implements different binning strategies, which can be selected with the `strategy` parameter. The ‘`uniform’` strategy uses constant-width bins. The `‘quantile’` strategy uses the quantiles values to have equally populated bins in each feature. The `‘kmeans’` strategy defines bins based on a k-means clustering procedure performed on each feature independently.

###### Feature `binarization`

Feature `binarization` is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution.

###### Generating polynomial features

Often it’s useful to add complexity to the model by considering nonlinear features of the input data. A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms. It is implemented in `PolynomialFeatures`

##### 特征归一化/标准化

如果你的数据包含许多异常值，使用均值和方差缩放可能并不是一个很好的选择。这种情况下，你可以使用`robust_scale`以及`RobustScaler`作为替代品。它们对你的数据的中心和范围使用更有鲁棒性的估计。

中心化**稀疏数据**会破坏数据的稀疏结构，因此很少有一个比较明智的实现方式。但是缩放稀疏输入是有意义的，尤其是当几个特征在不同的量级范围时，最推荐的缩放方式是采用**最大绝对值缩放**。

1. 数据本来就不应该是正态的：如果明确知道样本数据所代表的总体本来就不是正态分布的，可以考虑寻求变换，通常都会找到恰当的变换参数。但有些数据也不一定能够变换成功，如分辨力很低的数据(数据的取值很少)，如像客户满意度调查这样的截尾数据，这时可以采用非参数检验来进行分析。
2. 存在异常点：异常点通常可以用直方图或箱线图来查看。如果发现有异常点，通常的做法是先看看这些异常点是怎么来的，要回头检查一下数据收集的过程。产生异常点的原因非常多，不同的过程其原因也不同。如果确认是异常点，可以考虑剔除。但如果找不到产生异常点的原因，它可能就是一个正常数据，此时可以考虑补充抽样，看看能不能把异常点与大多数数据中的空间填补上。

3. 双峰(多峰)数据：产生这样的数据，可能是把两组(或多组)数据混到一起了，可能每组数据都服从正态分布，但混在一起就不行了。恰当的做法是尽可能把数据按不同属性分开分析。

4. 平顶的数据：平顶的数据是指在直方图上看到的图形是相对比较平坦的。原因：不同均值的数据混在一起，或者是数据收集的周期过长，过程发生了缓慢的移动。对于第一种原因，要考虑尽可能把混在一起的数据按其属性分开，每个属性的数据单独分析。对于第二种原因，可以考虑只取近期的数据进行分析，历史数据在当前可能不那么适用了

偏度衡量随机变量概率分布的不对称性，是相对于平均值不对称程度的度量，通过对偏度系数的测量，我们能够判定数据分布的不对称程度以及方向。偏度的衡量是相对于正态分布来说，正态分布的偏度为0，即若数据分布是对称的，偏度为0。若偏度大于0，则分布右偏，即分布有一条长尾在右；若偏度小于0，则分布为左偏，即分布有一条长尾在左；同时偏度的绝对值越大，说明分布的偏移程度越严重。峰度：是研究数据分布陡峭或者平滑的统计量，通过对峰度系数的测量，我们能够判定数据相对于正态分布而言是更陡峭还是更平缓。若峰度 = 0 , 分布的峰态服从正态分布；若峰度>0,分布的峰态陡峭（高尖）；若峰度<0,分布的峰态平缓；

**对偏态分布进行处理的原因**：而很多模型要求：误差服从独立同分布，时间序列平稳。这需要寻找一种方式让数据尽量满足假设，让方差恒定，即让波动率相对稳定。右偏的。取对数可以将大于中位数的值按一定比例缩小，从而形成正态分布的数据

###### 非线性转换

**映射到均分分布上的转换**：利用分位点信息来转换特征使之符合均匀分布，这种转换倾向于将最常见的数值打散，如此能减少异常值的影响。 然而，该转换确实扭曲了特征内部和特征之间的相关性和距离。**映射到正态分布上的转换**：如果数据不是正态分布的，比如说出现长尾现象的，尤其是数据的平均数和中位数相差很大的时候。这里主要采用一种叫做$\text{Power Transformer}$的方法，这种转换通过一些列参数单调变换使得数据更符合正态分布。$\text{PowerTransformer}$现在支持两种转换，两者都有一个参数 $λ$需要设定：$\text{Box-Cox}$转换：要求输入数据严格为正数。$\text{Yeo-Johnson}$变换：正数或负数。

标准化、归一化、幂律分布处理

数据转换的方式有：

- 数据归一化(MinMaxScaler)；
- 标准化(StandardScaler)；
- 对数变换(log1p)；
- 转换数据类型(astype)；
- 独热编码(OneHotEncoder)；
- 标签编码(LabelEncoder)；
- 修复偏斜特征(boxcox1p)等。

#### stats

##### 正态检验

probplot:

normaltest: 

kurtosistest:

skewtest:

anderson: t tests the null hypothesis that a sample is drawn from a population that follows a particular distribution.

shapiro: The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.

jarque_bera: tests whether the sample data has the skewness and kurtosis matching a normal distribution.

ks_1samp, kstest:  the distribution F(x) of an observed random variable against a given distribution G(x). Under the null hypothesis, the two distributions are identical, F(x)=G(x).

##### 数据转换

boxcox

yeo

##### 相关性分析

###### 数值*数值

linregress: Calculate a linear least-squares regression for two sets of measurements.

pearsonr: Pearson correlation coefficient and p-value for testing non-correlation.

spearmanr: Calculate a Spearman correlation coefficient with associated p-value.

kendalltau: Calculate Kendall’s tau, a correlation measure for ordinal data.

siegelslopes: Computes the Siegel estimator for a set of points (x, y).

theilslops: Computes the Theil-Sen estimator for a set of points (x, y).



###### 数值*类别

mood：Mood’s two-sample test for scale parameters is a non-parametric test for the null hypothesis that two samples are drawn from the same distribution with the same scale parameter.

pointbiserialr: The point biserial correlation is used to measure the relationship between a binary variable, x, and a continuous variable, y. 

anderson_ksamp: It tests the null hypothesis that k-samples are drawn from the same population without having to specify the distribution function of that population.

ansari: a non-parametric test for the equality of the scale parameter of the distributions from which two samples were drawn.

wilcoxon:  tests the null hypothesis that two related paired samples come from the same distribution.

epps_singleton_2sample: Test the null hypothesis that two samples have the same underlying probability distribution.x

ks_2samp: This is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution. 

ttest_ind: the null hypothesis that 2 independent samples have identical average (expected) values. xThis test assumes that the populations have identical variances by default.

###### 类别*类别

binon_test：This is an exact, two-sided test of the null hypothesis that the probability of success in a Bernoulli experiment is *p*. 查看各个类别的坏样本率是否符合总体的坏样本率，判断特征对于建模是否有用的方式。

power_divergence: This function tests the null hypothesis that the categorical data has the given frequencies, using the Cressie-Read power divergence statistic.

chisquare: tests the null hypothesis that the categorical data has the given frequencies.x

##### 数据截断

###### 箱型

sigmaclip

###### 分位数

trimboth, trim1

##### 方差分析

fligner: Fligner’s test tests the null hypothesis that all input samples are from populations with equal variances.  be superior in terms of robustness of departures from normality and power

levene: tests the null hypothesis that all input samples are from populations with equal variances. 

bartlett: tests the null hypothesis that all input samples are from populations with equal variances. x

obrientransform: Used to test for homogeneity of variance prior to running one-way stats. Each array in `*args` is one level of a factor.将不同方差的数据集变换为相同方差的。

kruskal: he null hypothesis that the population median of all of the groups are equal. It is a non-parametric version of ANOVA.