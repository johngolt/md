### 数据建模

1. Perform an Exploratory Data Analysis (EDA) on your data set;
2. Build a quick and dirty model, or a baseline model, which can serve as a comparison against later models that you will build;
3. Iterate this process. You will do more EDA and build another model;
4. Engineer features: take the features that you already have and combine them or extract more information from them to eventually come to the last point.

5. Get a model that performs better.

##### 数据源

结构化数据和非结构化数据：如一些以表格形式进行存储的数据，都是结构化数据；而非结构化数据就是一堆数据，类似于文本、报文、日志之类的。

对于不同形态的金融产品，样本特征集的设计框架彼此不同，但设计出发点都是为了体现用户的全部潜在风险。以个人信贷为例，样本特征集应该至少包括客户基础信息、申请行为信息、历史借贷信息、社交属性信息、征信查询信息、个人及家庭资产信息、历史交易类信息、地理信息。

对于小微金融信贷的特征集设计，除了个人信息之外，还会加入税务、上下游供应商、发票、公司社保缴纳、水电费、公司财务等体现企业经营稳定性和发展力的特征数据，综合评估小微企业的还款能力和小微企业主的还款意愿。

###### 目标变量界定

Flow rate迁徙率：前期资产等级，落入下一期的比率。
`vintage`账龄分析：以贷款的账龄为基础，观察贷后N个月的逾期比率，用于分析各时期的贷款贷后质量，可推测至当时放款月的进件客群变化以及策略规则调整对放款贷后质量的影响。

如果以客户授信开始计算账龄，则账龄是描述了一个客户获取我方额度的时间如果以客户第一笔借款开始计算账龄，则账龄是描述了一个客户真正使用我方产品的时间明确下`vintage`里面账龄的定义，这里的账龄是指单一笔借款在借款后的时间。

小A在 2019-02月有一笔1200的借款 订单号`x1`，分期12期，那在2019-02时间点对于这个`x1`账龄就是0，简称`Mob0`; 在2019-03时间点对于这个`x1`账龄就是1，简称`Mob1`; 在2019-04时间点对于这个`x1 `账龄就是2，简称`Mob2`。因为每个账龄就代表一个月，所以每个账龄内`x1`都会有一个还款日，`x1`的逾期状态都会发生改变，我们可以记录下`x1`在每个账龄的逾期天数，于是我们就得到了`x1`订单在`Mob0,Mob1,Mob2........`的逾期天数，这样就形成了一条描述`x1`在放款后每个账龄逾期表现的曲线。

对于评分卡目标变量Y的界定，我们主要从`Roll Rate`和`Vintage`来观察分析，重点需要考虑三个方面：逾期流转比例；观察期和表现期；样本容量。

 先分析`Roll rate `

![](D:/学习/MarkDown/picture/work/52.png)

`Roll Rate`的定义为在当前催收水平下不同逾期天数转化为坏账的概率。从`Roll Rate`我们看到2017年开始放款，`M0 to M1`的流转率约为7.8%，`M3 to M4`的流转率为100%，也就是说，处于`M3`逾期阶段内的客户基本很难催收，逾期天数大于60天的客户基本为坏客户了。
再分析`Vintage`

![](D:/学习/MarkDown/picture/work/53.png)

Vintage可以关注如下几个方面：

1. 观察每月审批通过客户后第N个月的逾期比率，对比每月波动，通常波动与审批策略调整有关，此波动在数据准备阶段的样本抽样过程需要关注；

2. 逾期分布，集中在通过后的前三个月说明审批的策略有待改进，超过三个月之后才慢慢增加，说明贷中的管理有待提高；

3. 确定逾期率在经历第N期趋于稳定；

 从上图Vintage分析，每月放款逾期`M2+`以上的剩余本金逾期率基本在`MOB=8`期时趋于稳定，如果我们的放款时间累积比较长，样本表现期可以覆盖到8期，那么就可以界定样本目标变量为在8期内逾期天数大于60天的客户为坏客户，也就是Y=1；如果样本的表现期不够8期，那我们就要再综合考虑流转率和帐龄，重新定义满足样本表现期的逾期天数。 

###### 样本分层

 样本分层就是将整体样本按照不同的数据特征表现分离开来，保证每个分层后的样本具备各自的样本代表性。 有了样本分层，那我们就要做一步模型选择了。因为整体样本的拆分，不同样本成了新的观察对象，所以我们就要针对不同样本去选择合适的模型和算法。

###### 拆分策略

在构建出样本特征集之后，需要按照模型开发、模型验证、模型测试三个环节，将总样本拆分成训练样本、验证样本、测试样本和近期样本。训练样本和验证样来自于总样本并在同一时间区间内，可以按照一定比例进行样本抽取，一般训练样本：验证样本=`7:3`或者N折交叉抽取。测试样本来自相邻“未来”时间区间，用以测试模型的预测能力、区分能力、排序能力、稳定性等指标，确保模型在不同时间段上仍有一定“活性”。近期样本是在开发前仅三个月左右的样本，没有表现结果。近期样本主要用来确保评分卡特征变量的稳定。

### 特征工程

特征工程又包含了`Feature Selection`特征选择、`Feature Extraction`特征提取和`Feature construction`特征构造等子问题

特征工程是将原始数据，通过业务逻辑理解、数据变换、特征交叉与组合等方式，量化成模型训练和预测可直接使用的特征的过程。其中主要包括了数据认知，数据清洗，特征提取，特征选择四个部分。

1. 数据认知：基于实际业务场景理解数据内容，发现数据与研究问题的关系。

2. 数据清洗：对数据进行规整，移除重复变量、处理缺失、异常数据等。

3. 特征提取：通过业务理解和技术实施，构造出描述研究问题的特征。

4. 特征选择：在构造的特征中筛选出最能刻画研究问题的特征。

在传统金融机构中，通过业务中的金融逻辑来反应借款人的风险是常用方法，例如：通过收入水平评估偿还能力，能力越大风险越小；通过历史还款记录评估还款意愿，记录越好风险越小；通过抵押物估值评估风险，估值越高风险越小。

#### 数据认知

##### 数据分析

数据探索可以帮助我们更好地了解数据的性质以及干净程度，包括数据的大小，数据的缺失值的分布，训练集与测试集的分布差异等，同时，数据集中的奇异现象又会进一步促进我们对其进行研究与观察，更好地了解业务，并构建相应强特征；

1. 全局分析：包括数据的大小，整体数据的缺失情况等；通过全局的分析，我们可以知道我们数据的整体情况，决定我们采用什么样的机器等等；   
2. 单变量分析：包括每个变量的分布，缺失情况等；通过单变量分析，我们可以进一步的了解每个变量的分布情况，是否有无用的变量(例如全部缺失的列)，是否出现了某些分布奇怪的变量等.   
3. 多变量分析：包括特征变量与特征变量之间的分析以及特征变量与标签之间的分析等；通过多变量分析，很多时候我们可以直接找到一些比较强的特征，此外变量之间的关系也可以帮助我们做一些简单的特征筛选。     

训练集与测试集的分布分析：寻找差异大的变量，这些差异大的变量往往是导致线下和线上差异的核心因素，这有利于我们更好的设计线下的验证方法。在时间序列上核心变量(数值型)的`boxplot`走势；在时间序列上核心变量(字符型)的frequency走势；核心变量的数据重复程度；核心变量的数据缺失程度；核心变量的数据离散程度；最后总结数据质量表现由好到坏的指标

###### 区分定量和定性数据

![](./picture/work/34.png)  

###### 分析特征变量的分布

特征变量为连续值：如果为长尾分布并且考虑使用线性模型，可以对变量进行幂变换或者对数变换。

特征变量为离散值：观察每个离散值的频率分布，对于频次较低的特征，可以考虑统一编码为“其他”类别。

###### 分析目标变量的分布

目标变量为连续值：查看其值域范围是否较大，如果较大，可以考虑对其进行对数变换，并以变换后的值作为新的目标变量进行建模，在这种情况下，需要对预测结果进行逆变换。一般情况下，可以对连续变量进行Box-Cox变换。通过变换可以使得模型更好的优化，通常也会带来效果上的提升。

目标变量为离散值：如果数据分布不平衡，考虑是否需要上采样/下采样；如果目标变量在某个ID上面分布不平衡，在划分本地训练集和验证集的时候，需要考虑分层采样-Stratified Sampling。看目标占比情况，直接 `value_counts()`就可以解决，看看样本是否失衡。

###### 分析变量之间两两的分布和相关度

可以用于发现高相关和共线性的特征。通过对数据进行探索性分析，还可以有助于启发数据清洗和特征抽取，譬如缺失值和异常值的处理，文本数据是否需要进行拼写纠正等。        

#### 数据清洗

###### 常值变量

可以通过`value_counts()`得到特征中值出现的次数，从而可以统计得到每个特征出现的频率，对于某个特征中如果某个值值出现的频率很高可以视为常值特征，将其删除；也可以根据值出现的频率考虑是否对特征进行二进制编码，或者对某些出现次数过少的特征进行合并。

###### 异常值

常见的是通过可视化的方式进行异常值的观察，就是用箱形图和散点图来观察。
常见处理方法：不处理，例如对于数模型，如`LightGBM`和`XGBoost`，这类对异常值不敏感的算法来说不太需要处理；把异常值的处理用缺失值的处理的思路来处理，比如mean、median进行填补；通过分箱进行泛化处理，在风控系统中，使用`lr`的时候很常用的处理手段；很多可能是业务异常的问题，所以可以结合业务和实际的情况进行处理，比如用户保密填充为-999，还有种是错误的导入导致的；

###### 特征缺失值的处理

接着看看有没有空值，直接统计 `isnull().sum()` 的个数，不过需要注意的是，可能统计出来没有缺失，并不是因为真的没有缺失，而且缺失被人用某个特殊值填充了，后面需要对缺失进行合理填充。怎么识别缺失值呢？一般可以通过 `data.describe()` 获取基本的描述性统计，根据均值、标准差、极大极小值等指标，结合变量含义来判断。再接着看不同类别之间的特征值分布情况，可通过画直方图（数值型变量）和计算变量值占比分布（类别变量）来观察。观察不同变量之间的相关性情况，可以通过绘制 相关矩阵的热力图 来观察大体情况。

特征值为连续值：按不同的分布类型对缺失值进行补全：偏正态分布，使用均值代替，可以保持数据的均值；偏长尾分布，使用中值代替，避免受`outlier`的影响；特征值为离散值：使用众数代替。

简单的可以是补一个平均值 (mean)、或者众数；对于含异常值的变量，更健壮的做法是补中位数；还可以通过模型预测缺失值。对于竞赛而言最好不要直接删除，最好另作`特殊编码`，或者想办法最大程度保留缺失值所带来的`信息`。：`统计`样本的缺失值数量，作为新的特征；将缺失数量做一个`排序`，如果发现 3 份数据（train、test、unlabeled）都呈阶梯状，于是就可以根据缺失数量将数据划分为若干部分，作为新的特征；使用`随机森林`中的临近矩阵对缺失值进行`插值`，但要求数据的因变量没有缺失值。

##### 数值特征

![](./picture/work/15.png)

对于数值特征，我们主要考虑的因素是它的**大小**和**分布**，一般分为`连续型`和`离散型`。

###### 截断/离群点盖帽

处理连续性数据特征如比例或者百分比类型的特征时，我们不需要高精度的原始数值，通常我们将其舍入近似到数值整型就够用了，这些整型数值可以被视作类别特征或者原始数值（即离散特征）都可以。

对于连续型数值特征，超出合理范围的很可能是噪声，需要截断；在保留重要信息的前提下进行截断，截断后的也可作为类别特征；长尾数据可以先进行对数变换，再截断。这样连续数值就没有那么精细了，也能反映出相互之间的差别。

###### 离散化

二值化：计数特征可以考虑转换为是否的二值化形式，基于要解决的问题构建模型时，通常原始频数或总数可能与此不相关。比如如果我要建立一个推荐系统用来推荐歌曲，我只希望知道一个人是否感兴趣或是否听过某歌曲。我不需要知道一首歌被听过的次数，因为我更关心的是一个人所听过的各种各样的歌曲。

对需要分桶的情况做一个经验性的总结：连续型数值特征的数值分布有偏向的可以分桶；离散型数值特征的数值跨越了不同的数量级可以分桶。分桶可以将连续性数值特征转换为离散型特征，每一个桶代表了某一个范围的连续性数值特征的密度。

$$
W O E_{i}=\ln \left(\frac{B a d_{i}}{\operatorname{Bad}_{T}} / \frac{\operatorname{Good}_{i}}{\operatorname{Good}_{T}}\right)=\ln \left(\frac{\operatorname{Bad}_{i}}{\operatorname{Bad}_{T}}\right)-\ln \left(\frac{\operatorname{Good}_{i}}{\operatorname{Good}_{T}}\right)
$$

$$
\begin{array}{c}{I V_{i}=\left(\frac{B a d_{i}}{B a d_{T}}-\frac{G o o d_{i}}{G o o d_{T}}\right) * W O E_{i}} \\ {=\left(\frac{B a d_{i}}{B a d_{T}}-\frac{G o o d_{i}}{G o o d_{T}}\right) * \ln \left(\frac{B a d_{i}}{B a d_{T}} / \frac{G o o d_{i}}{G o o d_{T}}\right)} \\ {I V=\sum_{i=1}^{n} I V_{i}}\end{array}
$$

WOE和IV的计算步骤：

- 对于连续型变量，进行分箱，可以选择等频、等距，或者自定义间隔；对于离散型变量，如果分箱太多，则进行分箱合并。
- 计每个分箱里的好人数和坏人数，别除以总的好人数和坏人数，得到每个分箱内的边际好人占比和边际坏人占比。
- 计算每个分箱里的$\text{WOE = ln(margin_bad_rate / margin_good_rate)}$
- 检查每个分箱（除null分箱外）里woe值是否满足单调性，若不满足，返回步骤1。
- 计算每个分箱里的IV，最终求和，即得到最终的IV。

分箱后过程中需要注意：分箱时需要注意样本量充足，保证统计意义；若相邻分箱的WOE值相同，则将其合并为一个分箱；当一个分箱内只有好人或坏人时，可对WOE公式进行修正如下
$$
W O E_{i}=\ln \left(\left(\frac{B a d_{i}+0.5}{G o o d_{i}+0.5}\right) /\left(\frac{B a d_{T}}{G o o d_{T}}\right)\right)
$$

在实践中，我们还需**跨数据集检验WOE分箱的单调性**。如果在训练集上保持单调，但在验证集和测试集上发生**翻转而不单调，**那么说明分箱并不合理，需要再次调整。

`ChiMerge`是监督的、自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起，直到满足确定的停止准则。 
基本思想：对于精确的离散化，相对类频率在一个区间内应当完全一致。因此，如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。 

- 初始化：根据要离散的属性对实例进行排序：每个实例属于一个区间 
- 合并区间，又包括两步骤：计算每一对相邻区间的卡方值 ；将卡方值最小的一对区间合并 
- 预先设定一个卡方的阈值，在阈值之下的区间都合并，阈值之上的区间保持分区间。 
  卡方的计算公式： $\chi^{2}=\Sigma_{i=1}^{m} \Sigma_{j=1}^{k} \frac{\left(A_{i} j-E_{i} j^){2}\right.}{E_{i j}}$

参数说明：$m=2$每次比较相邻两个区间，2个区间比较。k:类别数目。$A_{ij}$：第i区间第j类的实例数量。$R_i$：第i区间的实例数量$\mathrm{R}_{\mathrm{i}}=\Sigma_{\mathrm{j}=1}^{\mathrm{k}} \mathrm{A}_{\mathrm{i} j}$。$C_j$:第j类的实例数量$c_{j}=\sum_{i=1}^{m} A_{i j}$。$N$总的实例数量$\mathrm{N}=\Sigma_{\mathrm{j}=1}^{\mathrm{k}} \mathrm{C}_{\mathrm{j}}$。$E_{ij}=A_{ij}$的期望$E_{i j}=\frac{N_{i} * C_{j}}{N}$。

卡方阈值的确定：先选择显著性水平，再由公式得到对应的卡方值。得到卡方值需要指定自由度，自由度比类别数量小1。例如，有3类，自由度为2，则90%置信度下，卡方的值为4.6。

###### 缩放/归一化

为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。**注意归一化和标准化的区别**：标准化作用于每个特征列，通过去均值和缩放以方差值的方式将样本的所有特征列转化到同一量纲下；归一化作用于每一数据行，通过缩放以原样本的某个范数使得计算样本间相似度的时候有统一的标准。

如果你的数据包含许多异常值，使用均值和方差缩放可能并不是一个很好的选择。这种情况下，你可以使用`robust_scale`以及`RobustScaler`作为替代品。它们对你的数据的中心和范围使用更有鲁棒性的估计。

中心化**稀疏数据**会破坏数据的稀疏结构，因此很少有一个比较明智的实现方式。但是缩放稀疏输入是有意义的，尤其是当几个特征在不同的量级范围时，最推荐的缩放方式是采用**最大绝对值缩放**。

###### 有偏度的正态分布

1. 数据本来就不应该是正态的：如果明确知道样本数据所代表的总体本来就不是正态分布的，可以考虑寻求变换，通常都会找到恰当的变换参数。但有些数据也不一定能够变换成功，如分辨力很低的数据(数据的取值很少)，如像客户满意度调查这样的截尾数据，这时可以采用非参数检验来进行分析。
2. 存在异常点：异常点通常可以用直方图或箱线图来查看。如果发现有异常点，通常的做法是先看看这些异常点是怎么来的，要回头检查一下数据收集的过程。产生异常点的原因非常多，不同的过程其原因也不同。如果确认是异常点，可以考虑剔除。但如果找不到产生异常点的原因，它可能就是一个正常数据，此时可以考虑补充抽样，看看能不能把异常点与大多数数据中的空间填补上。

3. 双峰(多峰)数据：产生这样的数据，可能是把两组(或多组)数据混到一起了，可能每组数据都服从正态分布，但混在一起就不行了。恰当的做法是尽可能把数据按不同属性分开分析。

4. 平顶的数据：平顶的数据是指在直方图上看到的图形是相对比较平坦的。原因：不同均值的数据混在一起，或者是数据收集的周期过长，过程发生了缓慢的移动。对于第一种原因，要考虑尽可能把混在一起的数据按其属性分开，每个属性的数据单独分析。对于第二种原因，可以考虑只取近期的数据进行分析，历史数据在当前可能不那么适用了

偏度衡量随机变量概率分布的不对称性，是相对于平均值不对称程度的度量，通过对偏度系数的测量，我们能够判定数据分布的不对称程度以及方向。偏度的衡量是相对于正态分布来说，正态分布的偏度为0，即若数据分布是对称的，偏度为0。若偏度大于0，则分布右偏，即分布有一条长尾在右；若偏度小于0，则分布为左偏，即分布有一条长尾在左；同时偏度的绝对值越大，说明分布的偏移程度越严重。峰度：是研究数据分布陡峭或者平滑的统计量，通过对峰度系数的测量，我们能够判定数据相对于正态分布而言是更陡峭还是更平缓。若峰度 = 0 , 分布的峰态服从正态分布；若峰度>0,分布的峰态陡峭（高尖）；若峰度<0,分布的峰态平缓；

**对偏态分布进行处理的原因**：而很多模型要求：误差服从独立同分布，时间序列平稳。这需要寻找一种方式让数据尽量满足假设，让方差恒定，即让波动率相对稳定。右偏的。取对数可以将大于中位数的值按一定比例缩小，从而形成正态分布的数据

###### 非线性转换

**映射到均分分布上的转换**：利用分位点信息来转换特征使之符合均匀分布，这种转换倾向于将最常见的数值打散，如此能减少异常值的影响。 然而，该转换确实扭曲了特征内部和特征之间的相关性和距离。**映射到正态分布上的转换**：如果数据不是正态分布的，比如说出现长尾现象的，尤其是数据的平均数和中位数相差很大的时候。这里主要采用一种叫做$\text{Power Transformer}$的方法，这种转换通过一些列参数单调变换使得数据更符合正态分布。$\text{PowerTransformer}$现在支持两种转换，两者都有一个参数 $λ$需要设定：$\text{Box-Cox}$转换：要求输入数据严格为正数。$\text{Yeo-Johnson}$变换：正数或负数。

###### 行统计量

除了对原始数值变量进行处理外，直接对行向量进行统计也作为一类特征。例如统计行向量中的空值个数、零值个数、正负值个数以及均值、方差、最小值、最大值、偏度、峰度等

##### 类别特征

![](./picture/work/16.png)

###### 自然序数编码

序号编码通常用于处理类别间具有大小关系的数据，例如成绩，可以分为低、中、高档，存在大小排序关系。序号编码会按大小关系对类别特征赋予一个数值 ID，转换后保留了大小关系。

使用`OrdinalEncoder`类将类别特征编码到一个 n_samples大小的[0,n_classes−1]内取值的矢量，每个样本仅对应一个 label，即输入大小为 (n_samples,n_features)的数组：

###### 散列编码

假设有一个特征是`location_id`，表示商品的产地。如果对这个特征做`one-hot encoding`，将其转化为dummy variable。商品来自全国各市、全球各国，可能这个`location_id`就有成千上万个数值。转码之后，模型就会增加这一万个dummy变量。这对数据的读取、操作，模型的训练都是极大的挑战。`Hashing trick`就是用哈希函数这个小技巧来降维。若`location_id`都是整数，我们可以对所有的`location_id`取余，`location_id (mod p)`这个取余函数就是我们使用的`hashing function`。很显然进行取余操作之后，我们最多只有`p`个不同的数值了。在此之上再用`one-hot encoding`，我们只增加了`p`列。

###### Embedding

使用嵌入后的向量可以提高其他算法的准确性。因此，如果我们对类别特征做embedding之后，拿到embedding后的特征后，就可以放到其它的一些学习器中进行训练，并且效果在诸多比赛中得到了验证，所以说这是一种处理类别特征的方法。

#### 特征提取

##### 业务特征

 业务特征来源于实际业务场景中的数据，通过这些数据往往可以构造出大量的反应业务特点的特征。 

基本属性特征主要是对研究对象固有的性质和特点的描述，主要涉及身份信息、教育信息、工作信息等，基于这类记录类信息通常解析其内容衍生出可用于量化描述或分类的特征 

![](./picture/work/43.png)

基于详单数据的特征，这类数据的分析，首先是基本特征，而后统计和复杂特征层层递进，依据时间和空间的角度构造各种统计量，反应出研究对象的行为内容和稳定性。

![](./picture/work/44.png) 

关联信息特征，关联信息主要是通过社交数据建立人与人之间的联系，借助知识图谱的方法，对群体或节点的关联路径深度、关系类型、关系权重、关系密集度、关联节点属性等指标进行计算提取，将复杂的关系网络可视化。 

##### 时间序列数据

客户数据库中有三个神奇的要素，这三个要素构成了数据分析最好的指标：① 最近一次消费(Recency)；② 消费频率(Frequency)；③ 消费金额(Monetary)。在众多的客户关系管理`CRM`的分析模式中，`RFM`模型是被广泛提到的。`RFM`模型是衡量客户价值和客户创利能力的重要工具和手段。该模型通过一个客户的近期购买行为、购买的总体频率以及花了多少钱三项指标来描述该客户的价值状况。

消费的功能不仅在于提供的促销信息而已，营销人员的消费报告可以监督事业的健全度。优秀的营销人员会定期查看消费分析，以掌握趋势。月报告如果显示上一次购买很近的客户，(消费为1个月)人数如增加，则表示该公司是个稳健成长的公司；反之，如上一次消费为一个月的客户越来越少，则是该公司迈向不健全之路的征兆

影响复购的核心因素是商品，因此复购不适合做跨类目比较。比如食品类目和美妆类目：食品是属于“半标品”，产品的标品化程度越高，客户背叛的难度就越小，越难形成忠实用户；但是相对美妆，食品又属于易耗品，消耗周期短，购买频率高，相对容易产生重复购买，因此跨类目复购并不具有可比性。

理论上M值和F值是一样的，都带有时间范围，指的是一段时间（通常是1年）内的消费金额，在工作中我认为对于一般店铺的类目而言，产品的价格带都是比较单一的，比如：同一品牌美妆类，价格浮动范围基本在某个特定消费群的可接受范围内，加上单一品类购买频次不高，所以对于一般店铺而言，M值对客户细分的作用相对较弱。

##### 非业务特征

1-to-1衍生方法指对单个特征进行处理输出单个新特征，主要方法有单变量函数变换、顺序特征、缺失特征、分箱特征和WOE转换特征。

1. 单变量的函数变换，属于一种数学变换。常用的变换函数有：绝对值变换；平方、立方变换；对数变换；指数变换；倒数变换。

2. 顺序特征主要应用于对连续型变量的处理，按照一定顺序对变量值进行排序，将其排序位置作为构造变量的取值。其优势在于处理后的特征具有鲁棒性，不受极端值影响 

3. 缺失特征主要是对单个样本数据在所有特征上的缺失值统计，可理解为对用户信息完备度的统计，需注意的是若该值过大或大部分重要特征的值缺失则需要进行特殊处理，由于缺失值过多导致样本信息的大量缺失，通常考虑删掉该条记录。 

4. 分箱主要应用于对连续变量的离散化和多分类值离散变量的合并。离散化后的特征对异常数据有较强的鲁棒性，不易受极端值的影响；且能避免特征中无意义的波动对模型造成的影响，模型会更稳定。 

5. WOE转换是一种有监督的编码方式，将预测类别的集中度的属性作为编码的数值。通俗来讲就是特征取某个值的时候对违约比例的一种影响。 

###### 1-to-N特征衍生

1. `OneHot`编码主要应用于无序的分类变量 
2. 均值编码是针对高基数的类别特征进行处理，当类别特征的实例值过多时进行`OneHot`编码容易引起维度灾难，使得模型效果降低。均值编码在贝叶斯的架构下，利用所要预测的目标变量，有监督地确定最适合这个定性特征的编码方式。它最大的特点是基于经验贝叶斯方法利用已知数据估算先验概率和后验概率，通过对先验概率和后验概率做加权平均计算最终的特征编码值。 

###### N-to-N特征衍生

1. 基于多项式的变换，主要是对现有特征进行多项式特征组合形成新的特征矩阵 

2. 决策树算法衍生特征，在决策树的系列算法中，每个样本都会落入一个叶子结点上，将叶子结点作为新的特征用于训练模型 

####  特征选择

缺失率：一个变量，如果缺失率过高，他所包含的信息往往也比较少，做缺失值填补也会比较困难。区分能力：一般情况下，如果模型中加入过多的指标往往会比较冗余，因此在有限个指标中要使模型的效果更佳，自然需要挑选对坏样本识别能力较强的变量。稳定性：一个优秀的模型，除了能够很好的区分好坏样本以外，还需要足够的稳定，防止随着时间的推移过快衰退的现象出现。因此模型中的每一个变量也需要足够的稳定。要进行判断，可以计算不同时间切片之间该指标的PSI。业务逻辑：对于趋势与业务逻辑不一致的指标，往往也会予以剔除，来满足评分模型的可解释性

当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。当用特征选择的方法来理解数据的时候要留心，特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据。

1. 去掉取值变化小的特征，该方法一般用在特征选择前作为一个预处理的工作，即先去掉取值变化小的特征，然后再使用其他特征选择方法选择特征。  对与连续变量计算方差，通过方差来进行筛选，对于离散变量计算每个值所占比率，如果某个类别特别多，则可以考虑剔除这个特征。如果机器资源充足，并且希望尽量保留所有信息，可以把阈值设置得比较高，或者只过滤离散型特征只有一个取值的特征。

2. `Filter`方法，先进行特征选择，然后去训练学习器，所以特征选择的过程与学习器无关。相当于先对特征进行过滤操作，然后用特征子集来训练分类器。主要思想：对每一维特征“打分”，即给每一维的特征赋予权重，这样的权重就代表着该特征的重要性，然后依据权重排序。
3. `Wrapper`方法，直接把最后要使用的分类器作为特征选择的评价函数，对于特定的分类器选择最优的特征子集。主要思想：将子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。这样就将子集的选择看作是一个优化问题，这里有很多的优化算法可以解决，尤其是一些启发式的优化算法，如GA、$\text{PSO}$、DE、ABC等。主要方法：递归特征消除算法。优点：对特征进行搜索时围绕学习算法展开的，对特征选择的标准/规范是在学习算法的需求中展开的，能够考虑学习算法所属的任意学习偏差，从而确定最佳子特征，真正关注的是学习问题本身。由于每次尝试针对特定子集时必须运行学习算法，所以能够关注到学习算法的学习偏差/归纳偏差，因此封装能够发挥巨大的作用。缺点：运行速度远慢于过滤算法，实际应用用封装方法没有过滤方法流行。
4. `Embedded`方法，将特征选择嵌入到模型训练当中，其训练可能是相同的模型，但是特征选择完成后，还能给予特征选择完成的特征和模型训练出的超参数，再次训练优化。主要思想：在模型既定的情况下学习出对提高模型准确性最好的特征。也就是在确定模型的过程中，挑选出那些对模型的训练有重要意义的特征。主要方法：用带有`L1`正则化的项完成特征选择也可以结合`L2`惩罚项来优化、随机森林平均不纯度减少法/平均精确度减少法。优点：对特征进行搜索时围绕学习算法展开的，能够考虑学习算法所属的任意学习偏差。训练模型的次数小于Wrapper方法，比较节省时间。缺点：运行速度慢。

###### 单变量特征选择

1. `Pearson`相关系数：相关系数计算速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Pearson相关系数能够表征丰富的关系，符合表示关系的正负，绝对值能够表示强度。 相关系数作为特征排序机制，**它只对线性关系敏感**，如果关系是非线性的，即便两个变量具有一一对应的关系，相关系数系数也可能会接近0。 
2. 互信息和最大信息系数： 互信息法也是评价定性自变量对定性因变量的相关性的，但是并不方便直接用于特征选择。它不属于度量方式，也没有办法进行归一化，在不同的数据上的结果无法做比较。  只能用于离散型特征的选择，连续型特征需要先进行离散化才能用互信息进行特征选择，而互信息的结果对离散化的方式很敏感。 
3. 距离相关系数：距离相关系数是为了克服Pearson相关系数的弱点而生的。
4. 基于学习模型的特征排序： 这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。 
5. 卡方检验：只适用于分类问题中离散型特征筛选，不能用于分类问题中连续型特征的筛选，也不能用于回归问题的特征筛选

去掉取值变化小的特征方法一般用在特征选择前作为一个预处理的工作，即先去掉取值变化小的特征，然后再使用其他特征选择方法选择特征。如果机器资源充足，并且希望尽量保留所有信息，可以把阈值设置得比较高，或者只过滤离散型特征只有一个取值的特征。单变量特征选择可以用于理解数据、数据的结构、特点，也可以用于排除不相关特征，但是它不能发现冗余特征。

###### 随机森林选择

1. 平均不纯度减少：CART利用不纯度可以确定节点，对于分类问题，通常采用基尼不纯度，对于回归问题，通常采用的是方差或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的标准。

2. 平均精确度减少 ：通过直接度量每个特征对模型精确率的影响来进行特征选择。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大。对于重要的变量来说，打乱顺序就会降低模型的精确率。

###### 顶层特征选择

1. 稳定性选择：它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果。比如可以统计某个特征被认为是重要特征的频率，例如被选为重要特征的次数除以它所在的子集被测试的次数。

2. 递归特征消除：递归特征消除的主要思想是反复的构建模型然后选出最好的或者最差的的特征，把选出来的特征放到一遍，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。

单变量特征选择可以用于理解数据、数据的结构、特点，也可以用于排除不相关特征，但是它不能发现冗余特征。

正则化的线性模型可用于特征理解和特征选择。相比起`L1`正则化，`L2`正则化的表现更加稳定，`L2`正则化对于数据的理解来说很合适。由于响应变量和特征之间往往是非线性关系，可以采用basis expansion的方式将特征转换到一个更加合适的空间当中，在此基础上再考虑运用简单的线性模型。

随机森林是一种非常流行的特征选择方法，它易于使用。但它有两个主要问题：重要的特征有可能得分很低，关联特征问题；这种方法对特征变量类别多的特征越有利，偏向问题。

当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据。

###### 共线性

通过计算变量的`VIF`，可以检验共线性问题是否存在，通过综合考虑单变量或多变量的AR值判断应该保留哪些变量、剔除哪些变量。常用的检验方法主要有简单相关系数检验法、容限度法、方差扩大因子法、特征值和条件指数法、`Theil`多重共线性效应系数法等。

容限度是由每个自变量$X_j$作为因变量对其他自变量回归时得到的余差比例，即：$\text{Tolerance}_j=1-R_j^2$。其中，$R_j^2$表示第$j$个自变量对其他自变量进行回归得到的判定系数$R^2$。容限度很大时，$R_j^2$很小，说明所$X_j$包含的独立信息很多，可能成为重要解释变量；反之，容限度很小，$R_j^2$很大，说明$X_j$与其他自变量的信息重复性越大，其对因变量$Y$的解释能力越小。容限度的大小是根据研究者的具体需要制定的，通常当容限度小于0.1时，便认为变量$X_j$与其他变量之间的多重共线性超过了容许界限。

方差扩大因子是容限度的倒数。即：$VIF_j=1/Tolerance_j=1/(1-R_j^2 )$。它表示所对应的偏回归系数的方差由于多重共线性而扩大的倍数。一般认为：若$VIF>10$，说明模型中有很强的共线性关系；若条件指数值在10与30间为弱相关，在30与100间为中等相关，大于100为强相关。

 The condition `indices` are computed as the square roots of the ratios of the largest eigenvalue to each successive eigenvalue. Values greater than 15 indicate a possible problem with collinearity; greater than 30, a serious problem. 最大的特征值除以其他特征值后的平方根。

###### 逐步回归

逐步回归是一个不断往模型中增加或删除变量，直到找到最优的变量组合、新增变量不再产生具有统计显著意义的增量效应为止。一般来说，这就是指标筛选的最后一步了，如果使用SAS进行逐步回归往往也能控制住入模变量的显著性，因此此时最需要注意的是模型拟合出来的系数方向是否一致，如果出现不一致的情况，可以考虑在变量清单中剔除掉这部分指标，重新进行逐步回归分析，直到系数方向一致为止。

1. 前向逐步回归：`FS`回归是让所有建模指标变量一个一个地进入回归方程，按照预先设定的显著性检验标准，最显著的指标变量最先进入，然后其次就是次显著的指标变量进入，依次类推。
2. 后向逐步回归：它的逻辑是首先让全部指标变量都进入回归方程，按照预先设定的显著性检验标准，把不显著的变量逐一剔除。
3. 混合逐步回归：按照预先设定的显著性检验标准，逐步加入或者剔除指标变量，可以由前向逐步回归开始，也可以由后向逐步回归开始，例如由前向逐步回归开始，当新指标变量进入时，如果老指标变量不满足预先设定的标准可以后向剔除，而对比前向逐步回归，变量一旦进入，就不再退出

经过特征选择后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法有主成分分析法和线性判别分析，线性判别分析本身也是一个分类模型。 

### 模型选择

在信贷风控领域关于机器学习技术的探索主要分为三个方向。第一个方向，既然构造复杂模型存在着不稳定的风险，最稳妥的方式为使用机器学习或人工智能增加新特征，再使用评分卡模型。第二个方向为传统风控为体，机器学习为用。即特征筛选的标准和规则仍不变，仅替换评分卡模型为复杂模型比如`xgboost`等。第三个方向为大规模样本结合机器学习，保证模型的稳定和泛化。

###### 决策树算法

决策树优点：决策树易于理解和解释，可以可视化分析，容易提取出规则；可以同时处理标称型和数值型数据；测试数据集时，运行速度比较快；决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。

决策树缺点：对缺失数据处理比较困难；容易出现过拟合问题；忽略数据集中属性的相互关联；`ID3`算法计算信息增益时结果偏向数值比较多的特征。

改进措施:对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法；使用基于决策树的combination算法

###### 随机森林

优点：可以计算和比较哪些特征比较重要；训练速度快，容易做成并行化方法；在训练过程中，能够检测到特征之间的影响；对于不平衡数据集来说，随机森林能提供平衡数据集误差的有效方法；如有很大一部分的特征遗失，用RF算法仍然可以维持准确度；抗过拟合能力比较强；

缺点：在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合；对于小数据或者低维数据，可能不能产生很好的分类；

###### `KNN`算法

优点 ：`KNN`是一种在线技术，新数据可以直接加入数据集而不必进行重新训练;`KNN`理论简单，容易实现

缺点：对于样本容量大的数据集计算量比较大；样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多；`KNN`每一次分类都会重新进行一次全局运算；k值大小的选择。

###### `SVM`

优点：解决小样本下机器学习问题。解决非线性问题。无局部极小值问题。可以很好的处理高维数据集。泛化能力比较强。

缺点：对于核函数的高维映射解释力不强，尤其是径向基函数。对缺失数据敏感。

###### 朴素贝叶斯

优点：对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。支持增量式运算。即可以实时的对新增的样本进行训练。朴素贝叶斯对结果解释容易理解。

缺点：由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。

###### 逻辑回归

优点：计算代价不高，易于理解和实现

缺点：容易产生欠拟合。分类精度不高；不能很好地处理大量多类特征或变量；  对于非线性特征，需要进行转换。

###### `Adaboost`

优点：很好的利用了弱分类器进行级联。可以将不同的分类算法作为弱分类器。`AdaBoost`具有很高的精度。相对于bagging算法和Random Forest算法，`AdaBoost`充分考虑的每个分类器的权重。

缺点：`AdaBoost`迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。数据不平衡导致分类精度下降。训练比较耗时，每次重新选择当前分类器最好切分点。

###### 神经网络

神经网络优点：分类准确度高，学习能力极强。对噪声数据鲁棒性和容错性较强。有联想能力，能逼近任意非线性关系。

神经网络缺点：神经网络参数较多，权值和阈值。黑盒过程，不能观察中间结果。学习过程比较长，有可能陷入局部极小值。

##### 评价指标

秩相关系数，顾名思义，秩的相关系数。秩是指样本值的大小在全体样本从小到大排序后所占的次序。我们还要引入一个定义，对于一对数$(X_1, Y_1)$和$(X_2, Y_2)$，如果$X_1>X_2$且$Y_1> Y_2$或者$X_1<X_2$且$Y_1<Y_2$，则称$(X_1,Y_1)$和$(X_2, Y_2)$是一致的；如果$X_1>X_2$且$Y_1<Y_2$或者$X_1<X_2$且$Y_1>Y_2$，则称$(X_1,Y_1)$和$(X_2, Y_2)$是不一致的；如果$X_1=X_2$或$Y_1=Y_2$，则称$(X_1, Y_1)$和$(X_2,Y_2)$是一个`tie`。在评估逻辑回归模型的过程中，通常让逻辑回归中的目标变量作为X，逻辑回归的结果作为Y。记$n_c$为一致对的个数，$n_d$为不一致对的个数，$n_t$为$X$值不等而$Y$值相等的tie的个数，$N$为观测值的个数（样本量），可以有以下几个秩相关系数： 

一致性指标：$c=\frac{n_{c}+0.5 n_{t}}{n_{c}+n_{d}+n_{t}}$

`Gini coefficient`：$D_{Y X}=\frac{n_{c}-n_{d}}{n_{c}+n_{d}+n_{t}}$

`Goodman-Krustal Gamma`：$\Gamma=\frac{n_{c}-n_{d}}{n_{c}+n_{d}}$

`Kendall`：$\tau=\frac{n_{c}-n_{d}}{N(N-1) / 2}$

 同时，`Gini`也等于2倍的`AUC`减一，`AUC`为`ROC`曲线与坐标轴围成的面积。对于二分类问题，这几种计算方法是等价的。 

`ROC`：那么一个模型的特异度可以定义为`TNR=TN/(FP+TN)`，灵敏度可以定义为`TPR=TP/(TP+FN)`。而`ROC`曲线的横坐标是1-特异度=`1-TNR=FP/(FP+TN)=FPR`，纵坐标是灵敏度即`TPR`。`KS`曲线中的所谓“累积比率”其实就是`ROC`曲线中的`TPR`和`FPR`。`ROC`曲线以`FPR`为横轴，`TPR`为纵轴，而`KS`曲线以阈值为横轴，`TPR`、`FPR`为纵轴。所以说，从某种角度看，`ROC`曲线和`KS`曲线其实是一回事。 

$\text{KS(Kolmogorov-Smirnov)}$：KS用于模型风险区分能力进行评估， 指标衡量的是好坏样本累计分部之间的差值。 好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。KS的计算步骤如下： 计算每个评分区间的好坏账户数。 计算每个评分区间的累计好账户数占总好账户数比率和累计坏账户数占总坏账户数比率。 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值，然后对这些绝对值取最大值即得此评分卡的K-S值。

提升度曲线：可以衡量使用这个模型比随机选择对坏样本的预测能力提升了多少倍。通常计算`LIFT`的时候会把模型的最终得分按照从低到高，排序并等频分为10组，计算分数最低的一组对应的`累计坏样本占比/累计总样本占比`就等于`LIFT`值了。从直观上理解，累计坏样本占比相当于是使用模型的情况下最差的这一组能够从所有的坏样本中挑出多少比例的坏样本，而累计总样本占比等于随机抽样的情况下从所有坏样本抽取了多少比例的坏样本。

##### 模型的分析

而对模型的分析部分，则可以帮助我们了解模型哪些数据做的好，哪些数据做的不好，通过此类反馈，我们就可以对错误的数据展开研究，挖掘我们所遗漏的部分，进一步提升我们模型的预测性能。

模型特征重要性分析：`LGB/XGB`等的`importance`、`LR、SVM`的`coeff`等；特征重要性可以结合业务理解，有些奇怪的特征在模型中起着关键的作用，这些可以帮助我们更好地理解我们的业务，同时如果有些特征反常规，我们也可以看出来；可能这些就是过拟合的特征等等；     

模型分割方式分析：可视化模型的预测，包括`LGB`的每一颗数等；这些可以帮助我们很好的理解我们的模型，模型的分割方式是否符合常理也可以结合业务知识一起分析，帮助我们更好的设计模型；       

模型结果分析：这个在回归问题就是看预测的结果的分布；分类一般看混淆矩阵等。这么做可以帮助我们找到模型做的不好的地方，从而更好的修正我们的模型。

