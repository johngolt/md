1. Perform an Exploratory Data Analysis (EDA) on your data set;
2. Build a quick and dirty model, or a baseline model, which can serve as a comparison against later models that you will build;
3. Iterate this process. You will do more EDA and build another model;
4. Engineer features: take the features that you already have and combine them or extract more information from them to eventually come to the last point.

5. Get a model that performs better.

### 数据分析

#### 数据源

结构化数据和非结构化数据：如一些以表格形式进行存储的数据，都是结构化数据；而非结构化数据就是一堆数据，类似于文本、报文、日志之类的。对于不同形态的金融产品，样本特征集的设计框架彼此不同，但设计出发点都是为了体现用户的全部潜在风险。根据具体的产品和客群来确定使用的数据

##### 目标变量界定

Flow rate迁徙率：前期资产等级，落入下一期的比率。
`vintage`账龄分析：以贷款的账龄为基础，观察贷后N个月的逾期比率，用于分析各时期的贷款贷后质量，可推测至当时放款月的进件客群变化以及策略规则调整对放款贷后质量的影响。如果以客户授信开始计算账龄，则账龄是描述了一个客户获取我方额度的时间如果以客户第一笔借款开始计算账龄，则账龄是描述了一个客户真正使用我方产品的时间明确下`vintage`里面账龄的定义，这里的账龄是指单一笔借款在借款后的时间。

小A在 2019-02月有一笔1200的借款 订单号`x1`，分期12期，那在2019-02时间点对于这个`x1`账龄就是0，简称`Mob0`; 在2019-03时间点对于这个`x1`账龄就是1，简称`Mob1`; 在2019-04时间点对于这个`x1 `账龄就是2，简称`Mob2`。因为每个账龄就代表一个月，所以每个账龄内`x1`都会有一个还款日，`x1`的逾期状态都会发生改变，我们可以记录下`x1`在每个账龄的逾期天数，于是我们就得到了`x1`订单在`Mob0,Mob1,Mob2........`的逾期天数，这样就形成了一条描述`x1`在放款后每个账龄逾期表现的曲线。对于评分卡目标变量Y的界定，我们主要从`Roll Rate`和`Vintage`来观察分析，重点需要考虑三个方面：逾期流转比例；观察期和表现期；样本容量。

 先分析`Roll rate `

![](C:/test/gitfile/md/picture/work/52.png)

`Roll Rate`的定义为在当前催收水平下不同逾期天数转化为坏账的概率。从`Roll Rate`我们看到2017年开始放款，`M0 to M1`的流转率约为7.8%，`M3 to M4`的流转率为100%，也就是说，处于`M3`逾期阶段内的客户基本很难催收，逾期天数大于60天的客户基本为坏客户了。
再分析`Vintage`

![](C:/test/gitfile/md/picture/work/53.png)

Vintage可以关注如下几个方面：

1. 观察每月审批通过客户后第N个月的逾期比率，对比每月波动，通常波动与审批策略调整有关，此波动在数据准备阶段的样本抽样过程需要关注；

2. 逾期分布，集中在通过后的前三个月说明审批的策略有待改进，超过三个月之后才慢慢增加，说明贷中的管理有待提高；

3. 确定逾期率在经历第N期趋于稳定；

 从上图Vintage分析，每月放款逾期`M2+`以上的剩余本金逾期率基本在`MOB=8`期时趋于稳定，如果我们的放款时间累积比较长，样本表现期可以覆盖到8期，那么就可以界定样本目标变量为在8期内逾期天数大于60天的客户为坏客户，也就是Y=1；如果样本的表现期不够8期，那我们就要再综合考虑流转率和帐龄，重新定义满足样本表现期的逾期天数。 

样本分层就是将整体样本按照不同的数据特征表现分离开来，保证每个分层后的样本具备各自的样本代表性。 有了样本分层，那我们就要做一步模型选择了。因为整体样本的拆分，不同样本成了新的观察对象，所以我们就要针对不同样本去选择合适的模型和算法。在构建出样本特征集之后，需要按照模型开发、模型验证、模型测试三个环节，将总样本拆分成训练样本、验证样本、测试样本和近期样本。训练样本和验证样来自于总样本并在同一时间区间内，可以按照一定比例进行样本抽取，一般训练样本：验证样本=`7:3`或者N折交叉抽取。测试样本来自相邻“未来”时间区间，用以测试模型的预测能力、区分能力、排序能力、稳定性等指标，确保模型在不同时间段上仍有一定“活性”。近期样本是在开发前仅三个月左右的样本，没有表现结果。近期样本主要用来确保评分卡特征变量的稳定。

#### 数据总览

数据探索可以帮助我们更好地了解数据的性质以及干净程度，包括数据的大小，数据的缺失值的分布，训练集与测试集的分布差异等，同时，数据集中的奇异现象又会进一步促进我们对其进行研究与观察，更好地了解业务，并构建相应强特征；

1. 全局分析：包括数据的大小，整体数据的缺失情况等；通过全局的分析，我们可以知道我们数据的整体情况，决定我们采用什么样的机器等等；   
2. 单变量分析：包括每个变量的分布，缺失情况等；通过单变量分析，我们可以进一步的了解每个变量的分布情况，是否有无用的变量(例如全部缺失的列)，是否出现了某些分布奇怪的变量等.   
3. 多变量分析：包括特征变量与特征变量之间的分析以及特征变量与标签之间的分析等；通过多变量分析，很多时候我们可以直接找到一些比较强的特征，此外变量之间的关系也可以帮助我们做一些简单的特征筛选。     

训练集与测试集的分布分析：寻找差异大的变量，这些差异大的变量往往是导致线下和线上差异的核心因素，这有利于我们更好的设计线下的验证方法。在时间序列上核心变量(数值型)的`boxplot`走势；在时间序列上核心变量(字符型)的frequency走势；核心变量的数据重复程度；核心变量的数据缺失程度；核心变量的数据离散程度；最后总结数据质量表现由好到坏的指标

###### 区分定量和定性数据

![](C:/test/gitfile/md/picture/work/34.png)  

###### 分析特征变量的分布

特征变量为连续值：如果为长尾分布并且考虑使用线性模型，可以对变量进行幂变换或者对数变换。

特征变量为离散值：观察每个离散值的频率分布，对于频次较低的特征，可以考虑统一编码为“其他”类别。

###### 分析目标变量的分布

目标变量为连续值：查看其值域范围是否较大，如果较大，可以考虑对其进行对数变换，并以变换后的值作为新的目标变量进行建模，在这种情况下，需要对预测结果进行逆变换。一般情况下，可以对连续变量进行Box-Cox变换。通过变换可以使得模型更好的优化，通常也会带来效果上的提升。

目标变量为离散值：如果数据分布不平衡，考虑是否需要上采样/下采样；如果目标变量在某个ID上面分布不平衡，在划分本地训练集和验证集的时候，需要考虑分层采样-Stratified Sampling。看目标占比情况，直接 `value_counts()`就可以解决，看看样本是否失衡。

##### 相关统计量

##### 数据类型

#### 数据检测

##### 常值检测

可以通过`value_counts()`得到特征中值出现的次数，从而可以统计得到每个特征出现的频率，对于某个特征中如果某个值值出现的频率很高可以视为常值特征，将其删除；也可以根据值出现的频率考虑是否对特征进行二进制编码，或者对某些出现次数过少的特征进行合并。

##### 缺失值检测

- 查看每列的存在nan情况
- 排序函数sort_values()：将数据集依照某个字段中的数据进行排序,该函数即可根据指定列数据也可根据指定行的
- 可视化nan值与缺失值

接着看看有没有空值，直接统计 `isnull().sum()` 的个数，不过需要注意的是，可能统计出来没有缺失，并不是因为真的没有缺失，而且缺失被人用某个特殊值填充了，后面需要对缺失进行合理填充。怎么识别缺失值呢？一般可以通过 `data.describe()` 获取基本的描述性统计，根据均值、标准差、极大极小值等指标，结合变量含义来判断。再接着看不同类别之间的特征值分布情况，可通过画直方图（数值型变量）和计算变量值占比分布（类别变量）来观察。观察不同变量之间的相关性情况，可以通过绘制 相关矩阵的热力图 来观察大体情况。

##### 异常值检测

- 3σ原则：拉依达准则，该准则具体来说，就是先假设一组检测数据只含有随机误差，对原始数据进行计算处理得到标准差，然后按一定的概率确定一个区间，认为误差超过这个区间的就属于异常值。
- 箱线图：依据实际数据绘制，真实、直观地表现出了数据分布的本来面貌，且没有对数据作任何限制性要求（3σ原则要求数据服从正态分布或近似服从正态分布），其判断异常值的标准以四分位数和四分位距为基础。

#### 预测分布

##### 总体分布

##### 偏度和峰度

#### 特征分析

##### 数值特征

###### 相关性分析

可以用于发现高相关和共线性的特征。通过对数据进行探索性分析，还可以有助于启发数据清洗和特征抽取，譬如缺失值和异常值的处理，文本数据是否需要进行拼写纠正等。对两个或多个具备相关性的变量元素进行分析，从而衡量两个变量因素之间的相关密切程度。

###### 偏度和峰度

###### 可视化

- pd.melt()：处理数据，透视表格，可将宽数据转化为长数据，以便于后续分析。形成的数据即为，键：各特征名称，值：特征对应的值
- sns.FacetGrid() ：先sns.FacetGrid()画出轮廓,再map()填充内容
- sns.pairplot()：展示变量两两之间的关系（线性或非线性，有无较为明显的相关关系）。

##### 类别特征

###### Unique分布

对于一维数组或者列表，unique函数去除其中重复的元素，并按元素由大到小返回一个新的无元素重复的元组或者列表。

###### 可视化

- 箱型图可视化：直观识别数据中的离群点，判断数据离散分布情况，了解数据分布状态。
- 小提琴图可视化：用于显示数据分布及概率密度，这种图表结合了箱形图和密度图的特征，主要用来显示数据的分布形状
- 柱形图可视化类别
- 每个类别频数可视化

### 数据清洗

#### 缺失值处理

删除、不处理、统计量、建模预测、多重插补、分箱：缺失值作为一个箱

特征值为连续值：按不同的分布类型对缺失值进行补全：偏正态分布，使用均值代替，可以保持数据的均值；偏长尾分布，使用中值代替，避免受`outlier`的影响；特征值为离散值：使用众数代替。

简单的可以是补一个平均值 (mean)、或者众数；对于含异常值的变量，更健壮的做法是补中位数；还可以通过模型预测缺失值。对于竞赛而言最好不要直接删除，最好另作`特殊编码`，或者想办法最大程度保留缺失值所带来的`信息`。：`统计`样本的缺失值数量，作为新的特征；将缺失数量做一个`排序`，如果发现 3 份数据（train、test、unlabeled）都呈阶梯状，于是就可以根据缺失数量将数据划分为若干部分，作为新的特征；使用`随机森林`中的临近矩阵对缺失值进行`插值`，但要求数据的因变量没有缺失值。

##### 高维映射

感知压缩补全、矩阵补全

#### 异常值处理

常见处理方法：不处理，例如对于数模型，如`LightGBM`和`XGBoost`，这类对异常值不敏感的算法来说不太需要处理；把异常值的处理用缺失值的处理的思路来处理，比如mean、median进行填补；通过分箱进行泛化处理，在风控系统中，使用`lr`的时候很常用的处理手段；很多可能是业务异常的问题，所以可以结合业务和实际的情况进行处理，比如用户保密填充为-999，还有种是错误的导入导致的；

常用的异常值处理操作包括BOX-COX转换（处理有偏分布），箱线图分析删除异常值， 长尾截断等方式， 当然这些操作一般都是处理数值型的数据。

通过箱线图（$3-\sigma$分析删除异常值

BOX-COX转换（处理有偏分布）

长尾截断

处理连续性数据特征如比例或者百分比类型的特征时，我们不需要高精度的原始数值，通常我们将其舍入近似到数值整型就够用了，这些整型数值可以被视作类别特征或者原始数值（即离散特征）都可以。

对于连续型数值特征，超出合理范围的很可能是噪声，需要截断；在保留重要信息的前提下进行截断，截断后的也可作为类别特征；长尾数据可以先进行对数变换，再截断。这样连续数值就没有那么精细了，也能反映出相互之间的差别。

#### 数据分桶

连续值经常离散化或者分离成“箱子”进行分析, 为什么要做数据分桶呢？

- 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；
- 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰； 
- LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；
- 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
- 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化

###### 二值化

计数特征可以考虑转换为是否的二值化形式，基于要解决的问题构建模型时，通常原始频数或总数可能与此不相关。比如如果我要建立一个推荐系统用来推荐歌曲，我只希望知道一个人是否感兴趣或是否听过某歌曲。我不需要知道一首歌被听过的次数，因为我更关心的是一个人所听过的各种各样的歌曲。

###### WOE分桶

对需要分桶的情况做一个经验性的总结：连续型数值特征的数值分布有偏向的可以分桶；离散型数值特征的数值跨越了不同的数量级可以分桶。分桶可以将连续性数值特征转换为离散型特征，每一个桶代表了某一个范围的连续性数值特征的密度。

$$
W O E_{i}=\ln \left(\frac{B a d_{i}}{\operatorname{Bad}_{T}} / \frac{\operatorname{Good}_{i}}{\operatorname{Good}_{T}}\right)=\ln \left(\frac{\operatorname{Bad}_{i}}{\operatorname{Bad}_{T}}\right)-\ln \left(\frac{\operatorname{Good}_{i}}{\operatorname{Good}_{T}}\right)
$$

$$
\begin{array}{c}{I V_{i}=\left(\frac{B a d_{i}}{B a d_{T}}-\frac{G o o d_{i}}{G o o d_{T}}\right) * W O E_{i}} \\ {=\left(\frac{B a d_{i}}{B a d_{T}}-\frac{G o o d_{i}}{G o o d_{T}}\right) * \ln \left(\frac{B a d_{i}}{B a d_{T}} / \frac{G o o d_{i}}{G o o d_{T}}\right)} \\ {I V=\sum_{i=1}^{n} I V_{i}}\end{array}
$$

WOE和IV的计算步骤：

- 对于连续型变量，进行分箱，可以选择等频、等距，或者自定义间隔；对于离散型变量，如果分箱太多，则进行分箱合并。
- 计每个分箱里的好人数和坏人数，别除以总的好人数和坏人数，得到每个分箱内的边际好人占比和边际坏人占比。
- 计算每个分箱里的$\text{WOE = ln(margin_bad_rate / margin_good_rate)}$
- 检查每个分箱（除null分箱外）里woe值是否满足单调性，若不满足，返回步骤1。
- 计算每个分箱里的IV，最终求和，即得到最终的IV。

分箱后过程中需要注意：分箱时需要注意样本量充足，保证统计意义；若相邻分箱的WOE值相同，则将其合并为一个分箱；当一个分箱内只有好人或坏人时，可对WOE公式进行修正如下
$$
W O E_{i}=\ln \left(\left(\frac{B a d_{i}+0.5}{G o o d_{i}+0.5}\right) /\left(\frac{B a d_{T}}{G o o d_{T}}\right)\right)
$$

在实践中，我们还需**跨数据集检验WOE分箱的单调性**。如果在训练集上保持单调，但在验证集和测试集上发生**翻转而不单调，**那么说明分箱并不合理，需要再次调整。

###### 卡方分桶

`ChiMerge`是监督的、自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起，直到满足确定的停止准则。 
基本思想：对于精确的离散化，相对类频率在一个区间内应当完全一致。因此，如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。 

- 初始化：根据要离散的属性对实例进行排序：每个实例属于一个区间 
- 合并区间，又包括两步骤：计算每一对相邻区间的卡方值 ；将卡方值最小的一对区间合并 
- 预先设定一个卡方的阈值，在阈值之下的区间都合并，阈值之上的区间保持分区间。 
  卡方的计算公式： $\chi^{2}=\Sigma_{i=1}^{m} \Sigma_{j=1}^{k} \frac{\left(A_{i} j-E_{i} j^){2}\right.}{E_{i j}}$

参数说明：$m=2$每次比较相邻两个区间，2个区间比较。k:类别数目。$A_{ij}$：第i区间第j类的实例数量。$R_i$：第i区间的实例数量$\mathrm{R}_{\mathrm{i}}=\Sigma_{\mathrm{j}=1}^{\mathrm{k}} \mathrm{A}_{\mathrm{i} j}$。$C_j$:第j类的实例数量$c_{j}=\sum_{i=1}^{m} A_{i j}$。$N$总的实例数量$\mathrm{N}=\Sigma_{\mathrm{j}=1}^{\mathrm{k}} \mathrm{C}_{\mathrm{j}}$。$E_{ij}=A_{ij}$的期望$E_{i j}=\frac{N_{i} * C_{j}}{N}$。

卡方阈值的确定：先选择显著性水平，再由公式得到对应的卡方值。得到卡方值需要指定自由度，自由度比类别数量小1。例如，有3类，自由度为2，则90%置信度下，卡方的值为4.6。

###### best-KS分桶

#### 特征归一化

为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。**注意归一化和标准化的区别**：标准化作用于每个特征列，通过去均值和缩放以方差值的方式将样本的所有特征列转化到同一量纲下；归一化作用于每一数据行，通过缩放以原样本的某个范数使得计算样本间相似度的时候有统一的标准。

如果你的数据包含许多异常值，使用均值和方差缩放可能并不是一个很好的选择。这种情况下，你可以使用`robust_scale`以及`RobustScaler`作为替代品。它们对你的数据的中心和范围使用更有鲁棒性的估计。

中心化**稀疏数据**会破坏数据的稀疏结构，因此很少有一个比较明智的实现方式。但是缩放稀疏输入是有意义的，尤其是当几个特征在不同的量级范围时，最推荐的缩放方式是采用**最大绝对值缩放**。

1. 数据本来就不应该是正态的：如果明确知道样本数据所代表的总体本来就不是正态分布的，可以考虑寻求变换，通常都会找到恰当的变换参数。但有些数据也不一定能够变换成功，如分辨力很低的数据(数据的取值很少)，如像客户满意度调查这样的截尾数据，这时可以采用非参数检验来进行分析。
2. 存在异常点：异常点通常可以用直方图或箱线图来查看。如果发现有异常点，通常的做法是先看看这些异常点是怎么来的，要回头检查一下数据收集的过程。产生异常点的原因非常多，不同的过程其原因也不同。如果确认是异常点，可以考虑剔除。但如果找不到产生异常点的原因，它可能就是一个正常数据，此时可以考虑补充抽样，看看能不能把异常点与大多数数据中的空间填补上。

3. 双峰(多峰)数据：产生这样的数据，可能是把两组(或多组)数据混到一起了，可能每组数据都服从正态分布，但混在一起就不行了。恰当的做法是尽可能把数据按不同属性分开分析。

4. 平顶的数据：平顶的数据是指在直方图上看到的图形是相对比较平坦的。原因：不同均值的数据混在一起，或者是数据收集的周期过长，过程发生了缓慢的移动。对于第一种原因，要考虑尽可能把混在一起的数据按其属性分开，每个属性的数据单独分析。对于第二种原因，可以考虑只取近期的数据进行分析，历史数据在当前可能不那么适用了

偏度衡量随机变量概率分布的不对称性，是相对于平均值不对称程度的度量，通过对偏度系数的测量，我们能够判定数据分布的不对称程度以及方向。偏度的衡量是相对于正态分布来说，正态分布的偏度为0，即若数据分布是对称的，偏度为0。若偏度大于0，则分布右偏，即分布有一条长尾在右；若偏度小于0，则分布为左偏，即分布有一条长尾在左；同时偏度的绝对值越大，说明分布的偏移程度越严重。峰度：是研究数据分布陡峭或者平滑的统计量，通过对峰度系数的测量，我们能够判定数据相对于正态分布而言是更陡峭还是更平缓。若峰度 = 0 , 分布的峰态服从正态分布；若峰度>0,分布的峰态陡峭（高尖）；若峰度<0,分布的峰态平缓；

**对偏态分布进行处理的原因**：而很多模型要求：误差服从独立同分布，时间序列平稳。这需要寻找一种方式让数据尽量满足假设，让方差恒定，即让波动率相对稳定。右偏的。取对数可以将大于中位数的值按一定比例缩小，从而形成正态分布的数据

###### 非线性转换

**映射到均分分布上的转换**：利用分位点信息来转换特征使之符合均匀分布，这种转换倾向于将最常见的数值打散，如此能减少异常值的影响。 然而，该转换确实扭曲了特征内部和特征之间的相关性和距离。**映射到正态分布上的转换**：如果数据不是正态分布的，比如说出现长尾现象的，尤其是数据的平均数和中位数相差很大的时候。这里主要采用一种叫做$\text{Power Transformer}$的方法，这种转换通过一些列参数单调变换使得数据更符合正态分布。$\text{PowerTransformer}$现在支持两种转换，两者都有一个参数 $λ$需要设定：$\text{Box-Cox}$转换：要求输入数据严格为正数。$\text{Yeo-Johnson}$变换：正数或负数。

标准化、归一化、幂律分布处理



### 特征工程

特征工程又包含了`Feature Selection`特征选择、`Feature Extraction`特征提取和`Feature construction`特征构造等子问题。特征工程是将原始数据，通过业务逻辑理解、数据变换、特征交叉与组合等方式，量化成模型训练和预测可直接使用的特征的过程。其中主要包括了数据认知，数据清洗，特征提取，特征选择四个部分。

1. 数据认知：基于实际业务场景理解数据内容，发现数据与研究问题的关系。

2. 数据清洗：对数据进行规整，移除重复变量、处理缺失、异常数据等。

3. 特征提取：通过业务理解和技术实施，构造出描述研究问题的特征。

4. 特征选择：在构造的特征中筛选出最能刻画研究问题的特征。

在传统金融机构中，通过业务中的金融逻辑来反应借款人的风险是常用方法，例如：通过收入水平评估偿还能力，能力越大风险越小；通过历史还款记录评估还款意愿，记录越好风险越小；通过抵押物估值评估风险，估值越高风险越小。

#### 特征构造

业务特征来源于实际业务场景中的数据，通过这些数据往往可以构造出大量的反应业务特点的特征。 基本属性特征主要是对研究对象固有的性质和特点的描述，主要涉及身份信息、教育信息、工作信息等，基于这类记录类信息通常解析其内容衍生出可用于量化描述或分类的特征 

![](C:/test/gitfile/md/picture/work/43.png)

基于详单数据的特征，这类数据的分析，首先是基本特征，而后统计和复杂特征层层递进，依据时间和空间的角度构造各种统计量，反应出研究对象的行为内容和稳定性。

![](C:/test/gitfile/md/picture/work/44.png) 

关联信息特征，关联信息主要是通过社交数据建立人与人之间的联系，借助知识图谱的方法，对群体或节点的关联路径深度、关系类型、关系权重、关系密集度、关联节点属性等指标进行计算提取，将复杂的关系网络可视化。 

###### 1-to-N特征衍生

1. `OneHot`编码主要应用于无序的分类变量 
2. 均值编码是针对高基数的类别特征进行处理，当类别特征的实例值过多时进行`OneHot`编码容易引起维度灾难，使得模型效果降低。均值编码在贝叶斯的架构下，利用所要预测的目标变量，有监督地确定最适合这个定性特征的编码方式。它最大的特点是基于经验贝叶斯方法利用已知数据估算先验概率和后验概率，通过对先验概率和后验概率做加权平均计算最终的特征编码值。 

###### N-to-N特征衍生

1. 基于多项式的变换，主要是对现有特征进行多项式特征组合形成新的特征矩阵 

2. 决策树算法衍生特征，在决策树的系列算法中，每个样本都会落入一个叶子结点上，将叶子结点作为新的特征用于训练模型 

##### 统计量特征

指通过统计单个或者多个变量的统计值(max,min,count,mean)等而形成新的特征。

「单变量：」
如果某个特征与目标高度相关，那么可以根据具体的情况取这个特征的统计值作为新的特征。
「多变量：」
如果特征与特征之间存在交互影响时，那么可以聚合分组两个或多个变量之后，再以统计值构造出新的特征。

##### 时间特征

客户数据库中有三个神奇的要素，这三个要素构成了数据分析最好的指标：① 最近一次消费(Recency)；② 消费频率(Frequency)；③ 消费金额(Monetary)。在众多的客户关系管理`CRM`的分析模式中，`RFM`模型是被广泛提到的。`RFM`模型是衡量客户价值和客户创利能力的重要工具和手段。该模型通过一个客户的近期购买行为、购买的总体频率以及花了多少钱三项指标来描述该客户的价值状况。

消费的功能不仅在于提供的促销信息而已，营销人员的消费报告可以监督事业的健全度。优秀的营销人员会定期查看消费分析，以掌握趋势。月报告如果显示上一次购买很近的客户，(消费为1个月)人数如增加，则表示该公司是个稳健成长的公司；反之，如上一次消费为一个月的客户越来越少，则是该公司迈向不健全之路的征兆

影响复购的核心因素是商品，因此复购不适合做跨类目比较。比如食品类目和美妆类目：食品是属于“半标品”，产品的标品化程度越高，客户背叛的难度就越小，越难形成忠实用户；但是相对美妆，食品又属于易耗品，消耗周期短，购买频率高，相对容易产生重复购买，因此跨类目复购并不具有可比性。

理论上M值和F值是一样的，都带有时间范围，指的是一段时间（通常是1年）内的消费金额，在工作中我认为对于一般店铺的类目而言，产品的价格带都是比较单一的，比如：同一品牌美妆类，价格浮动范围基本在某个特定消费群的可接受范围内，加上单一品类购买频次不高，所以对于一般店铺而言，M值对客户细分的作用相对较弱。

##### 空间特征

##### 非线性变换

1-to-1衍生方法指对单个特征进行处理输出单个新特征，主要方法有单变量函数变换、顺序特征、缺失特征、分箱特征和WOE转换特征。

1. 单变量的函数变换，属于一种数学变换。常用的变换函数有：绝对值变换；平方、立方变换；对数变换；指数变换；倒数变换。

2. 顺序特征主要应用于对连续型变量的处理，按照一定顺序对变量值进行排序，将其排序位置作为构造变量的取值。其优势在于处理后的特征具有鲁棒性，不受极端值影响 

3. 缺失特征主要是对单个样本数据在所有特征上的缺失值统计，可理解为对用户信息完备度的统计，需注意的是若该值过大或大部分重要特征的值缺失则需要进行特殊处理，由于缺失值过多导致样本信息的大量缺失，通常考虑删掉该条记录。 

4. 分箱主要应用于对连续变量的离散化和多分类值离散变量的合并。离散化后的特征对异常数据有较强的鲁棒性，不易受极端值的影响；且能避免特征中无意义的波动对模型造成的影响，模型会更稳定。 

5. WOE转换是一种有监督的编码方式，将预测类别的集中度的属性作为编码的数值。通俗来讲就是特征取某个值的时候对违约比例的一种影响。

##### 特征组合

##### 特征交叉

#### 特征选择

缺失率：一个变量，如果缺失率过高，他所包含的信息往往也比较少，做缺失值填补也会比较困难。区分能力：一般情况下，如果模型中加入过多的指标往往会比较冗余，因此在有限个指标中要使模型的效果更佳，自然需要挑选对坏样本识别能力较强的变量。稳定性：一个优秀的模型，除了能够很好的区分好坏样本以外，还需要足够的稳定，防止随着时间的推移过快衰退的现象出现。因此模型中的每一个变量也需要足够的稳定。要进行判断，可以计算不同时间切片之间该指标的PSI。业务逻辑：对于趋势与业务逻辑不一致的指标，往往也会予以剔除，来满足评分模型的可解释性

当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。当用特征选择的方法来理解数据的时候要留心，特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据。

1. 去掉取值变化小的特征，该方法一般用在特征选择前作为一个预处理的工作，即先去掉取值变化小的特征，然后再使用其他特征选择方法选择特征。  对与连续变量计算方差，通过方差来进行筛选，对于离散变量计算每个值所占比率，如果某个类别特别多，则可以考虑剔除这个特征。如果机器资源充足，并且希望尽量保留所有信息，可以把阈值设置得比较高，或者只过滤离散型特征只有一个取值的特征。

2. `Filter`方法，先进行特征选择，然后去训练学习器，所以特征选择的过程与学习器无关。相当于先对特征进行过滤操作，然后用特征子集来训练分类器。主要思想：对每一维特征“打分”，即给每一维的特征赋予权重，这样的权重就代表着该特征的重要性，然后依据权重排序。
3. `Wrapper`方法，直接把最后要使用的分类器作为特征选择的评价函数，对于特定的分类器选择最优的特征子集。主要思想：将子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。这样就将子集的选择看作是一个优化问题，这里有很多的优化算法可以解决，尤其是一些启发式的优化算法，如GA、$\text{PSO}$、DE、ABC等。主要方法：递归特征消除算法。优点：对特征进行搜索时围绕学习算法展开的，对特征选择的标准/规范是在学习算法的需求中展开的，能够考虑学习算法所属的任意学习偏差，从而确定最佳子特征，真正关注的是学习问题本身。由于每次尝试针对特定子集时必须运行学习算法，所以能够关注到学习算法的学习偏差/归纳偏差，因此封装能够发挥巨大的作用。缺点：运行速度远慢于过滤算法，实际应用用封装方法没有过滤方法流行。
4. `Embedded`方法，将特征选择嵌入到模型训练当中，其训练可能是相同的模型，但是特征选择完成后，还能给予特征选择完成的特征和模型训练出的超参数，再次训练优化。主要思想：在模型既定的情况下学习出对提高模型准确性最好的特征。也就是在确定模型的过程中，挑选出那些对模型的训练有重要意义的特征。主要方法：用带有`L1`正则化的项完成特征选择也可以结合`L2`惩罚项来优化、随机森林平均不纯度减少法/平均精确度减少法。优点：对特征进行搜索时围绕学习算法展开的，能够考虑学习算法所属的任意学习偏差。训练模型的次数小于Wrapper方法，比较节省时间。缺点：运行速度慢。

###### 单变量特征选择

1. `Pearson`相关系数：相关系数计算速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Pearson相关系数能够表征丰富的关系，符合表示关系的正负，绝对值能够表示强度。 相关系数作为特征排序机制，**它只对线性关系敏感**，如果关系是非线性的，即便两个变量具有一一对应的关系，相关系数系数也可能会接近0。 
2. 互信息和最大信息系数： 互信息法也是评价定性自变量对定性因变量的相关性的，但是并不方便直接用于特征选择。它不属于度量方式，也没有办法进行归一化，在不同的数据上的结果无法做比较。  只能用于离散型特征的选择，连续型特征需要先进行离散化才能用互信息进行特征选择，而互信息的结果对离散化的方式很敏感。 
3. 距离相关系数：距离相关系数是为了克服Pearson相关系数的弱点而生的。
4. 基于学习模型的特征排序： 这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。 
5. 卡方检验：只适用于分类问题中离散型特征筛选，不能用于分类问题中连续型特征的筛选，也不能用于回归问题的特征筛选

去掉取值变化小的特征方法一般用在特征选择前作为一个预处理的工作，即先去掉取值变化小的特征，然后再使用其他特征选择方法选择特征。如果机器资源充足，并且希望尽量保留所有信息，可以把阈值设置得比较高，或者只过滤离散型特征只有一个取值的特征。单变量特征选择可以用于理解数据、数据的结构、特点，也可以用于排除不相关特征，但是它不能发现冗余特征。

###### 随机森林选择

1. 平均不纯度减少：CART利用不纯度可以确定节点，对于分类问题，通常采用基尼不纯度，对于回归问题，通常采用的是方差或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的标准。

2. 平均精确度减少 ：通过直接度量每个特征对模型精确率的影响来进行特征选择。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大。对于重要的变量来说，打乱顺序就会降低模型的精确率。

###### 顶层特征选择

1. 稳定性选择：它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果。比如可以统计某个特征被认为是重要特征的频率，例如被选为重要特征的次数除以它所在的子集被测试的次数。

2. 递归特征消除：递归特征消除的主要思想是反复的构建模型然后选出最好的或者最差的的特征，把选出来的特征放到一遍，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。

单变量特征选择可以用于理解数据、数据的结构、特点，也可以用于排除不相关特征，但是它不能发现冗余特征。

正则化的线性模型可用于特征理解和特征选择。相比起`L1`正则化，`L2`正则化的表现更加稳定，`L2`正则化对于数据的理解来说很合适。由于响应变量和特征之间往往是非线性关系，可以采用basis expansion的方式将特征转换到一个更加合适的空间当中，在此基础上再考虑运用简单的线性模型。

随机森林是一种非常流行的特征选择方法，它易于使用。但它有两个主要问题：重要的特征有可能得分很低，关联特征问题；这种方法对特征变量类别多的特征越有利，偏向问题。

当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据。

###### 共线性

通过计算变量的`VIF`，可以检验共线性问题是否存在，通过综合考虑单变量或多变量的AR值判断应该保留哪些变量、剔除哪些变量。常用的检验方法主要有简单相关系数检验法、容限度法、方差扩大因子法、特征值和条件指数法、`Theil`多重共线性效应系数法等。

容限度是由每个自变量$X_j$作为因变量对其他自变量回归时得到的余差比例，即：$\text{Tolerance}_j=1-R_j^2$。其中，$R_j^2$表示第$j$个自变量对其他自变量进行回归得到的判定系数$R^2$。容限度很大时，$R_j^2$很小，说明所$X_j$包含的独立信息很多，可能成为重要解释变量；反之，容限度很小，$R_j^2$很大，说明$X_j$与其他自变量的信息重复性越大，其对因变量$Y$的解释能力越小。容限度的大小是根据研究者的具体需要制定的，通常当容限度小于0.1时，便认为变量$X_j$与其他变量之间的多重共线性超过了容许界限。

方差扩大因子是容限度的倒数。即：$VIF_j=1/Tolerance_j=1/(1-R_j^2 )$。它表示所对应的偏回归系数的方差由于多重共线性而扩大的倍数。一般认为：若$VIF>10$，说明模型中有很强的共线性关系；若条件指数值在10与30间为弱相关，在30与100间为中等相关，大于100为强相关。

 The condition `indices` are computed as the square roots of the ratios of the largest eigenvalue to each successive eigenvalue. Values greater than 15 indicate a possible problem with collinearity; greater than 30, a serious problem. 最大的特征值除以其他特征值后的平方根。

###### 逐步回归

逐步回归是一个不断往模型中增加或删除变量，直到找到最优的变量组合、新增变量不再产生具有统计显著意义的增量效应为止。一般来说，这就是指标筛选的最后一步了，如果使用SAS进行逐步回归往往也能控制住入模变量的显著性，因此此时最需要注意的是模型拟合出来的系数方向是否一致，如果出现不一致的情况，可以考虑在变量清单中剔除掉这部分指标，重新进行逐步回归分析，直到系数方向一致为止。

1. 前向逐步回归：`FS`回归是让所有建模指标变量一个一个地进入回归方程，按照预先设定的显著性检验标准，最显著的指标变量最先进入，然后其次就是次显著的指标变量进入，依次类推。
2. 后向逐步回归：它的逻辑是首先让全部指标变量都进入回归方程，按照预先设定的显著性检验标准，把不显著的变量逐一剔除。
3. 混合逐步回归：按照预先设定的显著性检验标准，逐步加入或者剔除指标变量，可以由前向逐步回归开始，也可以由后向逐步回归开始，例如由前向逐步回归开始，当新指标变量进入时，如果老指标变量不满足预先设定的标准可以后向剔除，而对比前向逐步回归，变量一旦进入，就不再退出

经过特征选择后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法有主成分分析法和线性判别分析，线性判别分析本身也是一个分类模型。 

#### 降维

### 建模调参

#### 线性模型回归

线性回归对于特征的要求

标签变化、处理长尾分布

理解线性回归模型

#### 模型性能验证

函数：评价函数、目标函数

方法：交叉验证方法、留一验证方法、针对时间序列的验证方法

秩相关系数，顾名思义，秩的相关系数。秩是指样本值的大小在全体样本从小到大排序后所占的次序。我们还要引入一个定义，对于一对数$(X_1, Y_1)$和$(X_2, Y_2)$，如果$X_1>X_2$且$Y_1> Y_2$或者$X_1<X_2$且$Y_1<Y_2$，则称$(X_1,Y_1)$和$(X_2, Y_2)$是一致的；如果$X_1>X_2$且$Y_1<Y_2$或者$X_1<X_2$且$Y_1>Y_2$，则称$(X_1,Y_1)$和$(X_2, Y_2)$是不一致的；如果$X_1=X_2$或$Y_1=Y_2$，则称$(X_1, Y_1)$和$(X_2,Y_2)$是一个`tie`。在评估逻辑回归模型的过程中，通常让逻辑回归中的目标变量作为X，逻辑回归的结果作为Y。记$n_c$为一致对的个数，$n_d$为不一致对的个数，$n_t$为$X$值不等而$Y$值相等的tie的个数，$N$为观测值的个数（样本量），可以有以下几个秩相关系数： 

一致性指标：$c=\frac{n_{c}+0.5 n_{t}}{n_{c}+n_{d}+n_{t}}$

`Gini coefficient`：$D_{Y X}=\frac{n_{c}-n_{d}}{n_{c}+n_{d}+n_{t}}$

`Goodman-Krustal Gamma`：$\Gamma=\frac{n_{c}-n_{d}}{n_{c}+n_{d}}$

`Kendall`：$\tau=\frac{n_{c}-n_{d}}{N(N-1) / 2}$

同时，`Gini`也等于2倍的`AUC`减一，`AUC`为`ROC`曲线与坐标轴围成的面积。对于二分类问题，这几种计算方法是等价的。 

`ROC`：那么一个模型的特异度可以定义为`TNR=TN/(FP+TN)`，灵敏度可以定义为`TPR=TP/(TP+FN)`。而`ROC`曲线的横坐标是1-特异度=`1-TNR=FP/(FP+TN)=FPR`，纵坐标是灵敏度即`TPR`。`KS`曲线中的所谓“累积比率”其实就是`ROC`曲线中的`TPR`和`FPR`。`ROC`曲线以`FPR`为横轴，`TPR`为纵轴，而`KS`曲线以阈值为横轴，`TPR`、`FPR`为纵轴。所以说，从某种角度看，`ROC`曲线和`KS`曲线其实是一回事。 

$\text{KS(Kolmogorov-Smirnov)}$：KS用于模型风险区分能力进行评估， 指标衡量的是好坏样本累计分部之间的差值。 好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。KS的计算步骤如下： 计算每个评分区间的好坏账户数。 计算每个评分区间的累计好账户数占总好账户数比率和累计坏账户数占总坏账户数比率。 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值，然后对这些绝对值取最大值即得此评分卡的K-S值。

提升度曲线：可以衡量使用这个模型比随机选择对坏样本的预测能力提升了多少倍。通常计算`LIFT`的时候会把模型的最终得分按照从低到高，排序并等频分为10组，计算分数最低的一组对应的`累计坏样本占比/累计总样本占比`就等于`LIFT`值了。从直观上理解，累计坏样本占比相当于是使用模型的情况下最差的这一组能够从所有的坏样本中挑出多少比例的坏样本，而累计总样本占比等于随机抽样的情况下从所有坏样本抽取了多少比例的坏样本。

而对模型的分析部分，则可以帮助我们了解模型哪些数据做的好，哪些数据做的不好，通过此类反馈，我们就可以对错误的数据展开研究，挖掘我们所遗漏的部分，进一步提升我们模型的预测性能。

模型特征重要性分析：`LGB/XGB`等的`importance`、`LR、SVM`的`coeff`等；特征重要性可以结合业务理解，有些奇怪的特征在模型中起着关键的作用，这些可以帮助我们更好地理解我们的业务，同时如果有些特征反常规，我们也可以看出来；可能这些就是过拟合的特征等等；     

模型分割方式分析：可视化模型的预测，包括`LGB`的每一颗数等；这些可以帮助我们很好的理解我们的模型，模型的分割方式是否符合常理也可以结合业务知识一起分析，帮助我们更好的设计模型；       

模型结果分析：这个在回归问题就是看预测的结果的分布；分类一般看混淆矩阵等。这么做可以帮助我们找到模型做的不好的地方，从而更好的修正我们的模型。

根据loss对样本加权的工作就已经有很多。神奇的是，其实在一条线上有着截然相反的想法的研究：第一类工作的想法是如果一个样本训练得不够好，也就是loss高的话，那么说明现在的模型没有很好fit到这样的数据，所以应该对这样的样本给予更高的权重。这一类工作就对应到经典的Hard Negative (Example) Mining，近期的工作如Focal Loss也是这个思想。另一类工作的想法是学习需要循序渐进，应该先学习简单的样本，逐渐加大难度，最终如果仍然后Loss很大的样本，那么认为这些样本可能是Outlier，强行fit这些样本反而可能会使泛化性能下降。这一类中对应的是Curriculum Learning或者Self-Paced Learning类型的工作。本质上，这两个极端对应的是对训练数据本身分布的不同假设。第一类方法认为那些fit不好的样本恰恰是模型应当着重去学习的，第二类方法认为那些fit不上的样本则很可能是训练的label有误。

###### K-S检验

KS检验，是统计学中的一种非参数假设检验，用来检测单样本是否服从某一分布，或者两样本是否服从相同分布。在单样本的情况下，我们想检验这个样本是否服从某一分布函数$F_0(x)$，记$F_1(x)$是该样本的经验分布函数。我们构造KS统计量：$D_n=\max_x|F_1(x)-F_0(x)|$

经验分布函数与目标分布的累积分布函数的最大差值就是我们要求的KS统计量：95%置信度的KS统计量的临界值由$D_n=\frac{1.36}{\sqrt{n}}$。两样本的KS检验，95%置信度的临界值为$D_n=1.36\sqrt{\frac{1}{n_x}+\frac{1}{n_y}}$，如果我们根据样本得到的KS统计量的值小于$D_n$，那么我们就接收原假设。否则，拒绝原假设。

#### 嵌入式特征选择

Lasso、Ridge回归、决策树

#### 模型对比

在信贷风控领域关于机器学习技术的探索主要分为三个方向。第一个方向，既然构造复杂模型存在着不稳定的风险，最稳妥的方式为使用机器学习或人工智能增加新特征，再使用评分卡模型。第二个方向为传统风控为体，机器学习为用。即特征筛选的标准和规则仍不变，仅替换评分卡模型为复杂模型比如`xgboost`等。第三个方向为大规模样本结合机器学习，保证模型的稳定和泛化。

###### 决策树算法

决策树优点：决策树易于理解和解释，可以可视化分析，容易提取出规则；可以同时处理标称型和数值型数据；测试数据集时，运行速度比较快；决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。

决策树缺点：对缺失数据处理比较困难；容易出现过拟合问题；忽略数据集中属性的相互关联；`ID3`算法计算信息增益时结果偏向数值比较多的特征。

改进措施:对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法；使用基于决策树的combination算法

###### 随机森林

优点：可以计算和比较哪些特征比较重要；训练速度快，容易做成并行化方法；在训练过程中，能够检测到特征之间的影响；对于不平衡数据集来说，随机森林能提供平衡数据集误差的有效方法；如有很大一部分的特征遗失，用RF算法仍然可以维持准确度；抗过拟合能力比较强；

缺点：在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合；对于小数据或者低维数据，可能不能产生很好的分类；

###### `KNN`算法

优点 ：`KNN`是一种在线技术，新数据可以直接加入数据集而不必进行重新训练;`KNN`理论简单，容易实现

缺点：对于样本容量大的数据集计算量比较大；样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多；`KNN`每一次分类都会重新进行一次全局运算；k值大小的选择。

###### `SVM`

优点：解决小样本下机器学习问题。解决非线性问题。无局部极小值问题。可以很好的处理高维数据集。泛化能力比较强。

缺点：对于核函数的高维映射解释力不强，尤其是径向基函数。对缺失数据敏感。

###### 朴素贝叶斯

优点：对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。支持增量式运算。即可以实时的对新增的样本进行训练。朴素贝叶斯对结果解释容易理解。

缺点：由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。

###### 逻辑回归

优点：计算代价不高，易于理解和实现

缺点：容易产生欠拟合。分类精度不高；不能很好地处理大量多类特征或变量；  对于非线性特征，需要进行转换。

###### `Adaboost`

优点：很好的利用了弱分类器进行级联。可以将不同的分类算法作为弱分类器。`AdaBoost`具有很高的精度。相对于bagging算法和Random Forest算法，`AdaBoost`充分考虑的每个分类器的权重。

缺点：`AdaBoost`迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。数据不平衡导致分类精度下降。训练比较耗时，每次重新选择当前分类器最好切分点。

###### 神经网络

神经网络优点：分类准确度高，学习能力极强。对噪声数据鲁棒性和容错性较强。有联想能力，能逼近任意非线性关系。

神经网络缺点：神经网络参数较多，权值和阈值。黑盒过程，不能观察中间结果。学习过程比较长，有可能陷入局部极小值。

#### 模型调参

贪心调参方法、网格调参方法、贝叶斯调参方法

### 模型融合

#### 简单加权融合

回归：算术平均、几何平均

分类：投票

综合：排序融合、log融合

#### Boosting/Bagging

#### Stacking/Blending

**Edited Nearest Neighbours(ENN)**：对于属于多数类的一个样本，如果其K个近邻点有超过一半都不属于多数类，则这个样本会被剔除。这个方法的另一个变种是所有的K个近邻点都不属于多数类，则这个样本会被剔除。

 

- **NearMiss-1**：选择到最近的K个少数类样本平均距离最近的多数类样本
- **NearMiss-2**：选择到最远的K个少数类样本平均距离最近的多数类样本
- **NearMiss-3**：对于每个少数类样本选择K个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围



##### 描述统计

描述统计是通过图表或数学方法，对数据资料进行整理、分析，并对数据的分布状态、数字特征和随机变量之间关系进行估计和描述的方法。描述统计分为集中趋势分析、离中趋势分析和相关分析三大部分。

1. 集中趋势分析：集中趋势分析主要靠平均数、中数、众数等统计指标来表示数据的集中趋势。例如被试的平均成绩多少？是正偏分布还是负偏分布？

2. 离中趋势分析：离中趋势分析主要靠全距、四分差、平均差、方差（协方差：用来度量两个随机变量关系的统计量）、标准差等统计指标来研究数据的离中趋势。例如，我们想知道两个教学班的语文成绩中，哪个班级内的成绩分布更分散，就可以用两个班级的四分差或百分点来比较。

3. 相关分析：相关分析探讨数据之间是否具有统计学上的关联性。这种关系既包括两个数据之间的单一相关关系，也包括多个数据之间的多重相关关系；既包括A大B就大(小)，A小B就小(大)的直线相关关系，也可以是复杂相关关系（A=Y-B*X）；既可以是A、B变量同时增大这种正相关关系，也可以是A变量增大时B变量减小这种负相关，还包括两变量共同变化的紧密程度——即相关系数。

4. 推论统计：它以统计结果为依据，来证明或推翻某个命题。具体来说,就是通过分析样本与样本分布的差异，来估算样本与总体、同一样本的前后测成绩差异，样本与样本的成绩差距、总体与总体的成绩差距是否具有显著性差异。

###### 参数检验

参数检验是在已知总体分布的条件下（一般要求总体服从正态分布）对一些主要的参数(如均值、百分数、方差、相关系数等）进行的检验。

> 1）U验 ：使用条件：当样本含量n较大时，样本值符合正态分布
>
> 2）T检验 使用条件：当样本含量n较小时，样本值符合正态分布

A：单样本t检验：推断该样本来自的总体均数μ与已知的某一总体均数μ0 (常为理论值或标准值)有无差别；

B：配对样本t检验：当总体均数未知时，且两个样本可以配对，同对中的两者在可能会影响处理效果的各种条件方面为相似；

C：两独立样本t检验：无法找到在各方面极为相似的两样本作配对比较时使用。

###### 非参数检验

非参数检验则不考虑总体分布是否已知，常常也不是针对总体参数，而是针对总体的某些一般性假设（如总体分布的位罝是否相同，总体分布是否正态）进行检验。适用情况：顺序类型的数据资料，这类数据的分布形态一般是未知的。

A**：**虽然是连续数据，但总体分布形态未知或者非正态；

B：总体分布虽然正态，数据也是连续类型，但样本容量极小，如10以下；

###### 信度分析

信度即可靠性，它是指采用同样的方法对同一对象重复测量时所得结果的一致性程度。 信度指标多以相关系数表示，大致可分为三类：稳定系数（跨时间的一致性），等值系数（跨形式的一致性）和内在一致性系数（跨项目的一致性）。信度分析的方法主要有以下四种：**重测信度法、复本信度法、折半信度法、α信度系数法。**

- 重测信度法编辑：这一方法是用同样的问卷对同一组被调查者间隔一定时间重复施测，计算两次施测结果的相关系数。显然，重测信度属于稳定系数。重测信度法特别适用于事实式问卷，如性别、出生年月等在两次施测中不应有任何差异，大多数被调查者的兴趣、爱好、习惯等在短时间内也不会有十分明显的变化。如果没有突发事件导致被调查者的态度、意见突变，这种方法也适用于态度、意见式问卷。
- 复本信度法编辑：让同一组被调查者一次填答两份问卷复本，计算两个复本的相关系数。复本信度属于等值系数。复本信度法要求两个复本除表述方式不同外，在内容、格式、难度和对应题项的提问方向等方面要完全一致，而在实际调查中，很难使调查问卷达到这种要求，因此采用这种方法者较少。
- α信度系数法编辑：α信度系数是目前最常用的信度系数，其公式为：$α=\frac{k}{k-1}*(1-\sum{\frac{S_i^2}{S_T^2}})$。其中，K为量表中题项的总数， $S_i^2$为第i题得分的题内方差， $S_T^2$为全部题项总得分的方差。从公式中可以看出，属于内在一致性系数。这种方法适用于态度、意见式问卷的信度分析。总量表的信度系数最好在0.8以上，0.7-0.8之间可以接受；分量表的信度系数最好在0.7以上，0.6-0.7还可以接受。Cronbach 's alpha系数如果在0.6以下就要考虑重新编问卷。

分类：外在信度：不同时间测量时量表的一致性程度，常用方法重测信度。内在信度：每个量表是否测量到单一的概念，同时组成两表的内在体项一致性如何，常用方法分半信度。

###### 列联表分析

列联表是观测数据按两个或更多属性（定性变量）分类时所列出的频数表。列联表又称交互分类表，所谓交互分类，**是指同时依据两个变量的值，将所研究的个案分类。**交互分类的目的是将两变量分组，然后比较各组的分布状况，以寻找变量间的关系。用于分析离散变量或定型变量之间是否存在相关。

列联表分析的基本问题是，判明所考察的各属性之间有无关联，即是否独立。在r×с表中，若以$p_i$、$p_j$和$p_{ij}$分别表示总体中的个体属于等级$A_i$，属于等级$B_j$和同时属于$A_i、B_j$的概率,“A、B两属性无关联”的假设可以表述为$H_0$：$p_{ij}=p_i·p_j$，(i=1，2，…，r；j=1,2,…，с)，未知参数$p_{ij}、p_i、p_j$的最大似然估计分别为行和及列和为样本大小。当$H_0$成立，且一切$p_i>0$和$p_j>0$时，统计量的渐近分布是自由度为(r－1)(с－1) 的$Ⅹ$分布，式中$E_{ij}=\frac{n_i\times n_j}{n}$称为期望频数。当n足够大，且表中各格的$E_{ij}$都不太小时，可以据此对$h_0$作检验：若Ⅹ值足够大，就拒绝假设$h_0$，即认为A与B有关联。

###### 相关分析

研究现象之间是否存在某种依存关系，对具体有依存关系的现象探讨相关方向及相关程度。

1. 单相关：两个因素之间的相关关系叫单相关，即研究时只涉及一个自变量和一个因变量；

2. 复相关 ：三个或三个以上因素的相关关系叫复相关，即研究时涉及两个或两个以上的自变量和因变量相关；

3. 偏相关：在某一现象与多种现象相关的场合，当假定其他变量不变时，其中两个变量之间的相关关系称为偏相关。

###### 方差分析

使用条件：各样本须是相互独立的随机样本；各样本来自正态分布总体；各总体方差相等。

- 单因素方差分析：一项试验只有一个影响因素，或者存在多个影响因素时，只分析一个因素与响应变量的关系
- 多因素有交互方差分析：一项实验有多个影响因素，分析多个影响因素与响应变量的关系，同时考虑多个影响因素之间的关系
- 多因素无交互方差分析：分析多个影响因素与响应变量的关系，但是影响因素之间没有影响关系或忽略影响关系
- 协方差分析：传统的方差分析存在明显的弊端，无法控制分析中存在的某些随机因素，使之影响了分析结果的准确度。协方差分析主要是在排除了协变量的影响后再对修正后的主效应进行方差分析，是将线性回归与方差分析结合起来的一种分析方法。

###### 回归分析

- 一元线性回归分析：只有一个自变量X与因变量Y有关，X与Y都必须是连续型变量，因变量y或其残差必须服从正态分布。

- 多元线性回归分析使用条件：分析多个自变量与因变量Y的关系，X与Y都必须是连续型变量，因变量y或其残差必须服从正态分布 。

> **横型诊断方法**
>
> 残差检验：观测值与估计值的差值要跟从正态分布
>
> 强影响点判断：寻找方式一般分为标准误差法、Mahalanobis距离法
>
> 共线性诊断：诊断方式：容忍度、方差扩大因子法(又称膨胀系数VIF)、特征根判定法、条件指针CI、方差比例，处理方法：增加样本容量或选取另外的回归如主成分回归等

数据转换的方式有：

- 数据归一化(MinMaxScaler)；
- 标准化(StandardScaler)；
- 对数变换(log1p)；
- 转换数据类型(astype)；
- 独热编码(OneHotEncoder)；
- 标签编码(LabelEncoder)；
- 修复偏斜特征(boxcox1p)等。

